{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "109707f2",
   "metadata": {},
   "source": [
    "# 04 - Test and Deploy Training Pipeline to Vertex Pipelines\n",
    "\n",
    "The purpose of this notebook is to test, deploy, and run the `TFX` pipeline on `Vertex Pipelines`. The notebook covers the following tasks:\n",
    "1. Run the tests locally.\n",
    "2. Run the pipeline using `Vertex Pipelines`\n",
    "3. Execute the pipeline deployment `CI/CD` steps using `Cloud Build`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a51af1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133e7d80",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3bb184e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version: 1.2.0\n",
      "KFP Version: 1.8.9\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import kfp\n",
    "import tfx\n",
    "\n",
    "print(\"Tensorflow Version:\", tfx.__version__)\n",
    "print(\"KFP Version:\", kfp.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7bdded",
   "metadata": {},
   "source": [
    "### Setup Google Cloud project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b4b22be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID: grandelli-demo-295810\n",
      "Region: us-central1\n",
      "Bucket name: grandelli-demo-295810-partner-training-2022\n",
      "Service Account: 155283586619-compute@developer.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "PROJECT = 'grandelli-demo-295810' # Change to your project id.\n",
    "REGION = 'us-central1' # Change to your region.\n",
    "BUCKET =  'grandelli-demo-295810-partner-training-2022' # Change to your bucket name.\n",
    "SERVICE_ACCOUNT = \"155283586619-compute@developer.gserviceaccount.com\"\n",
    "\n",
    "if PROJECT == \"\" or PROJECT is None or PROJECT == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT = shell_output[0]\n",
    "    \n",
    "if SERVICE_ACCOUNT == \"\" or SERVICE_ACCOUNT is None or SERVICE_ACCOUNT == \"[your-service-account]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud config list --format 'value(core.account)' 2>/dev/null\n",
    "    SERVICE_ACCOUNT = shell_output[0]\n",
    "    \n",
    "if BUCKET == \"\" or BUCKET is None or BUCKET == \"[your-bucket-name]\":\n",
    "    # Get your bucket name to GCP project id\n",
    "    BUCKET = PROJECT\n",
    "    # Try to create the bucket if it doesn't exists\n",
    "    ! gsutil mb -l $REGION gs://$BUCKET\n",
    "    print(\"\")\n",
    "    \n",
    "print(\"Project ID:\", PROJECT)\n",
    "print(\"Region:\", REGION)\n",
    "print(\"Bucket name:\", BUCKET)\n",
    "print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660a9dc9",
   "metadata": {},
   "source": [
    "### Set configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a75e169",
   "metadata": {},
   "outputs": [],
   "source": [
    "BQ_LOCATION = 'US'\n",
    "BQ_DATASET_NAME = 'partner_training' # Change to your BQ dataset name.\n",
    "BQ_TABLE_NAME = 'chicago_taxitrips_prep'\n",
    "\n",
    "VERSION = 'v01'\n",
    "DATASET_DISPLAY_NAME = 'chicago-taxi-tips'\n",
    "MODEL_DISPLAY_NAME = f'{DATASET_DISPLAY_NAME}-classifier-{VERSION}'\n",
    "PIPELINE_NAME = f'{MODEL_DISPLAY_NAME}-train-pipeline'\n",
    "\n",
    "CICD_IMAGE_NAME = 'cicd:latest'\n",
    "CICD_IMAGE_URI = f\"gcr.io/{PROJECT}/{CICD_IMAGE_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06be3555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'src/raw_schema/.ipynb_checkpoints/': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!rm -r src/raw_schema/.ipynb_checkpoints/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44678373",
   "metadata": {},
   "source": [
    "## 1. Run the CICD steps locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68204ed",
   "metadata": {},
   "source": [
    "### Set pipeline configurations for the local run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05a1d20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"DATASET_DISPLAY_NAME\"] = DATASET_DISPLAY_NAME\n",
    "os.environ[\"MODEL_DISPLAY_NAME\"] =  MODEL_DISPLAY_NAME\n",
    "os.environ[\"PIPELINE_NAME\"] = PIPELINE_NAME\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"BQ_LOCATION\"] = BQ_LOCATION\n",
    "os.environ[\"BQ_DATASET_NAME\"] = BQ_DATASET_NAME\n",
    "os.environ[\"BQ_TABLE_NAME\"] = BQ_TABLE_NAME\n",
    "os.environ[\"GCS_LOCATION\"] = f\"gs://{BUCKET}/{DATASET_DISPLAY_NAME}/e2e_tests\"\n",
    "os.environ[\"TRAIN_LIMIT\"] = \"1000\"\n",
    "os.environ[\"TEST_LIMIT\"] = \"100\"\n",
    "os.environ[\"UPLOAD_MODEL\"] = \"0\"\n",
    "os.environ[\"ACCURACY_THRESHOLD\"] = \"0.1\"\n",
    "os.environ[\"BEAM_RUNNER\"] = \"DirectRunner\"\n",
    "os.environ[\"TRAINING_RUNNER\"] = \"local\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d353e3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 14:43:03.745149: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT: grandelli-demo-295810\n",
      "REGION: us-central1\n",
      "GCS_LOCATION: gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests\n",
      "ARTIFACT_STORE_URI: gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts\n",
      "MODEL_REGISTRY_URI: gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/model_registry\n",
      "DATASET_DISPLAY_NAME: chicago-taxi-tips\n",
      "MODEL_DISPLAY_NAME: chicago-taxi-tips-classifier-v01\n",
      "PIPELINE_NAME: chicago-taxi-tips-classifier-v01-train-pipeline\n",
      "ML_USE_COLUMN: ml_use\n",
      "EXCLUDE_COLUMNS: trip_start_timestamp\n",
      "TRAIN_LIMIT: 1000\n",
      "TEST_LIMIT: 100\n",
      "SERVE_LIMIT: 0\n",
      "NUM_TRAIN_SPLITS: 4\n",
      "NUM_EVAL_SPLITS: 1\n",
      "ACCURACY_THRESHOLD: 0.1\n",
      "USE_KFP_SA: False\n",
      "TFX_IMAGE_URI: gcr.io/grandelli-demo-295810/tfx-chicago-taxi-tips:latest\n",
      "BEAM_RUNNER: DirectRunner\n",
      "BEAM_DIRECT_PIPELINE_ARGS: ['--project=grandelli-demo-295810', '--temp_location=gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/temp']\n",
      "BEAM_DATAFLOW_PIPELINE_ARGS: ['--project=grandelli-demo-295810', '--temp_location=gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/temp', '--region=us-central1', '--runner=DirectRunner']\n",
      "TRAINING_RUNNER: local\n",
      "VERTEX_TRAINING_ARGS: {'project': 'grandelli-demo-295810', 'worker_pool_specs': [{'machine_spec': {'machine_type': 'n1-standard-4'}, 'replica_count': 1, 'container_spec': {'image_uri': 'gcr.io/grandelli-demo-295810/tfx-chicago-taxi-tips:latest'}}]}\n",
      "VERTEX_TRAINING_CONFIG: {'ai_platform_training_enable_ucaip': True, 'ai_platform_training_ucaip_region': 'us-central1', 'ai_platform_training_args': {'project': 'grandelli-demo-295810', 'worker_pool_specs': [{'machine_spec': {'machine_type': 'n1-standard-4'}, 'replica_count': 1, 'container_spec': {'image_uri': 'gcr.io/grandelli-demo-295810/tfx-chicago-taxi-tips:latest'}}]}, 'use_gpu': False}\n",
      "SERVING_RUNTIME: tf2-cpu.2-5\n",
      "SERVING_IMAGE_URI: us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-5:latest\n",
      "BATCH_PREDICTION_BQ_DATASET_NAME: playground_us\n",
      "BATCH_PREDICTION_BQ_TABLE_NAME: chicago_taxitrips_prep\n",
      "BATCH_PREDICTION_BEAM_ARGS: {'runner': 'DirectRunner', 'temporary_dir': 'gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/temp', 'gcs_location': 'gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/temp', 'project': 'grandelli-demo-295810', 'region': 'us-central1', 'setup_file': './setup.py'}\n",
      "BATCH_PREDICTION_JOB_RESOURCES: {'machine_type': 'n1-standard-2', 'starting_replica_count': 1, 'max_replica_count': 10}\n",
      "DATASTORE_PREDICTION_KIND: chicago-taxi-tips-classifier-v01-predictions\n",
      "ENABLE_CACHE: 0\n",
      "UPLOAD_MODEL: 0\n"
     ]
    }
   ],
   "source": [
    "from src.tfx_pipelines import config\n",
    "import importlib\n",
    "importlib.reload(config)\n",
    "\n",
    "for key, value in config.__dict__.items():\n",
    "    if key.isupper(): print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d51c12",
   "metadata": {},
   "source": [
    "### Run unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9be84a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.7.12, pytest-7.1.0, pluggy-1.0.0\n",
      "rootdir: /home/jupyter/20220318_Training/gcp-partner-training-mlops\n",
      "plugins: anyio-3.3.4\n",
      "collected 2 items                                                              \n",
      "\n",
      "src/tests/datasource_utils_tests.py BigQuery Source: grandelli-demo-295810.partner_training.chicago_taxitrips_prep\n",
      ".BigQuery Source: grandelli-demo-295810.partner_training.chicago_taxitrips_prep\n",
      ".\n",
      "\n",
      "============================== 2 passed in 11.23s ==============================\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest src/tests/datasource_utils_tests.py -s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4358f955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.7.12, pytest-7.1.0, pluggy-1.0.0\n",
      "rootdir: /home/jupyter/20220318_Training/gcp-partner-training-mlops\n",
      "plugins: anyio-3.3.4\n",
      "collecting ... 2022-03-30 14:43:40.820150: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "collected 2 items                                                              \n",
      "\n",
      "src/tests/model_tests.py .2022-03-30 14:43:43.197665: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2022-03-30 14:43:43.197715: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-03-30 14:43:43.197750: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (vm-508f776f-5512-42a9-b529-8989959ded1b): /proc/driver/nvidia/version does not exist\n",
      "2022-03-30 14:43:43.198139: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "TensorFlow in TF remote image: 2.5.3\n",
      "max_tokens is deprecated, please use num_tokens instead.\n",
      "max_tokens is deprecated, please use num_tokens instead.\n",
      ".\n",
      "\n",
      "=============================== warnings summary ===============================\n",
      "../../.local/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py:22\n",
      "  /home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py:22: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "    import imp\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "========================= 2 passed, 1 warning in 2.94s =========================\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest src/tests/model_tests.py -s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa00fd5",
   "metadata": {},
   "source": [
    "### Run e2e pipeline test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb9aad70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.7.12, pytest-7.1.0, pluggy-1.0.0\n",
      "rootdir: /home/jupyter/20220318_Training/gcp-partner-training-mlops\n",
      "plugins: anyio-3.3.4\n",
      "collecting ... 2022-03-30 14:43:45.669774: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "collected 1 item                                                               \n",
      "\n",
      "src/tests/pipeline_deployment_tests.py upload_model: 0\n",
      "Pipeline e2e test artifacts stored in: gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests\n",
      "ML metadata store is ready.\n",
      "Excluding no splits because exclude_splits is not set.\n",
      "Excluding no splits because exclude_splits is not set.\n",
      "Pipeline components: ['HyperparamsGen', 'TrainDataGen', 'TestDataGen', 'StatisticsGen', 'SchemaImporter', 'ExampleValidator', 'DataTransformer', 'WarmstartModelResolver', 'ModelTrainer', 'BaselineModelResolver', 'ModelEvaluator', 'ModelPusher']\n",
      "Beam pipeline args: ['--project=grandelli-demo-295810', '--temp_location=gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/temp']\n",
      "Generating ephemeral wheel package for '/home/jupyter/20220318_Training/gcp-partner-training-mlops/src/preprocessing/transformations.py' (including modules: ['etl', 'transformations']).\n",
      "User module package has hash fingerprint version de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d.\n",
      "Executing: ['/opt/conda/bin/python', '/tmp/tmp52bu_wyd/_tfx_generated_setup.py', 'bdist_wheel', '--bdist-dir', '/tmp/tmpnrxbobt0', '--dist-dir', '/tmp/tmpm45qhytb']\n",
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "copying etl.py -> build/lib\n",
      "copying transformations.py -> build/lib\n",
      "/opt/conda/lib/python3.7/site-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "  setuptools.SetuptoolsDeprecationWarning,\n",
      "installing to /tmp/tmpnrxbobt0\n",
      "running install\n",
      "running install_lib\n",
      "copying build/lib/transformations.py -> /tmp/tmpnrxbobt0\n",
      "copying build/lib/etl.py -> /tmp/tmpnrxbobt0\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "creating tfx_user_code_DataTransformer.egg-info\n",
      "writing tfx_user_code_DataTransformer.egg-info/PKG-INFO\n",
      "writing dependency_links to tfx_user_code_DataTransformer.egg-info/dependency_links.txt\n",
      "writing top-level names to tfx_user_code_DataTransformer.egg-info/top_level.txt\n",
      "writing manifest file 'tfx_user_code_DataTransformer.egg-info/SOURCES.txt'\n",
      "reading manifest file 'tfx_user_code_DataTransformer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'tfx_user_code_DataTransformer.egg-info/SOURCES.txt'\n",
      "Copying tfx_user_code_DataTransformer.egg-info to /tmp/tmpnrxbobt0/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3.7.egg-info\n",
      "running install_scripts\n",
      "creating /tmp/tmpnrxbobt0/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d.dist-info/WHEEL\n",
      "creating '/tmp/tmpm45qhytb/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl' and adding '/tmp/tmpnrxbobt0' to it\n",
      "adding 'etl.py'\n",
      "adding 'transformations.py'\n",
      "adding 'tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d.dist-info/METADATA'\n",
      "adding 'tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d.dist-info/WHEEL'\n",
      "adding 'tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d.dist-info/top_level.txt'\n",
      "adding 'tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d.dist-info/RECORD'\n",
      "removing /tmp/tmpnrxbobt0\n",
      "Successfully built user code wheel distribution at 'gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/_wheels/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl'; target user module is 'transformations'.\n",
      "Full user module path is 'transformations@gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/_wheels/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl'\n",
      "Generating ephemeral wheel package for '/home/jupyter/20220318_Training/gcp-partner-training-mlops/src/model_training/runner.py' (including modules: ['runner', 'data', 'exporter', 'model', 'task', 'trainer', 'defaults']).\n",
      "User module package has hash fingerprint version 45151d477b837ba3e9c07eca0c4beadd51fa08b10afc0503d6ce3837f470c259.\n",
      "Executing: ['/opt/conda/bin/python', '/tmp/tmpnjgsz0l4/_tfx_generated_setup.py', 'bdist_wheel', '--bdist-dir', '/tmp/tmpy_838zy1', '--dist-dir', '/tmp/tmpdf5yhgvs']\n",
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "copying runner.py -> build/lib\n",
      "copying data.py -> build/lib\n",
      "copying exporter.py -> build/lib\n",
      "copying model.py -> build/lib\n",
      "copying task.py -> build/lib\n",
      "copying trainer.py -> build/lib\n",
      "copying defaults.py -> build/lib\n",
      "/opt/conda/lib/python3.7/site-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "  setuptools.SetuptoolsDeprecationWarning,\n",
      "installing to /tmp/tmpy_838zy1\n",
      "running install\n",
      "running install_lib\n",
      "copying build/lib/trainer.py -> /tmp/tmpy_838zy1\n",
      "copying build/lib/defaults.py -> /tmp/tmpy_838zy1\n",
      "copying build/lib/task.py -> /tmp/tmpy_838zy1\n",
      "copying build/lib/exporter.py -> /tmp/tmpy_838zy1\n",
      "copying build/lib/data.py -> /tmp/tmpy_838zy1\n",
      "copying build/lib/model.py -> /tmp/tmpy_838zy1\n",
      "copying build/lib/runner.py -> /tmp/tmpy_838zy1\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "creating tfx_user_code_ModelTrainer.egg-info\n",
      "writing tfx_user_code_ModelTrainer.egg-info/PKG-INFO\n",
      "writing dependency_links to tfx_user_code_ModelTrainer.egg-info/dependency_links.txt\n",
      "writing top-level names to tfx_user_code_ModelTrainer.egg-info/top_level.txt\n",
      "writing manifest file 'tfx_user_code_ModelTrainer.egg-info/SOURCES.txt'\n",
      "reading manifest file 'tfx_user_code_ModelTrainer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'tfx_user_code_ModelTrainer.egg-info/SOURCES.txt'\n",
      "Copying tfx_user_code_ModelTrainer.egg-info to /tmp/tmpy_838zy1/tfx_user_code_ModelTrainer-0.0+45151d477b837ba3e9c07eca0c4beadd51fa08b10afc0503d6ce3837f470c259-py3.7.egg-info\n",
      "running install_scripts\n",
      "creating /tmp/tmpy_838zy1/tfx_user_code_ModelTrainer-0.0+45151d477b837ba3e9c07eca0c4beadd51fa08b10afc0503d6ce3837f470c259.dist-info/WHEEL\n",
      "creating '/tmp/tmpdf5yhgvs/tfx_user_code_ModelTrainer-0.0+45151d477b837ba3e9c07eca0c4beadd51fa08b10afc0503d6ce3837f470c259-py3-none-any.whl' and adding '/tmp/tmpy_838zy1' to it\n",
      "adding 'data.py'\n",
      "adding 'defaults.py'\n",
      "adding 'exporter.py'\n",
      "adding 'model.py'\n",
      "adding 'runner.py'\n",
      "adding 'task.py'\n",
      "adding 'trainer.py'\n",
      "adding 'tfx_user_code_ModelTrainer-0.0+45151d477b837ba3e9c07eca0c4beadd51fa08b10afc0503d6ce3837f470c259.dist-info/METADATA'\n",
      "adding 'tfx_user_code_ModelTrainer-0.0+45151d477b837ba3e9c07eca0c4beadd51fa08b10afc0503d6ce3837f470c259.dist-info/WHEEL'\n",
      "adding 'tfx_user_code_ModelTrainer-0.0+45151d477b837ba3e9c07eca0c4beadd51fa08b10afc0503d6ce3837f470c259.dist-info/top_level.txt'\n",
      "adding 'tfx_user_code_ModelTrainer-0.0+45151d477b837ba3e9c07eca0c4beadd51fa08b10afc0503d6ce3837f470c259.dist-info/RECORD'\n",
      "removing /tmp/tmpy_838zy1\n",
      "Successfully built user code wheel distribution at 'gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/_wheels/tfx_user_code_ModelTrainer-0.0+45151d477b837ba3e9c07eca0c4beadd51fa08b10afc0503d6ce3837f470c259-py3-none-any.whl'; target user module is 'runner'.\n",
      "Full user module path is 'runner@gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/_wheels/tfx_user_code_ModelTrainer-0.0+45151d477b837ba3e9c07eca0c4beadd51fa08b10afc0503d6ce3837f470c259-py3-none-any.whl'\n",
      "Running pipeline:\n",
      " pipeline_info {\n",
      "  id: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "}\n",
      "nodes {\n",
      "  pipeline_node {\n",
      "    node_info {\n",
      "      type {\n",
      "        name: \"tfx.dsl.components.common.resolver.Resolver\"\n",
      "      }\n",
      "      id: \"BaselineModelResolver\"\n",
      "    }\n",
      "    contexts {\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline_run\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"2022-03-30T14:44:39.880285\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"node\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.BaselineModelResolver\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    inputs {\n",
      "      inputs {\n",
      "        key: \"model\"\n",
      "        value {\n",
      "          channels {\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"Model\"\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      inputs {\n",
      "        key: \"model_blessing\"\n",
      "        value {\n",
      "          channels {\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"ModelBlessing\"\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      resolver_config {\n",
      "        resolver_steps {\n",
      "          class_path: \"tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy.LatestBlessedModelStrategy\"\n",
      "          config_json: \"{}\"\n",
      "          input_keys: \"model\"\n",
      "          input_keys: \"model_blessing\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    downstream_nodes: \"ModelEvaluator\"\n",
      "    execution_options {\n",
      "      caching_options {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "nodes {\n",
      "  pipeline_node {\n",
      "    node_info {\n",
      "      type {\n",
      "        name: \"src.tfx_pipelines.components.hyperparameters_gen\"\n",
      "      }\n",
      "      id: \"HyperparamsGen\"\n",
      "    }\n",
      "    contexts {\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline_run\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"2022-03-30T14:44:39.880285\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"node\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.HyperparamsGen\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    outputs {\n",
      "      outputs {\n",
      "        key: \"hyperparameters\"\n",
      "        value {\n",
      "          artifact_spec {\n",
      "            type {\n",
      "              name: \"HyperParameters\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    parameters {\n",
      "      parameters {\n",
      "        key: \"batch_size\"\n",
      "        value {\n",
      "          field_value {\n",
      "            int_value: 512\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      parameters {\n",
      "        key: \"hidden_units\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"128,128\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      parameters {\n",
      "        key: \"learning_rate\"\n",
      "        value {\n",
      "          field_value {\n",
      "            double_value: 0.001\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      parameters {\n",
      "        key: \"num_epochs\"\n",
      "        value {\n",
      "          field_value {\n",
      "            int_value: 1\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    downstream_nodes: \"ModelTrainer\"\n",
      "    execution_options {\n",
      "      caching_options {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "nodes {\n",
      "  pipeline_node {\n",
      "    node_info {\n",
      "      type {\n",
      "        name: \"tfx.dsl.components.common.importer.Importer\"\n",
      "      }\n",
      "      id: \"SchemaImporter\"\n",
      "    }\n",
      "    contexts {\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline_run\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"2022-03-30T14:44:39.880285\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"node\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.SchemaImporter\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    outputs {\n",
      "      outputs {\n",
      "        key: \"result\"\n",
      "        value {\n",
      "          artifact_spec {\n",
      "            type {\n",
      "              name: \"Schema\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    parameters {\n",
      "      parameters {\n",
      "        key: \"artifact_uri\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"src/raw_schema\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      parameters {\n",
      "        key: \"reimport\"\n",
      "        value {\n",
      "          field_value {\n",
      "            int_value: 0\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    downstream_nodes: \"DataTransformer\"\n",
      "    downstream_nodes: \"ExampleValidator\"\n",
      "    downstream_nodes: \"ModelEvaluator\"\n",
      "    downstream_nodes: \"ModelTrainer\"\n",
      "    execution_options {\n",
      "      caching_options {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "nodes {\n",
      "  pipeline_node {\n",
      "    node_info {\n",
      "      type {\n",
      "        name: \"tfx.extensions.google_cloud_big_query.example_gen.component.BigQueryExampleGen\"\n",
      "      }\n",
      "      id: \"TestDataGen\"\n",
      "    }\n",
      "    contexts {\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline_run\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"2022-03-30T14:44:39.880285\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"node\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.TestDataGen\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    outputs {\n",
      "      outputs {\n",
      "        key: \"examples\"\n",
      "        value {\n",
      "          artifact_spec {\n",
      "            type {\n",
      "              name: \"Examples\"\n",
      "              properties {\n",
      "                key: \"span\"\n",
      "                value: INT\n",
      "              }\n",
      "              properties {\n",
      "                key: \"split_names\"\n",
      "                value: STRING\n",
      "              }\n",
      "              properties {\n",
      "                key: \"version\"\n",
      "                value: INT\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    parameters {\n",
      "      parameters {\n",
      "        key: \"input_config\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"\\\\n    SELECT \\\\n        IF(trip_month IS NULL, -1, trip_month) trip_month,\\\\n        IF(trip_day IS NULL, -1, trip_day) trip_day,\\\\n        IF(trip_day_of_week IS NULL, -1, trip_day_of_week) trip_day_of_week,\\\\n        IF(trip_hour IS NULL, -1, trip_hour) trip_hour,\\\\n        IF(trip_seconds IS NULL, -1, trip_seconds) trip_seconds,\\\\n        IF(trip_miles IS NULL, -1, trip_miles) trip_miles,\\\\n        IF(payment_type IS NULL, \\'NA\\', payment_type) payment_type,\\\\n        IF(pickup_grid IS NULL, \\'NA\\', pickup_grid) pickup_grid,\\\\n        IF(dropoff_grid IS NULL, \\'NA\\', dropoff_grid) dropoff_grid,\\\\n        IF(euclidean IS NULL, -1, euclidean) euclidean,\\\\n        IF(loc_cross IS NULL, \\'NA\\', loc_cross) loc_cross,\\\\n        tip_bin\\\\n    FROM partner_training.chicago_taxitrips_prep \\\\n    WHERE ML_use = \\'TEST\\'\\\\n    LIMIT 100\\\"\\n    }\\n  ]\\n}\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      parameters {\n",
      "        key: \"output_config\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"test\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      parameters {\n",
      "        key: \"output_data_format\"\n",
      "        value {\n",
      "          field_value {\n",
      "            int_value: 6\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      parameters {\n",
      "        key: \"output_file_format\"\n",
      "        value {\n",
      "          field_value {\n",
      "            int_value: 5\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    downstream_nodes: \"ModelEvaluator\"\n",
      "    execution_options {\n",
      "      caching_options {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "nodes {\n",
      "  pipeline_node {\n",
      "    node_info {\n",
      "      type {\n",
      "        name: \"tfx.extensions.google_cloud_big_query.example_gen.component.BigQueryExampleGen\"\n",
      "      }\n",
      "      id: \"TrainDataGen\"\n",
      "    }\n",
      "    contexts {\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline_run\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"2022-03-30T14:44:39.880285\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"node\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.TrainDataGen\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    outputs {\n",
      "      outputs {\n",
      "        key: \"examples\"\n",
      "        value {\n",
      "          artifact_spec {\n",
      "            type {\n",
      "              name: \"Examples\"\n",
      "              properties {\n",
      "                key: \"span\"\n",
      "                value: INT\n",
      "              }\n",
      "              properties {\n",
      "                key: \"split_names\"\n",
      "                value: STRING\n",
      "              }\n",
      "              properties {\n",
      "                key: \"version\"\n",
      "                value: INT\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    parameters {\n",
      "      parameters {\n",
      "        key: \"input_config\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"\\\\n    SELECT \\\\n        IF(trip_month IS NULL, -1, trip_month) trip_month,\\\\n        IF(trip_day IS NULL, -1, trip_day) trip_day,\\\\n        IF(trip_day_of_week IS NULL, -1, trip_day_of_week) trip_day_of_week,\\\\n        IF(trip_hour IS NULL, -1, trip_hour) trip_hour,\\\\n        IF(trip_seconds IS NULL, -1, trip_seconds) trip_seconds,\\\\n        IF(trip_miles IS NULL, -1, trip_miles) trip_miles,\\\\n        IF(payment_type IS NULL, \\'NA\\', payment_type) payment_type,\\\\n        IF(pickup_grid IS NULL, \\'NA\\', pickup_grid) pickup_grid,\\\\n        IF(dropoff_grid IS NULL, \\'NA\\', dropoff_grid) dropoff_grid,\\\\n        IF(euclidean IS NULL, -1, euclidean) euclidean,\\\\n        IF(loc_cross IS NULL, \\'NA\\', loc_cross) loc_cross,\\\\n        tip_bin\\\\n    FROM partner_training.chicago_taxitrips_prep \\\\n    WHERE ML_use = \\'UNASSIGNED\\'\\\\n    LIMIT 1000\\\"\\n    }\\n  ]\\n}\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      parameters {\n",
      "        key: \"output_config\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 4,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      parameters {\n",
      "        key: \"output_data_format\"\n",
      "        value {\n",
      "          field_value {\n",
      "            int_value: 6\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      parameters {\n",
      "        key: \"output_file_format\"\n",
      "        value {\n",
      "          field_value {\n",
      "            int_value: 5\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    downstream_nodes: \"DataTransformer\"\n",
      "    downstream_nodes: \"StatisticsGen\"\n",
      "    execution_options {\n",
      "      caching_options {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "nodes {\n",
      "  pipeline_node {\n",
      "    node_info {\n",
      "      type {\n",
      "        name: \"tfx.dsl.components.common.resolver.Resolver\"\n",
      "      }\n",
      "      id: \"WarmstartModelResolver\"\n",
      "    }\n",
      "    contexts {\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline_run\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"2022-03-30T14:44:39.880285\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"node\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.WarmstartModelResolver\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    inputs {\n",
      "      inputs {\n",
      "        key: \"latest_model\"\n",
      "        value {\n",
      "          channels {\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"Model\"\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      resolver_config {\n",
      "        resolver_steps {\n",
      "          class_path: \"tfx.dsl.input_resolution.strategies.latest_artifact_strategy.LatestArtifactStrategy\"\n",
      "          config_json: \"{}\"\n",
      "          input_keys: \"latest_model\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    downstream_nodes: \"ModelTrainer\"\n",
      "    execution_options {\n",
      "      caching_options {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "nodes {\n",
      "  pipeline_node {\n",
      "    node_info {\n",
      "      type {\n",
      "        name: \"tfx.components.statistics_gen.component.StatisticsGen\"\n",
      "      }\n",
      "      id: \"StatisticsGen\"\n",
      "    }\n",
      "    contexts {\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline_run\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"2022-03-30T14:44:39.880285\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"node\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.StatisticsGen\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    inputs {\n",
      "      inputs {\n",
      "        key: \"examples\"\n",
      "        value {\n",
      "          channels {\n",
      "            producer_node_query {\n",
      "              id: \"TrainDataGen\"\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline_run\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"2022-03-30T14:44:39.880285\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"node\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.TrainDataGen\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"Examples\"\n",
      "              }\n",
      "            }\n",
      "            output_key: \"examples\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    outputs {\n",
      "      outputs {\n",
      "        key: \"statistics\"\n",
      "        value {\n",
      "          artifact_spec {\n",
      "            type {\n",
      "              name: \"ExampleStatistics\"\n",
      "              properties {\n",
      "                key: \"span\"\n",
      "                value: INT\n",
      "              }\n",
      "              properties {\n",
      "                key: \"split_names\"\n",
      "                value: STRING\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    parameters {\n",
      "      parameters {\n",
      "        key: \"exclude_splits\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"[]\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    upstream_nodes: \"TrainDataGen\"\n",
      "    downstream_nodes: \"ExampleValidator\"\n",
      "    execution_options {\n",
      "      caching_options {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "nodes {\n",
      "  pipeline_node {\n",
      "    node_info {\n",
      "      type {\n",
      "        name: \"tfx.components.example_validator.component.ExampleValidator\"\n",
      "      }\n",
      "      id: \"ExampleValidator\"\n",
      "    }\n",
      "    contexts {\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline_run\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"2022-03-30T14:44:39.880285\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"node\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.ExampleValidator\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    inputs {\n",
      "      inputs {\n",
      "        key: \"schema\"\n",
      "        value {\n",
      "          channels {\n",
      "            producer_node_query {\n",
      "              id: \"SchemaImporter\"\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline_run\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"2022-03-30T14:44:39.880285\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"node\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.SchemaImporter\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"Schema\"\n",
      "              }\n",
      "            }\n",
      "            output_key: \"result\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      inputs {\n",
      "        key: \"statistics\"\n",
      "        value {\n",
      "          channels {\n",
      "            producer_node_query {\n",
      "              id: \"StatisticsGen\"\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline_run\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"2022-03-30T14:44:39.880285\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"node\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.StatisticsGen\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"ExampleStatistics\"\n",
      "              }\n",
      "            }\n",
      "            output_key: \"statistics\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    outputs {\n",
      "      outputs {\n",
      "        key: \"anomalies\"\n",
      "        value {\n",
      "          artifact_spec {\n",
      "            type {\n",
      "              name: \"ExampleAnomalies\"\n",
      "              properties {\n",
      "                key: \"span\"\n",
      "                value: INT\n",
      "              }\n",
      "              properties {\n",
      "                key: \"split_names\"\n",
      "                value: STRING\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    parameters {\n",
      "      parameters {\n",
      "        key: \"exclude_splits\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"[]\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    upstream_nodes: \"SchemaImporter\"\n",
      "    upstream_nodes: \"StatisticsGen\"\n",
      "    downstream_nodes: \"DataTransformer\"\n",
      "    execution_options {\n",
      "      caching_options {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "nodes {\n",
      "  pipeline_node {\n",
      "    node_info {\n",
      "      type {\n",
      "        name: \"tfx.components.transform.component.Transform\"\n",
      "      }\n",
      "      id: \"DataTransformer\"\n",
      "    }\n",
      "    contexts {\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline_run\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"2022-03-30T14:44:39.880285\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"node\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.DataTransformer\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    inputs {\n",
      "      inputs {\n",
      "        key: \"examples\"\n",
      "        value {\n",
      "          channels {\n",
      "            producer_node_query {\n",
      "              id: \"TrainDataGen\"\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline_run\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"2022-03-30T14:44:39.880285\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"node\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.TrainDataGen\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"Examples\"\n",
      "              }\n",
      "            }\n",
      "            output_key: \"examples\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      inputs {\n",
      "        key: \"schema\"\n",
      "        value {\n",
      "          channels {\n",
      "            producer_node_query {\n",
      "              id: \"SchemaImporter\"\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline_run\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"2022-03-30T14:44:39.880285\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"node\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.SchemaImporter\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"Schema\"\n",
      "              }\n",
      "            }\n",
      "            output_key: \"result\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    outputs {\n",
      "      outputs {\n",
      "        key: \"post_transform_anomalies\"\n",
      "        value {\n",
      "          artifact_spec {\n",
      "            type {\n",
      "              name: \"ExampleAnomalies\"\n",
      "              properties {\n",
      "                key: \"span\"\n",
      "                value: INT\n",
      "              }\n",
      "              properties {\n",
      "                key: \"split_names\"\n",
      "                value: STRING\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      outputs {\n",
      "        key: \"post_transform_schema\"\n",
      "        value {\n",
      "          artifact_spec {\n",
      "            type {\n",
      "              name: \"Schema\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      outputs {\n",
      "        key: \"post_transform_stats\"\n",
      "        value {\n",
      "          artifact_spec {\n",
      "            type {\n",
      "              name: \"ExampleStatistics\"\n",
      "              properties {\n",
      "                key: \"span\"\n",
      "                value: INT\n",
      "              }\n",
      "              properties {\n",
      "                key: \"split_names\"\n",
      "                value: STRING\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      outputs {\n",
      "        key: \"pre_transform_schema\"\n",
      "        value {\n",
      "          artifact_spec {\n",
      "            type {\n",
      "              name: \"Schema\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      outputs {\n",
      "        key: \"pre_transform_stats\"\n",
      "        value {\n",
      "          artifact_spec {\n",
      "            type {\n",
      "              name: \"ExampleStatistics\"\n",
      "              properties {\n",
      "                key: \"span\"\n",
      "                value: INT\n",
      "              }\n",
      "              properties {\n",
      "                key: \"split_names\"\n",
      "                value: STRING\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      outputs {\n",
      "        key: \"transform_graph\"\n",
      "        value {\n",
      "          artifact_spec {\n",
      "            type {\n",
      "              name: \"TransformGraph\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      outputs {\n",
      "        key: \"transformed_examples\"\n",
      "        value {\n",
      "          artifact_spec {\n",
      "            type {\n",
      "              name: \"Examples\"\n",
      "              properties {\n",
      "                key: \"span\"\n",
      "                value: INT\n",
      "              }\n",
      "              properties {\n",
      "                key: \"split_names\"\n",
      "                value: STRING\n",
      "              }\n",
      "              properties {\n",
      "                key: \"version\"\n",
      "                value: INT\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      outputs {\n",
      "        key: \"updated_analyzer_cache\"\n",
      "        value {\n",
      "          artifact_spec {\n",
      "            type {\n",
      "              name: \"TransformCache\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    parameters {\n",
      "      parameters {\n",
      "        key: \"custom_config\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"null\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      parameters {\n",
      "        key: \"disable_statistics\"\n",
      "        value {\n",
      "          field_value {\n",
      "            int_value: 0\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      parameters {\n",
      "        key: \"force_tf_compat_v1\"\n",
      "        value {\n",
      "          field_value {\n",
      "            int_value: 0\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      parameters {\n",
      "        key: \"module_path\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"transformations@gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/_wheels/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      parameters {\n",
      "        key: \"splits_config\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"{\\n  \\\"analyze\\\": [\\n    \\\"train\\\"\\n  ],\\n  \\\"transform\\\": [\\n    \\\"train\\\",\\n    \\\"eval\\\"\\n  ]\\n}\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    upstream_nodes: \"ExampleValidator\"\n",
      "    upstream_nodes: \"SchemaImporter\"\n",
      "    upstream_nodes: \"TrainDataGen\"\n",
      "    downstream_nodes: \"ModelTrainer\"\n",
      "    execution_options {\n",
      "      caching_options {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "nodes {\n",
      "  pipeline_node {\n",
      "    node_info {\n",
      "      type {\n",
      "        name: \"tfx.components.trainer.component.Trainer\"\n",
      "      }\n",
      "      id: \"ModelTrainer\"\n",
      "    }\n",
      "    contexts {\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline_run\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"2022-03-30T14:44:39.880285\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"node\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.ModelTrainer\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    inputs {\n",
      "      inputs {\n",
      "        key: \"base_model\"\n",
      "        value {\n",
      "          channels {\n",
      "            producer_node_query {\n",
      "              id: \"WarmstartModelResolver\"\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline_run\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"2022-03-30T14:44:39.880285\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"node\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.WarmstartModelResolver\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"Model\"\n",
      "              }\n",
      "            }\n",
      "            output_key: \"latest_model\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      inputs {\n",
      "        key: \"examples\"\n",
      "        value {\n",
      "          channels {\n",
      "            producer_node_query {\n",
      "              id: \"DataTransformer\"\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline_run\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"2022-03-30T14:44:39.880285\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"node\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.DataTransformer\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"Examples\"\n",
      "              }\n",
      "            }\n",
      "            output_key: \"transformed_examples\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      inputs {\n",
      "        key: \"hyperparameters\"\n",
      "        value {\n",
      "          channels {\n",
      "            producer_node_query {\n",
      "              id: \"HyperparamsGen\"\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline_run\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"2022-03-30T14:44:39.880285\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"node\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.HyperparamsGen\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"HyperParameters\"\n",
      "              }\n",
      "            }\n",
      "            output_key: \"hyperparameters\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      inputs {\n",
      "        key: \"schema\"\n",
      "        value {\n",
      "          channels {\n",
      "            producer_node_query {\n",
      "              id: \"SchemaImporter\"\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline_run\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"2022-03-30T14:44:39.880285\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"node\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.SchemaImporter\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"Schema\"\n",
      "              }\n",
      "            }\n",
      "            output_key: \"result\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      inputs {\n",
      "        key: \"transform_graph\"\n",
      "        value {\n",
      "          channels {\n",
      "            producer_node_query {\n",
      "              id: \"DataTransformer\"\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline_run\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"2022-03-30T14:44:39.880285\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"node\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.DataTransformer\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"TransformGraph\"\n",
      "              }\n",
      "            }\n",
      "            output_key: \"transform_graph\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    outputs {\n",
      "      outputs {\n",
      "        key: \"model\"\n",
      "        value {\n",
      "          artifact_spec {\n",
      "            type {\n",
      "              name: \"Model\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      outputs {\n",
      "        key: \"model_run\"\n",
      "        value {\n",
      "          artifact_spec {\n",
      "            type {\n",
      "              name: \"ModelRun\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    parameters {\n",
      "      parameters {\n",
      "        key: \"custom_config\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"null\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      parameters {\n",
      "        key: \"eval_args\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"{}\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      parameters {\n",
      "        key: \"module_path\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"runner@gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/_wheels/tfx_user_code_ModelTrainer-0.0+45151d477b837ba3e9c07eca0c4beadd51fa08b10afc0503d6ce3837f470c259-py3-none-any.whl\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      parameters {\n",
      "        key: \"train_args\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"{}\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    upstream_nodes: \"DataTransformer\"\n",
      "    upstream_nodes: \"HyperparamsGen\"\n",
      "    upstream_nodes: \"SchemaImporter\"\n",
      "    upstream_nodes: \"WarmstartModelResolver\"\n",
      "    downstream_nodes: \"ModelEvaluator\"\n",
      "    downstream_nodes: \"ModelPusher\"\n",
      "    execution_options {\n",
      "      caching_options {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "nodes {\n",
      "  pipeline_node {\n",
      "    node_info {\n",
      "      type {\n",
      "        name: \"tfx.components.evaluator.component.Evaluator\"\n",
      "      }\n",
      "      id: \"ModelEvaluator\"\n",
      "    }\n",
      "    contexts {\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline_run\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"2022-03-30T14:44:39.880285\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"node\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.ModelEvaluator\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    inputs {\n",
      "      inputs {\n",
      "        key: \"baseline_model\"\n",
      "        value {\n",
      "          channels {\n",
      "            producer_node_query {\n",
      "              id: \"BaselineModelResolver\"\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline_run\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"2022-03-30T14:44:39.880285\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"node\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.BaselineModelResolver\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"Model\"\n",
      "              }\n",
      "            }\n",
      "            output_key: \"model\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      inputs {\n",
      "        key: \"examples\"\n",
      "        value {\n",
      "          channels {\n",
      "            producer_node_query {\n",
      "              id: \"TestDataGen\"\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline_run\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"2022-03-30T14:44:39.880285\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"node\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.TestDataGen\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"Examples\"\n",
      "              }\n",
      "            }\n",
      "            output_key: \"examples\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      inputs {\n",
      "        key: \"model\"\n",
      "        value {\n",
      "          channels {\n",
      "            producer_node_query {\n",
      "              id: \"ModelTrainer\"\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline_run\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"2022-03-30T14:44:39.880285\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"node\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.ModelTrainer\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"Model\"\n",
      "              }\n",
      "            }\n",
      "            output_key: \"model\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      inputs {\n",
      "        key: \"schema\"\n",
      "        value {\n",
      "          channels {\n",
      "            producer_node_query {\n",
      "              id: \"SchemaImporter\"\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline_run\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"2022-03-30T14:44:39.880285\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"node\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.SchemaImporter\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"Schema\"\n",
      "              }\n",
      "            }\n",
      "            output_key: \"result\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    outputs {\n",
      "      outputs {\n",
      "        key: \"blessing\"\n",
      "        value {\n",
      "          artifact_spec {\n",
      "            type {\n",
      "              name: \"ModelBlessing\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      outputs {\n",
      "        key: \"evaluation\"\n",
      "        value {\n",
      "          artifact_spec {\n",
      "            type {\n",
      "              name: \"ModelEvaluation\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    parameters {\n",
      "      parameters {\n",
      "        key: \"eval_config\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"{\\n  \\\"metrics_specs\\\": [\\n    {\\n      \\\"metrics\\\": [\\n        {\\n          \\\"class_name\\\": \\\"ExampleCount\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"BinaryAccuracy\\\",\\n          \\\"threshold\\\": {\\n            \\\"change_threshold\\\": {\\n              \\\"absolute\\\": -1e-10,\\n              \\\"direction\\\": \\\"HIGHER_IS_BETTER\\\"\\n            },\\n            \\\"value_threshold\\\": {\\n              \\\"lower_bound\\\": 0.1\\n            }\\n          }\\n        }\\n      ]\\n    }\\n  ],\\n  \\\"model_specs\\\": [\\n    {\\n      \\\"label_key\\\": \\\"tip_bin\\\",\\n      \\\"prediction_key\\\": \\\"probabilities\\\",\\n      \\\"signature_name\\\": \\\"serving_tf_example\\\"\\n    }\\n  ],\\n  \\\"slicing_specs\\\": [\\n    {}\\n  ]\\n}\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      parameters {\n",
      "        key: \"example_splits\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"[\\\"test\\\"]\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      parameters {\n",
      "        key: \"fairness_indicator_thresholds\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"null\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    upstream_nodes: \"BaselineModelResolver\"\n",
      "    upstream_nodes: \"ModelTrainer\"\n",
      "    upstream_nodes: \"SchemaImporter\"\n",
      "    upstream_nodes: \"TestDataGen\"\n",
      "    downstream_nodes: \"ModelPusher\"\n",
      "    execution_options {\n",
      "      caching_options {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "nodes {\n",
      "  pipeline_node {\n",
      "    node_info {\n",
      "      type {\n",
      "        name: \"tfx.components.pusher.component.Pusher\"\n",
      "      }\n",
      "      id: \"ModelPusher\"\n",
      "    }\n",
      "    contexts {\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline_run\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"2022-03-30T14:44:39.880285\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"node\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.ModelPusher\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    inputs {\n",
      "      inputs {\n",
      "        key: \"model\"\n",
      "        value {\n",
      "          channels {\n",
      "            producer_node_query {\n",
      "              id: \"ModelTrainer\"\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline_run\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"2022-03-30T14:44:39.880285\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"node\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.ModelTrainer\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"Model\"\n",
      "              }\n",
      "            }\n",
      "            output_key: \"model\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      inputs {\n",
      "        key: \"model_blessing\"\n",
      "        value {\n",
      "          channels {\n",
      "            producer_node_query {\n",
      "              id: \"ModelEvaluator\"\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline_run\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"2022-03-30T14:44:39.880285\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"node\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.ModelEvaluator\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"ModelBlessing\"\n",
      "              }\n",
      "            }\n",
      "            output_key: \"blessing\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    outputs {\n",
      "      outputs {\n",
      "        key: \"pushed_model\"\n",
      "        value {\n",
      "          artifact_spec {\n",
      "            type {\n",
      "              name: \"PushedModel\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    parameters {\n",
      "      parameters {\n",
      "        key: \"custom_config\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"null\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      parameters {\n",
      "        key: \"push_destination\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"{\\n  \\\"filesystem\\\": {\\n    \\\"base_directory\\\": \\\"gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/model_registry/chicago-taxi-tips-classifier-v01\\\"\\n  }\\n}\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    upstream_nodes: \"ModelEvaluator\"\n",
      "    upstream_nodes: \"ModelTrainer\"\n",
      "    execution_options {\n",
      "      caching_options {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "runtime_spec {\n",
      "  pipeline_root {\n",
      "    field_value {\n",
      "      string_value: \"gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "    }\n",
      "  }\n",
      "  pipeline_run_id {\n",
      "    field_value {\n",
      "      string_value: \"2022-03-30T14:44:39.880285\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "execution_mode: SYNC\n",
      "deployment_config {\n",
      "  type_url: \"type.googleapis.com/tfx.orchestration.IntermediateDeploymentConfig\"\n",
      "  value: \"\\n\\223\\002\\n\\017DataTransformer\\022\\377\\001\\nHtype.googleapis.com/tfx.orchestration.executable_spec.BeamExecutableSpec\\022\\262\\001\\n,\\n*tfx.components.transform.executor.Executor\\022\\037--project=grandelli-demo-295810\\022a--temp_location=gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/temp\\n\\226\\002\\n\\rStatisticsGen\\022\\204\\002\\nHtype.googleapis.com/tfx.orchestration.executable_spec.BeamExecutableSpec\\022\\267\\001\\n1\\n/tfx.components.statistics_gen.executor.Executor\\022\\037--project=grandelli-demo-295810\\022a--temp_location=gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/temp\\n\\213\\001\\n\\013ModelPusher\\022|\\nOtype.googleapis.com/tfx.orchestration.executable_spec.PythonClassExecutableSpec\\022)\\n\\'tfx.components.pusher.executor.Executor\\n\\234\\001\\n\\020ExampleValidator\\022\\207\\001\\nOtype.googleapis.com/tfx.orchestration.executable_spec.PythonClassExecutableSpec\\0224\\n2tfx.components.example_validator.executor.Executor\\n\\241\\001\\n\\016HyperparamsGen\\022\\216\\001\\nOtype.googleapis.com/tfx.orchestration.executable_spec.PythonClassExecutableSpec\\022;\\n9src.tfx_pipelines.components.hyperparameters_gen_Executor\\n\\222\\002\\n\\016ModelEvaluator\\022\\377\\001\\nHtype.googleapis.com/tfx.orchestration.executable_spec.BeamExecutableSpec\\022\\262\\001\\n,\\n*tfx.components.evaluator.executor.Executor\\022\\037--project=grandelli-demo-295810\\022a--temp_location=gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/temp\\n\\250\\002\\n\\013TestDataGen\\022\\230\\002\\nHtype.googleapis.com/tfx.orchestration.executable_spec.BeamExecutableSpec\\022\\313\\001\\nE\\nCtfx.extensions.google_cloud_big_query.example_gen.executor.Executor\\022\\037--project=grandelli-demo-295810\\022a--temp_location=gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/temp\\n\\251\\002\\n\\014TrainDataGen\\022\\230\\002\\nHtype.googleapis.com/tfx.orchestration.executable_spec.BeamExecutableSpec\\022\\313\\001\\nE\\nCtfx.extensions.google_cloud_big_query.example_gen.executor.Executor\\022\\037--project=grandelli-demo-295810\\022a--temp_location=gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/temp\\n\\225\\001\\n\\014ModelTrainer\\022\\204\\001\\nOtype.googleapis.com/tfx.orchestration.executable_spec.PythonClassExecutableSpec\\0221\\n/tfx.components.trainer.executor.GenericExecutor\\022\\227\\001\\n\\013TestDataGen\\022\\207\\001\\nOtype.googleapis.com/tfx.orchestration.executable_spec.PythonClassExecutableSpec\\0224\\n2tfx.components.example_gen.driver.QueryBasedDriver\\022\\230\\001\\n\\014TrainDataGen\\022\\207\\001\\nOtype.googleapis.com/tfx.orchestration.executable_spec.PythonClassExecutableSpec\\0224\\n2tfx.components.example_gen.driver.QueryBasedDriver*F\\n0type.googleapis.com/ml_metadata.ConnectionConfig\\022\\022\\032\\020\\n\\014mlmd.sqllite\\020\\003\"\n",
      "}\n",
      "\n",
      "Using deployment config:\n",
      " executor_specs {\n",
      "  key: \"DataTransformer\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.components.transform.executor.Executor\"\n",
      "      }\n",
      "      beam_pipeline_args: \"--project=grandelli-demo-295810\"\n",
      "      beam_pipeline_args: \"--temp_location=gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/temp\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"ExampleValidator\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.example_validator.executor.Executor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"HyperparamsGen\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"src.tfx_pipelines.components.hyperparameters_gen_Executor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"ModelEvaluator\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.components.evaluator.executor.Executor\"\n",
      "      }\n",
      "      beam_pipeline_args: \"--project=grandelli-demo-295810\"\n",
      "      beam_pipeline_args: \"--temp_location=gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/temp\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"ModelPusher\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.pusher.executor.Executor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"ModelTrainer\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.trainer.executor.GenericExecutor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"StatisticsGen\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.components.statistics_gen.executor.Executor\"\n",
      "      }\n",
      "      beam_pipeline_args: \"--project=grandelli-demo-295810\"\n",
      "      beam_pipeline_args: \"--temp_location=gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/temp\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"TestDataGen\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.extensions.google_cloud_big_query.example_gen.executor.Executor\"\n",
      "      }\n",
      "      beam_pipeline_args: \"--project=grandelli-demo-295810\"\n",
      "      beam_pipeline_args: \"--temp_location=gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/temp\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"TrainDataGen\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.extensions.google_cloud_big_query.example_gen.executor.Executor\"\n",
      "      }\n",
      "      beam_pipeline_args: \"--project=grandelli-demo-295810\"\n",
      "      beam_pipeline_args: \"--temp_location=gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/temp\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "custom_driver_specs {\n",
      "  key: \"TestDataGen\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.example_gen.driver.QueryBasedDriver\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "custom_driver_specs {\n",
      "  key: \"TrainDataGen\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.example_gen.driver.QueryBasedDriver\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "metadata_connection_config {\n",
      "  sqlite {\n",
      "    filename_uri: \"mlmd.sqllite\"\n",
      "    connection_mode: READWRITE_OPENCREATE\n",
      "  }\n",
      "}\n",
      "\n",
      "Using connection config:\n",
      " sqlite {\n",
      "  filename_uri: \"mlmd.sqllite\"\n",
      "  connection_mode: READWRITE_OPENCREATE\n",
      "}\n",
      "\n",
      "Component BaselineModelResolver is running.\n",
      "Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.dsl.components.common.resolver.Resolver\"\n",
      "  }\n",
      "  id: \"BaselineModelResolver\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-03-30T14:44:39.880285\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.BaselineModelResolver\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      channels {\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"model_blessing\"\n",
      "    value {\n",
      "      channels {\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"ModelBlessing\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  resolver_config {\n",
      "    resolver_steps {\n",
      "      class_path: \"tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy.LatestBlessedModelStrategy\"\n",
      "      config_json: \"{}\"\n",
      "      input_keys: \"model\"\n",
      "      input_keys: \"model_blessing\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"ModelEvaluator\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "Running as an resolver node.\n",
      "MetadataStore with DB connection initialized\n",
      "WARNING: Logging before InitGoogleLogging() is written to STDERR\n",
      "I0330 14:44:39.943449   104 rdbms_metadata_access_object.cc:686] No property is defined for the Type\n",
      "I0330 14:44:40.010934   104 rdbms_metadata_access_object.cc:686] No property is defined for the Type\n",
      "I0330 14:44:40.038018   104 rdbms_metadata_access_object.cc:686] No property is defined for the Type\n",
      "Artifact type Model is not found in MLMD.\n",
      "Artifact type ModelBlessing is not found in MLMD.\n",
      "I0330 14:44:40.067982   104 rdbms_metadata_access_object.cc:686] No property is defined for the Type\n",
      "Component BaselineModelResolver is finished.\n",
      "Component HyperparamsGen is running.\n",
      "Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"src.tfx_pipelines.components.hyperparameters_gen\"\n",
      "  }\n",
      "  id: \"HyperparamsGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-03-30T14:44:39.880285\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.HyperparamsGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"hyperparameters\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"HyperParameters\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"batch_size\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 512\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"hidden_units\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"128,128\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"learning_rate\"\n",
      "    value {\n",
      "      field_value {\n",
      "        double_value: 0.001\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"num_epochs\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 1\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"ModelTrainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "MetadataStore with DB connection initialized\n",
      "I0330 14:44:40.120016   104 rdbms_metadata_access_object.cc:686] No property is defined for the Type\n",
      "MetadataStore with DB connection initialized\n",
      "I0330 14:44:40.159232   104 rdbms_metadata_access_object.cc:686] No property is defined for the Type\n",
      "Going to run a new execution 2\n",
      "Going to run a new execution: ExecutionInfo(execution_id=2, input_dict={}, output_dict=defaultdict(<class 'list'>, {'hyperparameters': [Artifact(artifact: uri: \"gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/HyperparamsGen/hyperparameters/2\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline:2022-03-30T14:44:39.880285:HyperparamsGen:hyperparameters:0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"HyperParameters\"\n",
      ")]}), exec_properties={'num_epochs': 1, 'learning_rate': 0.001, 'batch_size': 512, 'hidden_units': '128,128'}, execution_output_uri='gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/HyperparamsGen/.system/executor_execution/2/executor_output.pb', stateful_working_dir='gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/HyperparamsGen/.system/stateful_working_dir/2022-03-30T14:44:39.880285', tmp_dir='gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/HyperparamsGen/.system/executor_execution/2/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"src.tfx_pipelines.components.hyperparameters_gen\"\n",
      "  }\n",
      "  id: \"HyperparamsGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-03-30T14:44:39.880285\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.HyperparamsGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"hyperparameters\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"HyperParameters\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"batch_size\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 512\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"hidden_units\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"128,128\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"learning_rate\"\n",
      "    value {\n",
      "      field_value {\n",
      "        double_value: 0.001\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"num_epochs\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 1\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"ModelTrainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      ", pipeline_run_id='2022-03-30T14:44:39.880285')\n",
      "Hyperparameters: {'num_epochs': 1, 'batch_size': 512, 'learning_rate': 0.001, 'hidden_units': [128, 128]}\n",
      "Hyperparameters are written to: gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/HyperparamsGen/hyperparameters/2/hyperparameters.json\n",
      "Cleaning up stateless execution info.\n",
      "Execution 2 succeeded.\n",
      "Cleaning up stateful execution info.\n",
      "stateful_working_dir /home/jupyter/20220318_Training/gcp-partner-training-mlops/gs:/grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/HyperparamsGen/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Publishing output artifacts defaultdict(<class 'list'>, {'hyperparameters': [Artifact(artifact: uri: \"gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/HyperparamsGen/hyperparameters/2\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline:2022-03-30T14:44:39.880285:HyperparamsGen:hyperparameters:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.2.0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"HyperParameters\"\n",
      ")]}) for execution 2\n",
      "MetadataStore with DB connection initialized\n",
      "I0330 14:44:43.770259   104 rdbms_metadata_access_object.cc:686] No property is defined for the Type\n",
      "Component HyperparamsGen is finished.\n",
      "Component SchemaImporter is running.\n",
      "Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.dsl.components.common.importer.Importer\"\n",
      "  }\n",
      "  id: \"SchemaImporter\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-03-30T14:44:39.880285\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.SchemaImporter\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"result\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Schema\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"artifact_uri\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"src/raw_schema\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"reimport\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"DataTransformer\"\n",
      "downstream_nodes: \"ExampleValidator\"\n",
      "downstream_nodes: \"ModelEvaluator\"\n",
      "downstream_nodes: \"ModelTrainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "Running as an importer node.\n",
      "MetadataStore with DB connection initialized\n",
      "I0330 14:44:43.856077   104 rdbms_metadata_access_object.cc:686] No property is defined for the Type\n",
      "Processing source uri: src/raw_schema, properties: {}, custom_properties: {}\n",
      "I0330 14:44:43.918529   104 rdbms_metadata_access_object.cc:686] No property is defined for the Type\n",
      "Component SchemaImporter is finished.\n",
      "Component TestDataGen is running.\n",
      "Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.extensions.google_cloud_big_query.example_gen.component.BigQueryExampleGen\"\n",
      "  }\n",
      "  id: \"TestDataGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-03-30T14:44:39.880285\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.TestDataGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"input_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"\\\\n    SELECT \\\\n        IF(trip_month IS NULL, -1, trip_month) trip_month,\\\\n        IF(trip_day IS NULL, -1, trip_day) trip_day,\\\\n        IF(trip_day_of_week IS NULL, -1, trip_day_of_week) trip_day_of_week,\\\\n        IF(trip_hour IS NULL, -1, trip_hour) trip_hour,\\\\n        IF(trip_seconds IS NULL, -1, trip_seconds) trip_seconds,\\\\n        IF(trip_miles IS NULL, -1, trip_miles) trip_miles,\\\\n        IF(payment_type IS NULL, \\'NA\\', payment_type) payment_type,\\\\n        IF(pickup_grid IS NULL, \\'NA\\', pickup_grid) pickup_grid,\\\\n        IF(dropoff_grid IS NULL, \\'NA\\', dropoff_grid) dropoff_grid,\\\\n        IF(euclidean IS NULL, -1, euclidean) euclidean,\\\\n        IF(loc_cross IS NULL, \\'NA\\', loc_cross) loc_cross,\\\\n        tip_bin\\\\n    FROM partner_training.chicago_taxitrips_prep \\\\n    WHERE ML_use = \\'TEST\\'\\\\n    LIMIT 100\\\"\\n    }\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"test\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_data_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 6\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_file_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 5\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"ModelEvaluator\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "MetadataStore with DB connection initialized\n",
      "I0330 14:44:43.983976   104 rdbms_metadata_access_object.cc:686] No property is defined for the Type\n",
      "MetadataStore with DB connection initialized\n",
      "Going to run a new execution 4\n",
      "Going to run a new execution: ExecutionInfo(execution_id=4, input_dict={}, output_dict=defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/TestDataGen/examples/4\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline:2022-03-30T14:44:39.880285:TestDataGen:examples:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      ")]}), exec_properties={'output_data_format': 6, 'input_config': '{\\n  \"splits\": [\\n    {\\n      \"name\": \"single_split\",\\n      \"pattern\": \"\\\\n    SELECT \\\\n        IF(trip_month IS NULL, -1, trip_month) trip_month,\\\\n        IF(trip_day IS NULL, -1, trip_day) trip_day,\\\\n        IF(trip_day_of_week IS NULL, -1, trip_day_of_week) trip_day_of_week,\\\\n        IF(trip_hour IS NULL, -1, trip_hour) trip_hour,\\\\n        IF(trip_seconds IS NULL, -1, trip_seconds) trip_seconds,\\\\n        IF(trip_miles IS NULL, -1, trip_miles) trip_miles,\\\\n        IF(payment_type IS NULL, \\'NA\\', payment_type) payment_type,\\\\n        IF(pickup_grid IS NULL, \\'NA\\', pickup_grid) pickup_grid,\\\\n        IF(dropoff_grid IS NULL, \\'NA\\', dropoff_grid) dropoff_grid,\\\\n        IF(euclidean IS NULL, -1, euclidean) euclidean,\\\\n        IF(loc_cross IS NULL, \\'NA\\', loc_cross) loc_cross,\\\\n        tip_bin\\\\n    FROM partner_training.chicago_taxitrips_prep \\\\n    WHERE ML_use = \\'TEST\\'\\\\n    LIMIT 100\"\\n    }\\n  ]\\n}', 'output_config': '{\\n  \"split_config\": {\\n    \"splits\": [\\n      {\\n        \"hash_buckets\": 1,\\n        \"name\": \"test\"\\n      }\\n    ]\\n  }\\n}', 'output_file_format': 5, 'span': 0, 'version': None, 'input_fingerprint': None}, execution_output_uri='gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/TestDataGen/.system/executor_execution/4/executor_output.pb', stateful_working_dir='gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/TestDataGen/.system/stateful_working_dir/2022-03-30T14:44:39.880285', tmp_dir='gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/TestDataGen/.system/executor_execution/4/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.extensions.google_cloud_big_query.example_gen.component.BigQueryExampleGen\"\n",
      "  }\n",
      "  id: \"TestDataGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-03-30T14:44:39.880285\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.TestDataGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"input_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"\\\\n    SELECT \\\\n        IF(trip_month IS NULL, -1, trip_month) trip_month,\\\\n        IF(trip_day IS NULL, -1, trip_day) trip_day,\\\\n        IF(trip_day_of_week IS NULL, -1, trip_day_of_week) trip_day_of_week,\\\\n        IF(trip_hour IS NULL, -1, trip_hour) trip_hour,\\\\n        IF(trip_seconds IS NULL, -1, trip_seconds) trip_seconds,\\\\n        IF(trip_miles IS NULL, -1, trip_miles) trip_miles,\\\\n        IF(payment_type IS NULL, \\'NA\\', payment_type) payment_type,\\\\n        IF(pickup_grid IS NULL, \\'NA\\', pickup_grid) pickup_grid,\\\\n        IF(dropoff_grid IS NULL, \\'NA\\', dropoff_grid) dropoff_grid,\\\\n        IF(euclidean IS NULL, -1, euclidean) euclidean,\\\\n        IF(loc_cross IS NULL, \\'NA\\', loc_cross) loc_cross,\\\\n        tip_bin\\\\n    FROM partner_training.chicago_taxitrips_prep \\\\n    WHERE ML_use = \\'TEST\\'\\\\n    LIMIT 100\\\"\\n    }\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"test\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_data_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 6\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_file_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 5\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"ModelEvaluator\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      ", pipeline_run_id='2022-03-30T14:44:39.880285')\n",
      "Attempting to infer TFX Python dependency for beam\n",
      "Copying all content from install dir /home/jupyter/.local/lib/python3.7/site-packages/tfx to temp dir /tmp/tmpqebbutz8/build/tfx\n",
      "Generating a temp setup file at /tmp/tmpqebbutz8/build/tfx/setup.py\n",
      "Creating temporary sdist package, logs available at /tmp/tmpqebbutz8/build/tfx/setup.log\n",
      "E0330 14:44:48.070832695     104 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "Added --extra_package=/tmp/tmpqebbutz8/build/tfx/dist/tfx_ephemeral-1.2.0.tar.gz to beam args\n",
      "Length of label `tfx-extensions-google_cloud_big_query-example_gen-executor-executor` exceeds maximum length(63), trimmed.\n",
      "Generating examples.\n",
      "Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "E0330 14:44:49.686154148     104 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "Default Python SDK image for environment is apache/beam_python3.7_sdk:2.34.0\n",
      "==================== <function annotate_downstream_side_inputs at 0x7f8505d1f4d0> ====================\n",
      "==================== <function fix_side_input_pcoll_coders at 0x7f8505d1f5f0> ====================\n",
      "==================== <function pack_combiners at 0x7f8505d1fb00> ====================\n",
      "==================== <function lift_combiners at 0x7f8505d1fb90> ====================\n",
      "==================== <function expand_sdf at 0x7f8505d1fd40> ====================\n",
      "==================== <function expand_gbk at 0x7f8505d1fdd0> ====================\n",
      "==================== <function sink_flattens at 0x7f8505d1fef0> ====================\n",
      "==================== <function greedily_fuse at 0x7f8505d1ff80> ====================\n",
      "==================== <function read_to_impulse at 0x7f8505d1e050> ====================\n",
      "==================== <function impulse_to_input at 0x7f8505d1e0e0> ====================\n",
      "==================== <function sort_stages at 0x7f8505d1e320> ====================\n",
      "==================== <function setup_timer_mapping at 0x7f8505d1e290> ====================\n",
      "==================== <function populate_data_channel_coders at 0x7f8505d1e3b0> ====================\n",
      "Creating state cache with size 100\n",
      "Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7f8503173c10> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
      "Running ((((ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-Read-Impulse_12)+(ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-Read-Map-lambda-at-iobase-py-898-_13))+(InputToRecord/QueryTable/ReadFromBigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(InputToRecord/QueryTable/ReadFromBigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)\n",
      "Setting socket default timeout to 60 seconds.\n",
      "socket default timeout is 60.0 seconds.\n",
      "E0330 14:44:52.729236708     104 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "E0330 14:44:54.497502435     104 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "Attempting refresh to obtain initial access_token\n",
      "Refreshing access_token\n",
      "Started BigQuery job: <JobReference\n",
      " location: 'US'\n",
      " projectId: 'grandelli-demo-295810'>\n",
      " bq show -j --format=prettyjson --project_id=grandelli-demo-295810 None\n",
      "E0330 14:44:56.624755347     104 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "E0330 14:44:58.439492728     104 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "Using location 'US' from table <TableReference\n",
      " datasetId: 'partner_training'\n",
      " projectId: 'grandelli-demo-295810'\n",
      " tableId: 'chicago_taxitrips_prep'> referenced by query \n",
      "    SELECT \n",
      "        IF(trip_month IS NULL, -1, trip_month) trip_month,\n",
      "        IF(trip_day IS NULL, -1, trip_day) trip_day,\n",
      "        IF(trip_day_of_week IS NULL, -1, trip_day_of_week) trip_day_of_week,\n",
      "        IF(trip_hour IS NULL, -1, trip_hour) trip_hour,\n",
      "        IF(trip_seconds IS NULL, -1, trip_seconds) trip_seconds,\n",
      "        IF(trip_miles IS NULL, -1, trip_miles) trip_miles,\n",
      "        IF(payment_type IS NULL, 'NA', payment_type) payment_type,\n",
      "        IF(pickup_grid IS NULL, 'NA', pickup_grid) pickup_grid,\n",
      "        IF(dropoff_grid IS NULL, 'NA', dropoff_grid) dropoff_grid,\n",
      "        IF(euclidean IS NULL, -1, euclidean) euclidean,\n",
      "        IF(loc_cross IS NULL, 'NA', loc_cross) loc_cross,\n",
      "        tip_bin\n",
      "    FROM partner_training.chicago_taxitrips_prep \n",
      "    WHERE ML_use = 'TEST'\n",
      "    LIMIT 100\n",
      "Dataset grandelli-demo-295810:beam_temp_dataset_5c442d4539b24977b85b037f678a7997 does not exist so we will create it as temporary with location=US\n",
      "Started BigQuery job: <JobReference\n",
      " jobId: 'beam_bq_job_QUERY_BQ_EXPORT_JOB_bc0d7b6d-e_1648651501_462'\n",
      " location: 'US'\n",
      " projectId: 'grandelli-demo-295810'>\n",
      " bq show -j --format=prettyjson --project_id=grandelli-demo-295810 beam_bq_job_QUERY_BQ_EXPORT_JOB_bc0d7b6d-e_1648651501_462\n",
      "Job status: RUNNING\n",
      "Job status: DONE\n",
      "Started BigQuery job: <JobReference\n",
      " jobId: 'beam_bq_job_EXPORT_BQ_EXPORT_JOB_bc0d7b6d-e_1648651507_179'\n",
      " location: 'US'\n",
      " projectId: 'grandelli-demo-295810'>\n",
      " bq show -j --format=prettyjson --project_id=grandelli-demo-295810 beam_bq_job_EXPORT_BQ_EXPORT_JOB_bc0d7b6d-e_1648651507_179\n",
      "Job status: RUNNING\n",
      "Job status: DONE\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.048383235931396484 seconds.\n",
      "Running (((((((((ref_PCollection_PCollection_6_split/Read)+(InputToRecord/QueryTable/ReadFromBigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-_PassThroughThenCleanup-ParDo-PassTh_18))+(ref_AppliedPTransform_InputToRecord-ToTFExample_25))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_SplitData-ParDo-ApplyPartitionFnFn-ParDo-ApplyPartitionFnFn-_28))+(ref_AppliedPTransform_WriteSplit-test-MaybeSerialize_30))+(ref_AppliedPTransform_WriteSplit-test-Shuffle-AddRandomKeys_32))+(ref_AppliedPTransform_WriteSplit-test-Shuffle-ReshufflePerKey-Map-reify_timestamps-_34))+(WriteSplit[test]/Shuffle/ReshufflePerKey/GroupByKey/Write)\n",
      "Running (((((ref_AppliedPTransform_WriteSplit-test-Write-Write-WriteImpl-DoOnce-Impulse_42)+(ref_AppliedPTransform_WriteSplit-test-Write-Write-WriteImpl-DoOnce-FlatMap-lambda-at-core-py-3222-_43))+(ref_AppliedPTransform_WriteSplit-test-Write-Write-WriteImpl-DoOnce-Map-decode-_45))+(ref_AppliedPTransform_WriteSplit-test-Write-Write-WriteImpl-InitializeWrite_46))+(ref_PCollection_PCollection_25/Write))+(ref_PCollection_PCollection_26/Write)\n",
      "E0330 14:45:14.103952356     104 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "Running ((((((WriteSplit[test]/Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_WriteSplit-test-Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_36))+(ref_AppliedPTransform_WriteSplit-test-Shuffle-RemoveRandomKeys_37))+(ref_AppliedPTransform_WriteSplit-test-Write-Write-WriteImpl-WindowInto-WindowIntoFn-_47))+(ref_AppliedPTransform_WriteSplit-test-Write-Write-WriteImpl-WriteBundles_48))+(ref_AppliedPTransform_WriteSplit-test-Write-Write-WriteImpl-Pair_49))+(WriteSplit[test]/Write/Write/WriteImpl/GroupByKey/Write)\n",
      "Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n",
      "Running ((WriteSplit[test]/Write/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteSplit-test-Write-Write-WriteImpl-Extract_51))+(ref_PCollection_PCollection_31/Write)\n",
      "Running ((ref_PCollection_PCollection_25/Read)+(ref_AppliedPTransform_WriteSplit-test-Write-Write-WriteImpl-PreFinalize_52))+(ref_PCollection_PCollection_32/Write)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 0 files in 0.039159297943115234 seconds.\n",
      "Running (ref_PCollection_PCollection_25/Read)+(ref_AppliedPTransform_WriteSplit-test-Write-Write-WriteImpl-FinalizeWrite_53)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.04057574272155762 seconds.\n",
      "Starting the size estimation of the input\n",
      "Finished listing 0 files in 0.0395200252532959 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.40 seconds.\n",
      "Running ((((ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-FilesToRemoveImpulse-Impulse_6)+(ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-FilesToRemoveImpulse-FlatMap-lambda-_7))+(ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-FilesToRemoveImpulse-Map-decode-_9))+(ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-MapFilesToRemove_10))+(ref_PCollection_PCollection_4/Write)\n",
      "Running (((ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-_PassThroughThenCleanup-Create-Impul_20)+(ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-_PassThroughThenCleanup-Create-FlatM_21))+(ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-_PassThroughThenCleanup-Create-Map-d_23))+(ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-_PassThroughThenCleanup-ParDo-Remove_24)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.04318976402282715 seconds.\n",
      "Examples generated.\n",
      "Cleaning up stateless execution info.\n",
      "Execution 4 succeeded.\n",
      "Cleaning up stateful execution info.\n",
      "stateful_working_dir /home/jupyter/20220318_Training/gcp-partner-training-mlops/gs:/grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/TestDataGen/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Publishing output artifacts defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/TestDataGen/examples/4\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline:2022-03-30T14:44:39.880285:TestDataGen:examples:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.2.0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      ")]}) for execution 4\n",
      "MetadataStore with DB connection initialized\n",
      "Component TestDataGen is finished.\n",
      "Component TrainDataGen is running.\n",
      "Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.extensions.google_cloud_big_query.example_gen.component.BigQueryExampleGen\"\n",
      "  }\n",
      "  id: \"TrainDataGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-03-30T14:44:39.880285\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.TrainDataGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"input_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"\\\\n    SELECT \\\\n        IF(trip_month IS NULL, -1, trip_month) trip_month,\\\\n        IF(trip_day IS NULL, -1, trip_day) trip_day,\\\\n        IF(trip_day_of_week IS NULL, -1, trip_day_of_week) trip_day_of_week,\\\\n        IF(trip_hour IS NULL, -1, trip_hour) trip_hour,\\\\n        IF(trip_seconds IS NULL, -1, trip_seconds) trip_seconds,\\\\n        IF(trip_miles IS NULL, -1, trip_miles) trip_miles,\\\\n        IF(payment_type IS NULL, \\'NA\\', payment_type) payment_type,\\\\n        IF(pickup_grid IS NULL, \\'NA\\', pickup_grid) pickup_grid,\\\\n        IF(dropoff_grid IS NULL, \\'NA\\', dropoff_grid) dropoff_grid,\\\\n        IF(euclidean IS NULL, -1, euclidean) euclidean,\\\\n        IF(loc_cross IS NULL, \\'NA\\', loc_cross) loc_cross,\\\\n        tip_bin\\\\n    FROM partner_training.chicago_taxitrips_prep \\\\n    WHERE ML_use = \\'UNASSIGNED\\'\\\\n    LIMIT 1000\\\"\\n    }\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 4,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_data_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 6\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_file_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 5\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"DataTransformer\"\n",
      "downstream_nodes: \"StatisticsGen\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "MetadataStore with DB connection initialized\n",
      "MetadataStore with DB connection initialized\n",
      "Going to run a new execution 5\n",
      "Going to run a new execution: ExecutionInfo(execution_id=5, input_dict={}, output_dict=defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/TrainDataGen/examples/5\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline:2022-03-30T14:44:39.880285:TrainDataGen:examples:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      ")]}), exec_properties={'output_file_format': 5, 'output_config': '{\\n  \"split_config\": {\\n    \"splits\": [\\n      {\\n        \"hash_buckets\": 4,\\n        \"name\": \"train\"\\n      },\\n      {\\n        \"hash_buckets\": 1,\\n        \"name\": \"eval\"\\n      }\\n    ]\\n  }\\n}', 'output_data_format': 6, 'input_config': '{\\n  \"splits\": [\\n    {\\n      \"name\": \"single_split\",\\n      \"pattern\": \"\\\\n    SELECT \\\\n        IF(trip_month IS NULL, -1, trip_month) trip_month,\\\\n        IF(trip_day IS NULL, -1, trip_day) trip_day,\\\\n        IF(trip_day_of_week IS NULL, -1, trip_day_of_week) trip_day_of_week,\\\\n        IF(trip_hour IS NULL, -1, trip_hour) trip_hour,\\\\n        IF(trip_seconds IS NULL, -1, trip_seconds) trip_seconds,\\\\n        IF(trip_miles IS NULL, -1, trip_miles) trip_miles,\\\\n        IF(payment_type IS NULL, \\'NA\\', payment_type) payment_type,\\\\n        IF(pickup_grid IS NULL, \\'NA\\', pickup_grid) pickup_grid,\\\\n        IF(dropoff_grid IS NULL, \\'NA\\', dropoff_grid) dropoff_grid,\\\\n        IF(euclidean IS NULL, -1, euclidean) euclidean,\\\\n        IF(loc_cross IS NULL, \\'NA\\', loc_cross) loc_cross,\\\\n        tip_bin\\\\n    FROM partner_training.chicago_taxitrips_prep \\\\n    WHERE ML_use = \\'UNASSIGNED\\'\\\\n    LIMIT 1000\"\\n    }\\n  ]\\n}', 'span': 0, 'version': None, 'input_fingerprint': None}, execution_output_uri='gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/TrainDataGen/.system/executor_execution/5/executor_output.pb', stateful_working_dir='gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/TrainDataGen/.system/stateful_working_dir/2022-03-30T14:44:39.880285', tmp_dir='gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/TrainDataGen/.system/executor_execution/5/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.extensions.google_cloud_big_query.example_gen.component.BigQueryExampleGen\"\n",
      "  }\n",
      "  id: \"TrainDataGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-03-30T14:44:39.880285\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.TrainDataGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"input_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"\\\\n    SELECT \\\\n        IF(trip_month IS NULL, -1, trip_month) trip_month,\\\\n        IF(trip_day IS NULL, -1, trip_day) trip_day,\\\\n        IF(trip_day_of_week IS NULL, -1, trip_day_of_week) trip_day_of_week,\\\\n        IF(trip_hour IS NULL, -1, trip_hour) trip_hour,\\\\n        IF(trip_seconds IS NULL, -1, trip_seconds) trip_seconds,\\\\n        IF(trip_miles IS NULL, -1, trip_miles) trip_miles,\\\\n        IF(payment_type IS NULL, \\'NA\\', payment_type) payment_type,\\\\n        IF(pickup_grid IS NULL, \\'NA\\', pickup_grid) pickup_grid,\\\\n        IF(dropoff_grid IS NULL, \\'NA\\', dropoff_grid) dropoff_grid,\\\\n        IF(euclidean IS NULL, -1, euclidean) euclidean,\\\\n        IF(loc_cross IS NULL, \\'NA\\', loc_cross) loc_cross,\\\\n        tip_bin\\\\n    FROM partner_training.chicago_taxitrips_prep \\\\n    WHERE ML_use = \\'UNASSIGNED\\'\\\\n    LIMIT 1000\\\"\\n    }\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 4,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_data_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 6\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_file_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 5\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"DataTransformer\"\n",
      "downstream_nodes: \"StatisticsGen\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      ", pipeline_run_id='2022-03-30T14:44:39.880285')\n",
      "Attempting to infer TFX Python dependency for beam\n",
      "Copying all content from install dir /home/jupyter/.local/lib/python3.7/site-packages/tfx to temp dir /tmp/tmpyizyjsri/build/tfx\n",
      "Generating a temp setup file at /tmp/tmpyizyjsri/build/tfx/setup.py\n",
      "Creating temporary sdist package, logs available at /tmp/tmpyizyjsri/build/tfx/setup.log\n",
      "E0330 14:45:19.781332180     104 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "Added --extra_package=/tmp/tmpyizyjsri/build/tfx/dist/tfx_ephemeral-1.2.0.tar.gz to beam args\n",
      "Length of label `tfx-extensions-google_cloud_big_query-example_gen-executor-executor` exceeds maximum length(63), trimmed.\n",
      "Generating examples.\n",
      "Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "E0330 14:45:21.432897433     104 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "Default Python SDK image for environment is apache/beam_python3.7_sdk:2.34.0\n",
      "==================== <function annotate_downstream_side_inputs at 0x7f8505d1f4d0> ====================\n",
      "==================== <function fix_side_input_pcoll_coders at 0x7f8505d1f5f0> ====================\n",
      "==================== <function pack_combiners at 0x7f8505d1fb00> ====================\n",
      "==================== <function lift_combiners at 0x7f8505d1fb90> ====================\n",
      "==================== <function expand_sdf at 0x7f8505d1fd40> ====================\n",
      "==================== <function expand_gbk at 0x7f8505d1fdd0> ====================\n",
      "==================== <function sink_flattens at 0x7f8505d1fef0> ====================\n",
      "==================== <function greedily_fuse at 0x7f8505d1ff80> ====================\n",
      "==================== <function read_to_impulse at 0x7f8505d1e050> ====================\n",
      "==================== <function impulse_to_input at 0x7f8505d1e0e0> ====================\n",
      "==================== <function sort_stages at 0x7f8505d1e320> ====================\n",
      "==================== <function setup_timer_mapping at 0x7f8505d1e290> ====================\n",
      "==================== <function populate_data_channel_coders at 0x7f8505d1e3b0> ====================\n",
      "Creating state cache with size 100\n",
      "Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7f8502138f10> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
      "Running ((((ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-Read-Impulse_12)+(ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-Read-Map-lambda-at-iobase-py-898-_13))+(InputToRecord/QueryTable/ReadFromBigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(InputToRecord/QueryTable/ReadFromBigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)\n",
      "E0330 14:45:25.035354697     104 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "E0330 14:45:26.823582748     104 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "Started BigQuery job: <JobReference\n",
      " location: 'US'\n",
      " projectId: 'grandelli-demo-295810'>\n",
      " bq show -j --format=prettyjson --project_id=grandelli-demo-295810 None\n",
      "E0330 14:45:29.037045581     104 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "E0330 14:45:30.883942884     104 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "Using location 'US' from table <TableReference\n",
      " datasetId: 'partner_training'\n",
      " projectId: 'grandelli-demo-295810'\n",
      " tableId: 'chicago_taxitrips_prep'> referenced by query \n",
      "    SELECT \n",
      "        IF(trip_month IS NULL, -1, trip_month) trip_month,\n",
      "        IF(trip_day IS NULL, -1, trip_day) trip_day,\n",
      "        IF(trip_day_of_week IS NULL, -1, trip_day_of_week) trip_day_of_week,\n",
      "        IF(trip_hour IS NULL, -1, trip_hour) trip_hour,\n",
      "        IF(trip_seconds IS NULL, -1, trip_seconds) trip_seconds,\n",
      "        IF(trip_miles IS NULL, -1, trip_miles) trip_miles,\n",
      "        IF(payment_type IS NULL, 'NA', payment_type) payment_type,\n",
      "        IF(pickup_grid IS NULL, 'NA', pickup_grid) pickup_grid,\n",
      "        IF(dropoff_grid IS NULL, 'NA', dropoff_grid) dropoff_grid,\n",
      "        IF(euclidean IS NULL, -1, euclidean) euclidean,\n",
      "        IF(loc_cross IS NULL, 'NA', loc_cross) loc_cross,\n",
      "        tip_bin\n",
      "    FROM partner_training.chicago_taxitrips_prep \n",
      "    WHERE ML_use = 'UNASSIGNED'\n",
      "    LIMIT 1000\n",
      "Dataset grandelli-demo-295810:beam_temp_dataset_2c8ed54ce6c54c72bfd839ed40fc1dc1 does not exist so we will create it as temporary with location=US\n",
      "Started BigQuery job: <JobReference\n",
      " jobId: 'beam_bq_job_QUERY_BQ_EXPORT_JOB_0d4882b9-6_1648651533_40'\n",
      " location: 'US'\n",
      " projectId: 'grandelli-demo-295810'>\n",
      " bq show -j --format=prettyjson --project_id=grandelli-demo-295810 beam_bq_job_QUERY_BQ_EXPORT_JOB_0d4882b9-6_1648651533_40\n",
      "Job status: RUNNING\n",
      "Job status: DONE\n",
      "Started BigQuery job: <JobReference\n",
      " jobId: 'beam_bq_job_EXPORT_BQ_EXPORT_JOB_0d4882b9-6_1648651539_210'\n",
      " location: 'US'\n",
      " projectId: 'grandelli-demo-295810'>\n",
      " bq show -j --format=prettyjson --project_id=grandelli-demo-295810 beam_bq_job_EXPORT_BQ_EXPORT_JOB_0d4882b9-6_1648651539_210\n",
      "Job status: RUNNING\n",
      "Job status: DONE\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.05364584922790527 seconds.\n",
      "Running (((((((((((((ref_PCollection_PCollection_6_split/Read)+(InputToRecord/QueryTable/ReadFromBigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-_PassThroughThenCleanup-ParDo-PassTh_18))+(ref_AppliedPTransform_InputToRecord-ToTFExample_25))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_SplitData-ParDo-ApplyPartitionFnFn-ParDo-ApplyPartitionFnFn-_28))+(ref_AppliedPTransform_WriteSplit-eval-MaybeSerialize_55))+(ref_AppliedPTransform_WriteSplit-train-MaybeSerialize_30))+(ref_AppliedPTransform_WriteSplit-train-Shuffle-AddRandomKeys_32))+(ref_AppliedPTransform_WriteSplit-train-Shuffle-ReshufflePerKey-Map-reify_timestamps-_34))+(WriteSplit[train]/Shuffle/ReshufflePerKey/GroupByKey/Write))+(ref_AppliedPTransform_WriteSplit-eval-Shuffle-AddRandomKeys_57))+(ref_AppliedPTransform_WriteSplit-eval-Shuffle-ReshufflePerKey-Map-reify_timestamps-_59))+(WriteSplit[eval]/Shuffle/ReshufflePerKey/GroupByKey/Write)\n",
      "Running (((((ref_AppliedPTransform_WriteSplit-train-Write-Write-WriteImpl-DoOnce-Impulse_42)+(ref_AppliedPTransform_WriteSplit-train-Write-Write-WriteImpl-DoOnce-FlatMap-lambda-at-core-py-3222-_43))+(ref_AppliedPTransform_WriteSplit-train-Write-Write-WriteImpl-DoOnce-Map-decode-_45))+(ref_AppliedPTransform_WriteSplit-train-Write-Write-WriteImpl-InitializeWrite_46))+(ref_PCollection_PCollection_26/Write))+(ref_PCollection_PCollection_27/Write)\n",
      "Running ((((((WriteSplit[train]/Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_WriteSplit-train-Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_36))+(ref_AppliedPTransform_WriteSplit-train-Shuffle-RemoveRandomKeys_37))+(ref_AppliedPTransform_WriteSplit-train-Write-Write-WriteImpl-WindowInto-WindowIntoFn-_47))+(ref_AppliedPTransform_WriteSplit-train-Write-Write-WriteImpl-WriteBundles_48))+(ref_AppliedPTransform_WriteSplit-train-Write-Write-WriteImpl-Pair_49))+(WriteSplit[train]/Write/Write/WriteImpl/GroupByKey/Write)\n",
      "Running ((((ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-FilesToRemoveImpulse-Impulse_6)+(ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-FilesToRemoveImpulse-FlatMap-lambda-_7))+(ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-FilesToRemoveImpulse-Map-decode-_9))+(ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-MapFilesToRemove_10))+(ref_PCollection_PCollection_4/Write)\n",
      "Running (((((ref_AppliedPTransform_WriteSplit-eval-Write-Write-WriteImpl-DoOnce-Impulse_67)+(ref_AppliedPTransform_WriteSplit-eval-Write-Write-WriteImpl-DoOnce-FlatMap-lambda-at-core-py-3222-_68))+(ref_AppliedPTransform_WriteSplit-eval-Write-Write-WriteImpl-DoOnce-Map-decode-_70))+(ref_AppliedPTransform_WriteSplit-eval-Write-Write-WriteImpl-InitializeWrite_71))+(ref_PCollection_PCollection_43/Write))+(ref_PCollection_PCollection_44/Write)\n",
      "Running ((((((WriteSplit[eval]/Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_WriteSplit-eval-Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_61))+(ref_AppliedPTransform_WriteSplit-eval-Shuffle-RemoveRandomKeys_62))+(ref_AppliedPTransform_WriteSplit-eval-Write-Write-WriteImpl-WindowInto-WindowIntoFn-_72))+(ref_AppliedPTransform_WriteSplit-eval-Write-Write-WriteImpl-WriteBundles_73))+(ref_AppliedPTransform_WriteSplit-eval-Write-Write-WriteImpl-Pair_74))+(WriteSplit[eval]/Write/Write/WriteImpl/GroupByKey/Write)\n",
      "Running ((WriteSplit[eval]/Write/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteSplit-eval-Write-Write-WriteImpl-Extract_76))+(ref_PCollection_PCollection_49/Write)\n",
      "Running ((WriteSplit[train]/Write/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteSplit-train-Write-Write-WriteImpl-Extract_51))+(ref_PCollection_PCollection_32/Write)\n",
      "Running ((ref_PCollection_PCollection_26/Read)+(ref_AppliedPTransform_WriteSplit-train-Write-Write-WriteImpl-PreFinalize_52))+(ref_PCollection_PCollection_33/Write)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 0 files in 0.05085468292236328 seconds.\n",
      "Running ((ref_PCollection_PCollection_43/Read)+(ref_AppliedPTransform_WriteSplit-eval-Write-Write-WriteImpl-PreFinalize_77))+(ref_PCollection_PCollection_50/Write)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 0 files in 0.058370113372802734 seconds.\n",
      "Running (((ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-_PassThroughThenCleanup-Create-Impul_20)+(ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-_PassThroughThenCleanup-Create-FlatM_21))+(ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-_PassThroughThenCleanup-Create-Map-d_23))+(ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-_PassThroughThenCleanup-ParDo-Remove_24)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.05357861518859863 seconds.\n",
      "Running (ref_PCollection_PCollection_26/Read)+(ref_AppliedPTransform_WriteSplit-train-Write-Write-WriteImpl-FinalizeWrite_53)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.04659843444824219 seconds.\n",
      "Starting the size estimation of the input\n",
      "Finished listing 0 files in 0.03871917724609375 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Running (ref_PCollection_PCollection_43/Read)+(ref_AppliedPTransform_WriteSplit-eval-Write-Write-WriteImpl-FinalizeWrite_78)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.05572390556335449 seconds.\n",
      "Starting the size estimation of the input\n",
      "Finished listing 0 files in 0.04810309410095215 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Examples generated.\n",
      "Cleaning up stateless execution info.\n",
      "Execution 5 succeeded.\n",
      "Cleaning up stateful execution info.\n",
      "stateful_working_dir /home/jupyter/20220318_Training/gcp-partner-training-mlops/gs:/grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/TrainDataGen/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Publishing output artifacts defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/TrainDataGen/examples/5\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline:2022-03-30T14:44:39.880285:TrainDataGen:examples:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.2.0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      ")]}) for execution 5\n",
      "MetadataStore with DB connection initialized\n",
      "Component TrainDataGen is finished.\n",
      "Component WarmstartModelResolver is running.\n",
      "Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.dsl.components.common.resolver.Resolver\"\n",
      "  }\n",
      "  id: \"WarmstartModelResolver\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-03-30T14:44:39.880285\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.WarmstartModelResolver\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"latest_model\"\n",
      "    value {\n",
      "      channels {\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  resolver_config {\n",
      "    resolver_steps {\n",
      "      class_path: \"tfx.dsl.input_resolution.strategies.latest_artifact_strategy.LatestArtifactStrategy\"\n",
      "      config_json: \"{}\"\n",
      "      input_keys: \"latest_model\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"ModelTrainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "Running as an resolver node.\n",
      "MetadataStore with DB connection initialized\n",
      "Artifact type Model is not found in MLMD.\n",
      "Component WarmstartModelResolver is finished.\n",
      "Component StatisticsGen is running.\n",
      "Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.statistics_gen.component.StatisticsGen\"\n",
      "  }\n",
      "  id: \"StatisticsGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-03-30T14:44:39.880285\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.StatisticsGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"TrainDataGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-03-30T14:44:39.880285\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.TrainDataGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"statistics\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleStatistics\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"exclude_splits\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"[]\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"TrainDataGen\"\n",
      "downstream_nodes: \"ExampleValidator\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "MetadataStore with DB connection initialized\n",
      "I0330 14:45:49.604513   104 rdbms_metadata_access_object.cc:686] No property is defined for the Type\n",
      "MetadataStore with DB connection initialized\n",
      "Going to run a new execution 7\n",
      "Going to run a new execution: ExecutionInfo(execution_id=7, input_dict={'examples': [Artifact(artifact: id: 4\n",
      "type_id: 20\n",
      "uri: \"gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/TrainDataGen/examples/5\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"file_format\"\n",
      "  value {\n",
      "    string_value: \"tfrecords_gzip\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline:2022-03-30T14:44:39.880285:TrainDataGen:examples:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"payload_format\"\n",
      "  value {\n",
      "    string_value: \"FORMAT_TF_EXAMPLE\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.2.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1648651549393\n",
      "last_update_time_since_epoch: 1648651549393\n",
      ", artifact_type: id: 20\n",
      "name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      ")]}, output_dict=defaultdict(<class 'list'>, {'statistics': [Artifact(artifact: uri: \"gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/StatisticsGen/statistics/7\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline:2022-03-30T14:44:39.880285:StatisticsGen:statistics:0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"ExampleStatistics\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      ")]}), exec_properties={'exclude_splits': '[]'}, execution_output_uri='gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/StatisticsGen/.system/executor_execution/7/executor_output.pb', stateful_working_dir='gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/StatisticsGen/.system/stateful_working_dir/2022-03-30T14:44:39.880285', tmp_dir='gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/StatisticsGen/.system/executor_execution/7/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.statistics_gen.component.StatisticsGen\"\n",
      "  }\n",
      "  id: \"StatisticsGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-03-30T14:44:39.880285\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.StatisticsGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"TrainDataGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-03-30T14:44:39.880285\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.TrainDataGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"statistics\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleStatistics\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"exclude_splits\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"[]\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"TrainDataGen\"\n",
      "downstream_nodes: \"ExampleValidator\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      ", pipeline_run_id='2022-03-30T14:44:39.880285')\n",
      "Attempting to infer TFX Python dependency for beam\n",
      "Copying all content from install dir /home/jupyter/.local/lib/python3.7/site-packages/tfx to temp dir /tmp/tmpv39n_2ni/build/tfx\n",
      "Generating a temp setup file at /tmp/tmpv39n_2ni/build/tfx/setup.py\n",
      "Creating temporary sdist package, logs available at /tmp/tmpv39n_2ni/build/tfx/setup.log\n",
      "E0330 14:45:53.079945794     104 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "Added --extra_package=/tmp/tmpv39n_2ni/build/tfx/dist/tfx_ephemeral-1.2.0.tar.gz to beam args\n",
      "Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "Generating statistics for split train.\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.049379587173461914 seconds.\n",
      "Statistics for split train written to gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/StatisticsGen/statistics/7/Split-train.\n",
      "Generating statistics for split eval.\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.03920698165893555 seconds.\n",
      "Statistics for split eval written to gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/StatisticsGen/statistics/7/Split-eval.\n",
      "Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "Default Python SDK image for environment is apache/beam_python3.7_sdk:2.34.0\n",
      "==================== <function annotate_downstream_side_inputs at 0x7f8505d1f4d0> ====================\n",
      "==================== <function fix_side_input_pcoll_coders at 0x7f8505d1f5f0> ====================\n",
      "==================== <function pack_combiners at 0x7f8505d1fb00> ====================\n",
      "==================== <function lift_combiners at 0x7f8505d1fb90> ====================\n",
      "==================== <function expand_sdf at 0x7f8505d1fd40> ====================\n",
      "==================== <function expand_gbk at 0x7f8505d1fdd0> ====================\n",
      "==================== <function sink_flattens at 0x7f8505d1fef0> ====================\n",
      "==================== <function greedily_fuse at 0x7f8505d1ff80> ====================\n",
      "==================== <function read_to_impulse at 0x7f8505d1e050> ====================\n",
      "==================== <function impulse_to_input at 0x7f8505d1e0e0> ====================\n",
      "==================== <function sort_stages at 0x7f8505d1e320> ====================\n",
      "==================== <function setup_timer_mapping at 0x7f8505d1e290> ====================\n",
      "==================== <function populate_data_channel_coders at 0x7f8505d1e3b0> ====================\n",
      "Creating state cache with size 100\n",
      "Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7f8501de7650> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
      "Running ((((ref_AppliedPTransform_TFXIORead-train-RawRecordBeamSource-ReadRawRecords-ReadFromTFRecord-0-Read-Imp_7)+(ref_AppliedPTransform_TFXIORead-train-RawRecordBeamSource-ReadRawRecords-ReadFromTFRecord-0-Read-Map_8))+(TFXIORead[train]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(TFXIORead[train]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_2_split/Write)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.03897666931152344 seconds.\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.04209017753601074 seconds.\n",
      "Running ((((((((((((((((ref_PCollection_PCollection_2_split/Read)+(TFXIORead[train]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_TFXIORead-train-RawRecordBeamSource-ReadRawRecords-FlattenPCollsFromPatterns_11))+(ref_AppliedPTransform_TFXIORead-train-RawRecordBeamSource-CollectRawRecordTelemetry-ProfileRawRecord_13))+(ref_AppliedPTransform_TFXIORead-train-RawRecordToRecordBatch-RawRecordToRecordBatch-Batch-ParDo-_Glo_17))+(ref_AppliedPTransform_TFXIORead-train-RawRecordToRecordBatch-RawRecordToRecordBatch-Decode_18))+(ref_AppliedPTransform_TFXIORead-train-RawRecordToRecordBatch-CollectRecordBatchTelemetry-ProfileReco_20))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-KeyWithVoid_23))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-TopKU_26))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-RunCo_51))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/Precombine))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/Group/Write))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/Flatten/Transcode/0))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-RunCo_52))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PreCombineFn)/Precombine))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PreCombineFn)/Group/Write))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/Flatten/Write/0)\n",
      "Running (((((((((GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/Group/Read)+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/Merge))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-TopKU_31))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/Precombine))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-TopKU_40))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/Group/Write))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-TopKU_42))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Precombine))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Group/Write)\n",
      "Running (((((GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/Group/Read)+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/Merge))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-TopKU_37))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-TopKU_38))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesFeatureStatsProtos/Write/0)\n",
      "Running ((((GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Group/Read)+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Merge))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-TopKU_47))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesFeatureStatsProtos/Write/1)\n",
      "Running ((GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesFeatureStatsProtos/Read)+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Transcode/0))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Write/0)\n",
      "Running ((((((GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PreCombineFn)/Group/Read)+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PreCombineFn)/Merge))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PreCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-RunCo_57))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-RunCo_58))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/Flatten/Transcode/1))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/Flatten/Write/1)\n",
      "Running ((GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/Flatten/Read)+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PostCombineFn)/Precombine))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PostCombineFn)/Group/Write)\n",
      "Running ((((GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PostCombineFn)/Group/Read)+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PostCombineFn)/Merge))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PostCombineFn)/ExtractOutputs))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Transcode/1))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Write/1)\n",
      "Running ((GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Read)+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Precombine))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Group/Write)\n",
      "Running ((((((GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Group/Read)+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Merge))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-AddSl_69))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-ToLis_72))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Precombine))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Write)\n",
      "Running ((((GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Read)+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Merge))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-ToLis_77))+(ref_PCollection_PCollection_41/Write)\n",
      "Running ((((ref_AppliedPTransform_TFXIORead-eval-RawRecordBeamSource-ReadRawRecords-ReadFromTFRecord-0-Read-Impu_106)+(ref_AppliedPTransform_TFXIORead-eval-RawRecordBeamSource-ReadRawRecords-ReadFromTFRecord-0-Read-Map-_107))+(TFXIORead[eval]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(TFXIORead[eval]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_58_split/Write)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.06252837181091309 seconds.\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.042111873626708984 seconds.\n",
      "Running ((((((((((((((((ref_PCollection_PCollection_58_split/Read)+(TFXIORead[eval]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_TFXIORead-eval-RawRecordBeamSource-ReadRawRecords-FlattenPCollsFromPatterns_110))+(ref_AppliedPTransform_TFXIORead-eval-RawRecordBeamSource-CollectRawRecordTelemetry-ProfileRawRecords_112))+(ref_AppliedPTransform_TFXIORead-eval-RawRecordToRecordBatch-RawRecordToRecordBatch-Batch-ParDo-_Glob_116))+(ref_AppliedPTransform_TFXIORead-eval-RawRecordToRecordBatch-RawRecordToRecordBatch-Decode_117))+(ref_AppliedPTransform_TFXIORead-eval-RawRecordToRecordBatch-CollectRecordBatchTelemetry-ProfileRecor_119))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-KeyWithVoid_122))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-TopKUn_125))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-RunCom_150))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/Precombine))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/Group/Write))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-RunCom_151))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/Flatten/Transcode/0))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PreCombineFn)/Precombine))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PreCombineFn)/Group/Write))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/Flatten/Write/0)\n",
      "Running (((((((((GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/Group/Read)+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/Merge))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-TopKUn_130))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/Precombine))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-TopKUn_139))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/Group/Write))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-TopKUn_141))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Precombine))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Group/Write)\n",
      "Running ((((GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Group/Read)+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Merge))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-TopKUn_146))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesFeatureStatsProtos/Write/1)\n",
      "Running (((((ref_AppliedPTransform_WriteStatsOutput-train-WriteStats-Write-WriteImpl-DoOnce-Impulse_90)+(ref_AppliedPTransform_WriteStatsOutput-train-WriteStats-Write-WriteImpl-DoOnce-FlatMap-lambda-at-cor_91))+(ref_AppliedPTransform_WriteStatsOutput-train-WriteStats-Write-WriteImpl-DoOnce-Map-decode-_93))+(ref_AppliedPTransform_WriteStatsOutput-train-WriteStats-Write-WriteImpl-InitializeWrite_94))+(ref_PCollection_PCollection_49/Write))+(ref_PCollection_PCollection_50/Write)\n",
      "Running (((((((ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-ToLis_79)+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-ToLis_80))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-ToLis_82))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-ToLis_83))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-MakeD_84))+(ref_AppliedPTransform_WriteStatsOutput-train-WriteStats-Write-WriteImpl-Map-lambda-at-iobase-py-1130_95))+(ref_AppliedPTransform_WriteStatsOutput-train-WriteStats-Write-WriteImpl-WindowInto-WindowIntoFn-_96))+(WriteStatsOutput[train]/WriteStats/Write/WriteImpl/GroupByKey/Write)\n",
      "Running ((WriteStatsOutput[train]/WriteStats/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteStatsOutput-train-WriteStats-Write-WriteImpl-WriteBundles_98))+(ref_PCollection_PCollection_54/Write)\n",
      "Running ((ref_PCollection_PCollection_49/Read)+(ref_AppliedPTransform_WriteStatsOutput-train-WriteStats-Write-WriteImpl-PreFinalize_99))+(ref_PCollection_PCollection_55/Write)\n",
      "Running (ref_PCollection_PCollection_49/Read)+(ref_AppliedPTransform_WriteStatsOutput-train-WriteStats-Write-WriteImpl-FinalizeWrite_100)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.03931856155395508 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Running ((((((GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PreCombineFn)/Group/Read)+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PreCombineFn)/Merge))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PreCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-RunCom_156))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-RunCom_157))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/Flatten/Transcode/1))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/Flatten/Write/1)\n",
      "Running ((GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/Flatten/Read)+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PostCombineFn)/Precombine))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PostCombineFn)/Group/Write)\n",
      "Running (((((GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/Group/Read)+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/Merge))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-TopKUn_136))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-TopKUn_137))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesFeatureStatsProtos/Write/0)\n",
      "Running ((GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesFeatureStatsProtos/Read)+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Transcode/0))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Write/0)\n",
      "Running ((((GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PostCombineFn)/Group/Read)+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PostCombineFn)/Merge))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PostCombineFn)/ExtractOutputs))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Transcode/1))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Write/1)\n",
      "Running ((GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Read)+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Precombine))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Group/Write)\n",
      "Running ((((((GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Group/Read)+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Merge))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-AddSli_168))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-ToList_171))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Precombine))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Write)\n",
      "Running ((((GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Read)+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Merge))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-ToList_176))+(ref_PCollection_PCollection_97/Write)\n",
      "Running (((((ref_AppliedPTransform_WriteStatsOutput-eval-WriteStats-Write-WriteImpl-DoOnce-Impulse_189)+(ref_AppliedPTransform_WriteStatsOutput-eval-WriteStats-Write-WriteImpl-DoOnce-FlatMap-lambda-at-core_190))+(ref_AppliedPTransform_WriteStatsOutput-eval-WriteStats-Write-WriteImpl-DoOnce-Map-decode-_192))+(ref_AppliedPTransform_WriteStatsOutput-eval-WriteStats-Write-WriteImpl-InitializeWrite_193))+(ref_PCollection_PCollection_105/Write))+(ref_PCollection_PCollection_106/Write)\n",
      "Running (((((((ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-ToList_178)+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-ToList_179))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-ToList_181))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-ToList_182))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-MakeDa_183))+(ref_AppliedPTransform_WriteStatsOutput-eval-WriteStats-Write-WriteImpl-Map-lambda-at-iobase-py-1130-_194))+(ref_AppliedPTransform_WriteStatsOutput-eval-WriteStats-Write-WriteImpl-WindowInto-WindowIntoFn-_195))+(WriteStatsOutput[eval]/WriteStats/Write/WriteImpl/GroupByKey/Write)\n",
      "Running ((WriteStatsOutput[eval]/WriteStats/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteStatsOutput-eval-WriteStats-Write-WriteImpl-WriteBundles_197))+(ref_PCollection_PCollection_110/Write)\n",
      "Running ((ref_PCollection_PCollection_105/Read)+(ref_AppliedPTransform_WriteStatsOutput-eval-WriteStats-Write-WriteImpl-PreFinalize_198))+(ref_PCollection_PCollection_111/Write)\n",
      "Running (ref_PCollection_PCollection_105/Read)+(ref_AppliedPTransform_WriteStatsOutput-eval-WriteStats-Write-WriteImpl-FinalizeWrite_199)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.04494023323059082 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Cleaning up stateless execution info.\n",
      "Execution 7 succeeded.\n",
      "Cleaning up stateful execution info.\n",
      "stateful_working_dir /home/jupyter/20220318_Training/gcp-partner-training-mlops/gs:/grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/StatisticsGen/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Publishing output artifacts defaultdict(<class 'list'>, {'statistics': [Artifact(artifact: uri: \"gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/StatisticsGen/statistics/7\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline:2022-03-30T14:44:39.880285:StatisticsGen:statistics:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.2.0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"ExampleStatistics\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      ")]}) for execution 7\n",
      "MetadataStore with DB connection initialized\n",
      "Component StatisticsGen is finished.\n",
      "Component ExampleValidator is running.\n",
      "Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.example_validator.component.ExampleValidator\"\n",
      "  }\n",
      "  id: \"ExampleValidator\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-03-30T14:44:39.880285\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.ExampleValidator\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"schema\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"SchemaImporter\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-03-30T14:44:39.880285\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.SchemaImporter\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Schema\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"result\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"statistics\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"StatisticsGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-03-30T14:44:39.880285\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.StatisticsGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"ExampleStatistics\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"statistics\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"anomalies\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleAnomalies\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"exclude_splits\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"[]\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"SchemaImporter\"\n",
      "upstream_nodes: \"StatisticsGen\"\n",
      "downstream_nodes: \"DataTransformer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "MetadataStore with DB connection initialized\n",
      "I0330 14:46:00.481302   104 rdbms_metadata_access_object.cc:686] No property is defined for the Type\n",
      "MetadataStore with DB connection initialized\n",
      "Going to run a new execution 8\n",
      "Going to run a new execution: ExecutionInfo(execution_id=8, input_dict={'schema': [Artifact(artifact: id: 2\n",
      "type_id: 18\n",
      "uri: \"src/raw_schema\"\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.2.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1648651483936\n",
      "last_update_time_since_epoch: 1648651483936\n",
      ", artifact_type: id: 18\n",
      "name: \"Schema\"\n",
      ")], 'statistics': [Artifact(artifact: id: 5\n",
      "type_id: 22\n",
      "uri: \"gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/StatisticsGen/statistics/7\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline:2022-03-30T14:44:39.880285:StatisticsGen:statistics:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.2.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1648651560395\n",
      "last_update_time_since_epoch: 1648651560395\n",
      ", artifact_type: id: 22\n",
      "name: \"ExampleStatistics\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      ")]}, output_dict=defaultdict(<class 'list'>, {'anomalies': [Artifact(artifact: uri: \"gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/ExampleValidator/anomalies/8\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline:2022-03-30T14:44:39.880285:ExampleValidator:anomalies:0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"ExampleAnomalies\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      ")]}), exec_properties={'exclude_splits': '[]'}, execution_output_uri='gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/ExampleValidator/.system/executor_execution/8/executor_output.pb', stateful_working_dir='gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/ExampleValidator/.system/stateful_working_dir/2022-03-30T14:44:39.880285', tmp_dir='gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/ExampleValidator/.system/executor_execution/8/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.example_validator.component.ExampleValidator\"\n",
      "  }\n",
      "  id: \"ExampleValidator\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-03-30T14:44:39.880285\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.ExampleValidator\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"schema\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"SchemaImporter\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-03-30T14:44:39.880285\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.SchemaImporter\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Schema\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"result\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"statistics\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"StatisticsGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-03-30T14:44:39.880285\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.StatisticsGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"ExampleStatistics\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"statistics\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"anomalies\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleAnomalies\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"exclude_splits\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"[]\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"SchemaImporter\"\n",
      "upstream_nodes: \"StatisticsGen\"\n",
      "downstream_nodes: \"DataTransformer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      ", pipeline_run_id='2022-03-30T14:44:39.880285')\n",
      "Validating schema against the computed statistics for split train.\n",
      "Validation complete for split train. Anomalies written to gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/ExampleValidator/anomalies/8/Split-train.\n",
      "Validating schema against the computed statistics for split eval.\n",
      "Validation complete for split eval. Anomalies written to gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/ExampleValidator/anomalies/8/Split-eval.\n",
      "Cleaning up stateless execution info.\n",
      "Execution 8 succeeded.\n",
      "Cleaning up stateful execution info.\n",
      "stateful_working_dir /home/jupyter/20220318_Training/gcp-partner-training-mlops/gs:/grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/ExampleValidator/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Publishing output artifacts defaultdict(<class 'list'>, {'anomalies': [Artifact(artifact: uri: \"gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/ExampleValidator/anomalies/8\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline:2022-03-30T14:44:39.880285:ExampleValidator:anomalies:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.2.0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"ExampleAnomalies\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      ")]}) for execution 8\n",
      "MetadataStore with DB connection initialized\n",
      "Component ExampleValidator is finished.\n",
      "Component DataTransformer is running.\n",
      "Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.transform.component.Transform\"\n",
      "  }\n",
      "  id: \"DataTransformer\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-03-30T14:44:39.880285\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.DataTransformer\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"TrainDataGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-03-30T14:44:39.880285\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.TrainDataGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"schema\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"SchemaImporter\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-03-30T14:44:39.880285\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.SchemaImporter\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Schema\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"result\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"post_transform_anomalies\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleAnomalies\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"post_transform_schema\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Schema\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"post_transform_stats\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleStatistics\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"pre_transform_schema\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Schema\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"pre_transform_stats\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleStatistics\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"transform_graph\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"TransformGraph\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"transformed_examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"updated_analyzer_cache\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"TransformCache\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"custom_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"disable_statistics\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"force_tf_compat_v1\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"module_path\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"transformations@gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/_wheels/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"splits_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"analyze\\\": [\\n    \\\"train\\\"\\n  ],\\n  \\\"transform\\\": [\\n    \\\"train\\\",\\n    \\\"eval\\\"\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"ExampleValidator\"\n",
      "upstream_nodes: \"SchemaImporter\"\n",
      "upstream_nodes: \"TrainDataGen\"\n",
      "downstream_nodes: \"ModelTrainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "MetadataStore with DB connection initialized\n",
      "I0330 14:46:05.746047   104 rdbms_metadata_access_object.cc:686] No property is defined for the Type\n",
      "MetadataStore with DB connection initialized\n",
      "Going to run a new execution 9\n",
      "Going to run a new execution: ExecutionInfo(execution_id=9, input_dict={'examples': [Artifact(artifact: id: 4\n",
      "type_id: 20\n",
      "uri: \"gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/TrainDataGen/examples/5\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"file_format\"\n",
      "  value {\n",
      "    string_value: \"tfrecords_gzip\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline:2022-03-30T14:44:39.880285:TrainDataGen:examples:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"payload_format\"\n",
      "  value {\n",
      "    string_value: \"FORMAT_TF_EXAMPLE\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.2.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1648651549393\n",
      "last_update_time_since_epoch: 1648651549393\n",
      ", artifact_type: id: 20\n",
      "name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      ")], 'schema': [Artifact(artifact: id: 2\n",
      "type_id: 18\n",
      "uri: \"src/raw_schema\"\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.2.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1648651483936\n",
      "last_update_time_since_epoch: 1648651483936\n",
      ", artifact_type: id: 18\n",
      "name: \"Schema\"\n",
      ")]}, output_dict=defaultdict(<class 'list'>, {'updated_analyzer_cache': [Artifact(artifact: uri: \"gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/DataTransformer/updated_analyzer_cache/9\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline:2022-03-30T14:44:39.880285:DataTransformer:updated_analyzer_cache:0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"TransformCache\"\n",
      ")], 'pre_transform_stats': [Artifact(artifact: uri: \"gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/DataTransformer/pre_transform_stats/9\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline:2022-03-30T14:44:39.880285:DataTransformer:pre_transform_stats:0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"ExampleStatistics\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      ")], 'post_transform_schema': [Artifact(artifact: uri: \"gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/DataTransformer/post_transform_schema/9\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline:2022-03-30T14:44:39.880285:DataTransformer:post_transform_schema:0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"Schema\"\n",
      ")], 'pre_transform_schema': [Artifact(artifact: uri: \"gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/DataTransformer/pre_transform_schema/9\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline:2022-03-30T14:44:39.880285:DataTransformer:pre_transform_schema:0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"Schema\"\n",
      ")], 'post_transform_anomalies': [Artifact(artifact: uri: \"gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/DataTransformer/post_transform_anomalies/9\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline:2022-03-30T14:44:39.880285:DataTransformer:post_transform_anomalies:0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"ExampleAnomalies\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      ")], 'post_transform_stats': [Artifact(artifact: uri: \"gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/DataTransformer/post_transform_stats/9\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline:2022-03-30T14:44:39.880285:DataTransformer:post_transform_stats:0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"ExampleStatistics\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      ")], 'transform_graph': [Artifact(artifact: uri: \"gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/DataTransformer/transform_graph/9\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline:2022-03-30T14:44:39.880285:DataTransformer:transform_graph:0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"TransformGraph\"\n",
      ")], 'transformed_examples': [Artifact(artifact: uri: \"gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/DataTransformer/transformed_examples/9\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline:2022-03-30T14:44:39.880285:DataTransformer:transformed_examples:0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      ")]}), exec_properties={'force_tf_compat_v1': 0, 'module_path': 'transformations@gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/_wheels/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl', 'splits_config': '{\\n  \"analyze\": [\\n    \"train\"\\n  ],\\n  \"transform\": [\\n    \"train\",\\n    \"eval\"\\n  ]\\n}', 'custom_config': 'null', 'disable_statistics': 0}, execution_output_uri='gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/DataTransformer/.system/executor_execution/9/executor_output.pb', stateful_working_dir='gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/DataTransformer/.system/stateful_working_dir/2022-03-30T14:44:39.880285', tmp_dir='gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/DataTransformer/.system/executor_execution/9/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.transform.component.Transform\"\n",
      "  }\n",
      "  id: \"DataTransformer\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-03-30T14:44:39.880285\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.DataTransformer\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"TrainDataGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-03-30T14:44:39.880285\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.TrainDataGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"schema\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"SchemaImporter\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-03-30T14:44:39.880285\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.SchemaImporter\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Schema\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"result\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"post_transform_anomalies\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleAnomalies\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"post_transform_schema\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Schema\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"post_transform_stats\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleStatistics\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"pre_transform_schema\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Schema\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"pre_transform_stats\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleStatistics\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"transform_graph\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"TransformGraph\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"transformed_examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"updated_analyzer_cache\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"TransformCache\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"custom_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"disable_statistics\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"force_tf_compat_v1\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"module_path\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"transformations@gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/_wheels/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"splits_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"analyze\\\": [\\n    \\\"train\\\"\\n  ],\\n  \\\"transform\\\": [\\n    \\\"train\\\",\\n    \\\"eval\\\"\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"ExampleValidator\"\n",
      "upstream_nodes: \"SchemaImporter\"\n",
      "upstream_nodes: \"TrainDataGen\"\n",
      "downstream_nodes: \"ModelTrainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      ", pipeline_run_id='2022-03-30T14:44:39.880285')\n",
      "Attempting to infer TFX Python dependency for beam\n",
      "Copying all content from install dir /home/jupyter/.local/lib/python3.7/site-packages/tfx to temp dir /tmp/tmpubly2flc/build/tfx\n",
      "Generating a temp setup file at /tmp/tmpubly2flc/build/tfx/setup.py\n",
      "Creating temporary sdist package, logs available at /tmp/tmpubly2flc/build/tfx/setup.log\n",
      "E0330 14:46:13.655623833     104 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "Added --extra_package=/tmp/tmpubly2flc/build/tfx/dist/tfx_ephemeral-1.2.0.tar.gz to beam args\n",
      "udf_utils.get_fn {'module_file': None, 'module_path': 'transformations@gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/_wheels/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl', 'preprocessing_fn': None} 'preprocessing_fn'\n",
      "Installing '/tmp/tmpqe824jvl/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl' to a temporary directory.\n",
      "Executing: ['/opt/conda/bin/python', '-m', 'pip', 'install', '--target', '/tmp/tmp83k2j8kc', '/tmp/tmpqe824jvl/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl']\n",
      "E0330 14:46:15.454724430     104 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "Processing /tmp/tmpqe824jvl/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl\n",
      "Installing collected packages: tfx-user-code-DataTransformer\n",
      "Successfully installed tfx-user-code-DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d\n",
      "Successfully installed '/tmp/tmpqe824jvl/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl'.\n",
      "udf_utils.get_fn {'module_file': None, 'module_path': 'transformations@gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/_wheels/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl', 'stats_options_updater_fn': None} 'stats_options_updater_fn'\n",
      "Installing '/tmp/tmporzdijsr/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl' to a temporary directory.\n",
      "Executing: ['/opt/conda/bin/python', '-m', 'pip', 'install', '--target', '/tmp/tmpp9m5z1jx', '/tmp/tmporzdijsr/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl']\n",
      "E0330 14:46:18.680467598     104 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "Processing /tmp/tmporzdijsr/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl\n",
      "Installing collected packages: tfx-user-code-DataTransformer\n",
      "Successfully installed tfx-user-code-DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d\n",
      "Successfully installed '/tmp/tmporzdijsr/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl'.\n",
      "Feature trip_month has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_day has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_day_of_week has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_hour has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_seconds has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_miles has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature payment_type has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature pickup_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature dropoff_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature euclidean has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature loc_cross has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature tip_bin has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "2022-03-30 14:46:21.832009: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2022-03-30 14:46:21.832084: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-03-30 14:46:21.832120: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (vm-508f776f-5512-42a9-b529-8989959ded1b): /proc/driver/nvidia/version does not exist\n",
      "2022-03-30 14:46:21.832415: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "From /home/jupyter/.local/lib/python3.7/site-packages/tensorflow_transform/tf_utils.py:261: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use ref() instead.\n",
      "Feature trip_month has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_day has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_day_of_week has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_hour has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_seconds has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_miles has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature payment_type has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature pickup_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature dropoff_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature euclidean has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature loc_cross has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature tip_bin has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_month has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_day has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_day_of_week has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_hour has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_seconds has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_miles has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature payment_type has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature pickup_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature dropoff_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature euclidean has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature loc_cross has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature tip_bin has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_month has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_day has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_day_of_week has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_hour has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_seconds has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_miles has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature payment_type has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature pickup_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature dropoff_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature euclidean has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature loc_cross has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature tip_bin has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_month has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_day has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_day_of_week has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_hour has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_seconds has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_miles has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature payment_type has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature pickup_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature dropoff_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature euclidean has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature loc_cross has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature tip_bin has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_month has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_day has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_day_of_week has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_hour has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_seconds has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_miles has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature payment_type has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature pickup_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature dropoff_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature euclidean has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature loc_cross has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature tip_bin has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Installing '/tmp/tmpmeek87tm/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl' to a temporary directory.\n",
      "Executing: ['/opt/conda/bin/python', '-m', 'pip', 'install', '--target', '/tmp/tmpadq8px8_', '/tmp/tmpmeek87tm/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl']\n",
      "E0330 14:46:22.500592602     104 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "Processing /tmp/tmpmeek87tm/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl\n",
      "Installing collected packages: tfx-user-code-DataTransformer\n",
      "Successfully installed tfx-user-code-DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d\n",
      "Successfully installed '/tmp/tmpmeek87tm/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl'.\n",
      "Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "This output type hint will be ignored and not used for type-checking purposes. Typically, output type hints for a PTransform are single (or nested) types wrapped by a PCollection, PDone, or None. Got: Tuple[Dict[str, Union[NoneType, _Dataset]], Union[Dict[str, Dict[str, PCollection]], NoneType]] instead.\n",
      "Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_1/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_2/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_3/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_4/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_5/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_6/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_7/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_1/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_2/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_3/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_4/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_5/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_6/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_7/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "This output type hint will be ignored and not used for type-checking purposes. Typically, output type hints for a PTransform are single (or nested) types wrapped by a PCollection, PDone, or None. Got: Tuple[Dict[str, Union[NoneType, _Dataset]], Union[Dict[str, Dict[str, PCollection]], NoneType]] instead.\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.0544893741607666 seconds.\n",
      "2022-03-30 14:46:44.315517: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-03-30 14:46:44.316213: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2299995000 Hz\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.04537320137023926 seconds.\n",
      "Feature trip_month has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_day has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_day_of_week has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_hour has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_seconds has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_miles has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature payment_type has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature pickup_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature dropoff_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature euclidean has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature loc_cross has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature tip_bin has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.036253929138183594 seconds.\n",
      "Feature trip_month has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_day has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_day_of_week has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_hour has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_seconds has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature trip_miles has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature payment_type has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature pickup_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature dropoff_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature euclidean has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature loc_cross has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature tip_bin has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "Default Python SDK image for environment is apache/beam_python3.7_sdk:2.34.0\n",
      "==================== <function annotate_downstream_side_inputs at 0x7f8505d1f4d0> ====================\n",
      "==================== <function fix_side_input_pcoll_coders at 0x7f8505d1f5f0> ====================\n",
      "==================== <function pack_combiners at 0x7f8505d1fb00> ====================\n",
      "==================== <function lift_combiners at 0x7f8505d1fb90> ====================\n",
      "==================== <function expand_sdf at 0x7f8505d1fd40> ====================\n",
      "==================== <function expand_gbk at 0x7f8505d1fdd0> ====================\n",
      "==================== <function sink_flattens at 0x7f8505d1fef0> ====================\n",
      "==================== <function greedily_fuse at 0x7f8505d1ff80> ====================\n",
      "==================== <function read_to_impulse at 0x7f8505d1e050> ====================\n",
      "==================== <function impulse_to_input at 0x7f8505d1e0e0> ====================\n",
      "==================== <function sort_stages at 0x7f8505d1e320> ====================\n",
      "==================== <function setup_timer_mapping at 0x7f8505d1e290> ====================\n",
      "==================== <function populate_data_channel_coders at 0x7f8505d1e3b0> ====================\n",
      "Creating state cache with size 100\n",
      "Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7f84d87ebe90> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
      "Running (((((ref_AppliedPTransform_Materialize-TransformIndex1-Write-Write-WriteImpl-DoOnce-Impulse_1390)+(ref_AppliedPTransform_Materialize-TransformIndex1-Write-Write-WriteImpl-DoOnce-FlatMap-lambda-at-cor_1391))+(ref_AppliedPTransform_Materialize-TransformIndex1-Write-Write-WriteImpl-DoOnce-Map-decode-_1393))+(ref_AppliedPTransform_Materialize-TransformIndex1-Write-Write-WriteImpl-InitializeWrite_1394))+(ref_PCollection_PCollection_815/Write))+(ref_PCollection_PCollection_816/Write)\n",
      "Running (((((ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex4-Write-WriteImpl-DoOnce-Impulse_969)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex4-Write-WriteImpl-DoOnce-FlatMap-_970))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex4-Write-WriteImpl-DoOnce-Map-deco_972))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex4-Write-WriteImpl-InitializeWrite_973))+(ref_PCollection_PCollection_553/Write))+(ref_PCollection_PCollection_554/Write)\n",
      "Running (((((ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex9-Write-WriteImpl-DoOnce-Impulse_1049)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex9-Write-WriteImpl-DoOnce-FlatMap-_1050))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex9-Write-WriteImpl-DoOnce-Map-deco_1052))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex9-Write-WriteImpl-InitializeWrite_1053))+(ref_PCollection_PCollection_608/Write))+(ref_PCollection_PCollection_609/Write)\n",
      "Running ((((ref_AppliedPTransform_Analyze-CreateSavedModelForAnalyzerInputs-Phase0-tf_v2_only-CreateSole-Impulse_44)+(ref_AppliedPTransform_Analyze-CreateSavedModelForAnalyzerInputs-Phase0-tf_v2_only-CreateSole-FlatMap_45))+(ref_AppliedPTransform_Analyze-CreateSavedModelForAnalyzerInputs-Phase0-tf_v2_only-CreateSole-Map-dec_47))+(ref_AppliedPTransform_Analyze-CreateSavedModelForAnalyzerInputs-Phase0-tf_v2_only-CreateSavedModel_48))+(ref_PCollection_PCollection_23/Write)\n",
      "2022-03-30 14:47:00.667818: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "Assets written to: gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/DataTransformer/transform_graph/9/.temp_path/tftransform_tmp/854823a3f82a447390716eead7ed309b/assets\n",
      "Running ((((ref_AppliedPTransform_TFXIOReadAndDecode-AnalysisIndex0-RawRecordBeamSource-ReadRawRecords-ReadFromT_20)+(ref_AppliedPTransform_TFXIOReadAndDecode-AnalysisIndex0-RawRecordBeamSource-ReadRawRecords-ReadFromT_21))+(TFXIOReadAndDecode[AnalysisIndex0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(TFXIOReadAndDecode[AnalysisIndex0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_9_split/Write)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.033721923828125 seconds.\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.04036307334899902 seconds.\n",
      "Running ((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((ref_PCollection_PCollection_9_split/Read)+(TFXIOReadAndDecode[AnalysisIndex0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_TFXIOReadAndDecode-AnalysisIndex0-RawRecordBeamSource-ReadRawRecords-FlattenPC_24))+(ref_AppliedPTransform_TFXIOReadAndDecode-AnalysisIndex0-RawRecordBeamSource-CollectRawRecordTelemetr_26))+(ref_AppliedPTransform_TFXIOReadAndDecode-AnalysisIndex0-RawRecordToRecordBatch-RawRecordToRecordBatc_30))+(ref_AppliedPTransform_TFXIOReadAndDecode-AnalysisIndex0-RawRecordToRecordBatch-RawRecordToRecordBatc_31))+(ref_AppliedPTransform_TFXIOReadAndDecode-AnalysisIndex0-RawRecordToRecordBatch-CollectRecordBatchTel_33))+(ref_AppliedPTransform_Analyze-ExtractInputForSavedModel-AnalysisIndex0-Identity_57))+(ref_AppliedPTransform_FlattenAnalysisDatasets_1077))+(ref_AppliedPTransform_Analyze-ApplySavedModel-Phase0-AnalysisIndex0-ApplySavedModel_59))+(ref_AppliedPTransform_Analyze-ApplySavedModel-Phase0-AnalysisIndex0-ConvertToNumpy_60))+(ref_AppliedPTransform_Analyze-PackedCombineAccumulate-ApplySavedModel-Phase0-AnalysisIndex0-InitialP_63))+(ref_AppliedPTransform_Analyze-TensorSource-compute_and_apply_vocabulary-vocabulary-AnalysisIndex0-Ex_144))+(ref_AppliedPTransform_Analyze-TensorSource-compute_and_apply_vocabulary_1-vocabulary-AnalysisIndex0-_219))+(ref_AppliedPTransform_Analyze-TensorSource-compute_and_apply_vocabulary_2-vocabulary-AnalysisIndex0-_294))+(ref_AppliedPTransform_Analyze-TensorSource-compute_and_apply_vocabulary_3-vocabulary-AnalysisIndex0-_369))+(ref_AppliedPTransform_Analyze-TensorSource-compute_and_apply_vocabulary_4-vocabulary-AnalysisIndex0-_444))+(ref_AppliedPTransform_Analyze-TensorSource-compute_and_apply_vocabulary_5-vocabulary-AnalysisIndex0-_519))+(ref_AppliedPTransform_Analyze-TensorSource-compute_and_apply_vocabulary_6-vocabulary-AnalysisIndex0-_594))+(ref_AppliedPTransform_Analyze-TensorSource-compute_and_apply_vocabulary_7-vocabulary-AnalysisIndex0-_669))+(ref_AppliedPTransform_Analyze-PackedCombineAccumulate-ApplySavedModel-Phase0-AnalysisIndex0-InitialP_66))+(Analyze/PackedCombineAccumulate[ApplySavedModel[Phase0][AnalysisIndex0]]/InitialPackedCombineGlobally/CombinePerKey/Flatten/Transcode/0))+(ref_AppliedPTransform_Analyze-PackedCombineAccumulate-ApplySavedModel-Phase0-AnalysisIndex0-InitialP_67))+(Analyze/PackedCombineAccumulate[ApplySavedModel[Phase0][AnalysisIndex0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PreCombineFn)/Precombine))+(Analyze/PackedCombineAccumulate[ApplySavedModel[Phase0][AnalysisIndex0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PreCombineFn)/Group/Write))+(Analyze/PackedCombineAccumulate[ApplySavedModel[Phase0][AnalysisIndex0]]/InitialPackedCombineGlobally/CombinePerKey/Flatten/Write/0))+(ref_AppliedPTransform_Analyze-VocabularyAccumulate-compute_and_apply_vocabulary-vocabulary-AnalysisI_146))+(ref_AppliedPTransform_Analyze-VocabularyAccumulate-compute_and_apply_vocabulary-vocabulary-AnalysisI_148))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Precombine))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Group/Write))+(ref_AppliedPTransform_Analyze-VocabularyAccumulate-compute_and_apply_vocabulary_1-vocabulary-Analysi_221))+(ref_AppliedPTransform_Analyze-VocabularyAccumulate-compute_and_apply_vocabulary_1-vocabulary-Analysi_223))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_1#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Precombine))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_1#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Group/Write))+(ref_AppliedPTransform_Analyze-VocabularyAccumulate-compute_and_apply_vocabulary_2-vocabulary-Analysi_296))+(ref_AppliedPTransform_Analyze-VocabularyAccumulate-compute_and_apply_vocabulary_2-vocabulary-Analysi_298))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_2#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Precombine))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_2#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Group/Write))+(ref_AppliedPTransform_Analyze-VocabularyAccumulate-compute_and_apply_vocabulary_3-vocabulary-Analysi_371))+(ref_AppliedPTransform_Analyze-VocabularyAccumulate-compute_and_apply_vocabulary_3-vocabulary-Analysi_373))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_3#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Precombine))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_3#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Group/Write))+(ref_AppliedPTransform_Analyze-VocabularyAccumulate-compute_and_apply_vocabulary_4-vocabulary-Analysi_446))+(ref_AppliedPTransform_Analyze-VocabularyAccumulate-compute_and_apply_vocabulary_4-vocabulary-Analysi_448))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_4#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Precombine))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_4#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Group/Write))+(ref_AppliedPTransform_Analyze-VocabularyAccumulate-compute_and_apply_vocabulary_5-vocabulary-Analysi_521))+(ref_AppliedPTransform_Analyze-VocabularyAccumulate-compute_and_apply_vocabulary_5-vocabulary-Analysi_523))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_5#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Precombine))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_5#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Group/Write))+(ref_AppliedPTransform_Analyze-VocabularyAccumulate-compute_and_apply_vocabulary_6-vocabulary-Analysi_596))+(ref_AppliedPTransform_Analyze-VocabularyAccumulate-compute_and_apply_vocabulary_6-vocabulary-Analysi_598))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_6#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Precombine))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_6#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Group/Write))+(ref_AppliedPTransform_Analyze-VocabularyAccumulate-compute_and_apply_vocabulary_7-vocabulary-Analysi_671))+(ref_AppliedPTransform_Analyze-VocabularyAccumulate-compute_and_apply_vocabulary_7-vocabulary-Analysi_673))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_7#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Precombine))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_7#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Group/Write))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-FilterInternalColumn_1079))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-K_1082))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-G_1085))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-G_1110))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/Precombine))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/Group/Write))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-G_1111))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/Flatten/Transcode/0))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PreCombineFn)/Precombine))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PreCombineFn)/Group/Write))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/Flatten/Write/0)\n",
      "Running ((((((Analyze/PackedCombineAccumulate[ApplySavedModel[Phase0][AnalysisIndex0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PreCombineFn)/Group/Read)+(Analyze/PackedCombineAccumulate[ApplySavedModel[Phase0][AnalysisIndex0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PreCombineFn)/Merge))+(Analyze/PackedCombineAccumulate[ApplySavedModel[Phase0][AnalysisIndex0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PreCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_Analyze-PackedCombineAccumulate-ApplySavedModel-Phase0-AnalysisIndex0-InitialP_72))+(ref_AppliedPTransform_Analyze-PackedCombineAccumulate-ApplySavedModel-Phase0-AnalysisIndex0-InitialP_73))+(Analyze/PackedCombineAccumulate[ApplySavedModel[Phase0][AnalysisIndex0]]/InitialPackedCombineGlobally/CombinePerKey/Flatten/Transcode/1))+(Analyze/PackedCombineAccumulate[ApplySavedModel[Phase0][AnalysisIndex0]]/InitialPackedCombineGlobally/CombinePerKey/Flatten/Write/1)\n",
      "Running ((Analyze/PackedCombineAccumulate[ApplySavedModel[Phase0][AnalysisIndex0]]/InitialPackedCombineGlobally/CombinePerKey/Flatten/Read)+(Analyze/PackedCombineAccumulate[ApplySavedModel[Phase0][AnalysisIndex0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/Precombine))+(Analyze/PackedCombineAccumulate[ApplySavedModel[Phase0][AnalysisIndex0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/Group/Write)\n",
      "Running ((((Analyze/PackedCombineAccumulate[ApplySavedModel[Phase0][AnalysisIndex0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/Group/Read)+(Analyze/PackedCombineAccumulate[ApplySavedModel[Phase0][AnalysisIndex0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/Merge))+(Analyze/PackedCombineAccumulate[ApplySavedModel[Phase0][AnalysisIndex0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_Analyze-PackedCombineAccumulate-ApplySavedModel-Phase0-AnalysisIndex0-InitialP_79))+(ref_PCollection_PCollection_42/Write)\n",
      "Running (((((ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex5-Write-WriteImpl-DoOnce-Impulse_985)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex5-Write-WriteImpl-DoOnce-FlatMap-_986))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex5-Write-WriteImpl-DoOnce-Map-deco_988))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex5-Write-WriteImpl-InitializeWrite_989))+(ref_PCollection_PCollection_564/Write))+(ref_PCollection_PCollection_565/Write)\n",
      "Running ((((((((((((((((((((((((((((((ref_AppliedPTransform_Analyze-PackedCombineAccumulate-ApplySavedModel-Phase0-AnalysisIndex0-InitialP_81)+(ref_AppliedPTransform_Analyze-PackedCombineAccumulate-ApplySavedModel-Phase0-AnalysisIndex0-InitialP_82))+(ref_AppliedPTransform_Analyze-PackedCombineAccumulate-ApplySavedModel-Phase0-AnalysisIndex0-InitialP_84))+(ref_AppliedPTransform_Analyze-PackedCombineAccumulate-ApplySavedModel-Phase0-AnalysisIndex0-InitialP_85))+(ref_AppliedPTransform_Analyze-CacheableCombineAccumulate-scale_to_z_score-mean_and_var-AnalysisIndex_94))+(ref_AppliedPTransform_Analyze-CacheableCombineAccumulate-scale_to_z_score_1-mean_and_var-AnalysisInd_100))+(ref_AppliedPTransform_Analyze-CacheableCombineAccumulate-scale_to_z_score_2-mean_and_var-AnalysisInd_106))+(ref_AppliedPTransform_Analyze-FlattenCache-CacheableCombineMerge-scale_to_z_score-mean_and_var-Flatt_96))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_z_score-mean_and_var-A_813))+(ref_AppliedPTransform_Analyze-AddKey-CacheableCombineMerge-scale_to_z_score-mean_and_var-AddKey_98))+(Analyze/FlattenInputForPackedCombineMerge[3]/Flatten/Write/0))+(ref_AppliedPTransform_Analyze-FlattenCache-CacheableCombineMerge-scale_to_z_score_1-mean_and_var-Fla_102))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_z_score_1-mean_and_var_822))+(ref_AppliedPTransform_Analyze-AddKey-CacheableCombineMerge-scale_to_z_score_1-mean_and_var-AddKey_104))+(Analyze/FlattenInputForPackedCombineMerge[3]/Flatten/Write/1))+(ref_AppliedPTransform_Analyze-FlattenCache-CacheableCombineMerge-scale_to_z_score_2-mean_and_var-Fla_108))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_z_score_2-mean_and_var_858))+(ref_AppliedPTransform_Analyze-AddKey-CacheableCombineMerge-scale_to_z_score_2-mean_and_var-AddKey_110))+(Analyze/FlattenInputForPackedCombineMerge[3]/Flatten/Write/2))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex4-Write-WriteImpl-WindowInto-Wind_974))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex5-Write-WriteImpl-WindowInto-Wind_990))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex9-Write-WriteImpl-WindowInto-Wind_1054))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex4-Write-WriteImpl-WriteBundles_975))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex4-Write-WriteImpl-Pair_976))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex4]/Write/WriteImpl/GroupByKey/Write))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex5-Write-WriteImpl-WriteBundles_991))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex5-Write-WriteImpl-Pair_992))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex5]/Write/WriteImpl/GroupByKey/Write))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex9-Write-WriteImpl-WriteBundles_1055))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex9-Write-WriteImpl-Pair_1056))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex9]/Write/WriteImpl/GroupByKey/Write)\n",
      "Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex9]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex9-Write-WriteImpl-Extract_1058))+(ref_PCollection_PCollection_614/Write)\n",
      "Running ((ref_PCollection_PCollection_608/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex9-Write-WriteImpl-PreFinalize_1059))+(ref_PCollection_PCollection_615/Write)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 0 files in 0.043036699295043945 seconds.\n",
      "Running (((((ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_7-vocabulary-Writ_724)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_7-vocabulary-Writ_725))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_7-vocabulary-Writ_727))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_7-vocabulary-Writ_728))+(ref_PCollection_PCollection_402/Write))+(ref_PCollection_PCollection_403/Write)\n",
      "Running (((((ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex10-Write-WriteImpl-DoOnce-Impulse_1065)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex10-Write-WriteImpl-DoOnce-FlatMap_1066))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex10-Write-WriteImpl-DoOnce-Map-dec_1068))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex10-Write-WriteImpl-InitializeWrit_1069))+(ref_PCollection_PCollection_619/Write))+(ref_PCollection_PCollection_620/Write)\n",
      "Running (((((((((((Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_7#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Group/Read)+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_7#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Merge))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_7#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/ExtractOutputs))+(Analyze/FlattenCache[VocabularyMerge[compute_and_apply_vocabulary_7#vocabulary]]/Flatten/Transcode/0))+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_7-vocabu_867))+(ref_AppliedPTransform_Analyze-FlattenCache-VocabularyMerge-compute_and_apply_vocabulary_7-vocabulary_679))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_7#vocabulary]/MergeCountPerToken/Precombine))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_7#vocabulary]/MergeCountPerToken/Group/Write))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex10-Write-WriteImpl-WindowInto-Win_1070))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex10-Write-WriteImpl-WriteBundles_1071))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex10-Write-WriteImpl-Pair_1072))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex10]/Write/WriteImpl/GroupByKey/Write)\n",
      "Running (((((((((((Analyze/VocabularyMerge[compute_and_apply_vocabulary_7#vocabulary]/MergeCountPerToken/Group/Read)+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_7#vocabulary]/MergeCountPerToken/Merge))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_7#vocabulary]/MergeCountPerToken/ExtractOutputs))+(ref_AppliedPTransform_Analyze-VocabularyMerge-compute_and_apply_vocabulary_7-vocabulary-SwapTokensAn_686))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_7-vocabulary-TotalVocabSi_690))+(ref_AppliedPTransform_Analyze-VocabularyPrune-compute_and_apply_vocabulary_7-vocabulary-KeepOnlyVali_706))+(Analyze/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Precombine))+(Analyze/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Write))+(ref_AppliedPTransform_Analyze-VocabularyPrune-compute_and_apply_vocabulary_7-vocabulary-ApplyThresho_708))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_7-vocabulary-Batc_712))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_7-vocabulary-Batc_713))+(ref_PCollection_PCollection_395/Write)\n",
      "Running ((((((ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_7-vocabulary-Prep_715)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_7-vocabulary-Prep_716))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_7-vocabulary-Prep_718))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_7-vocabulary-Orde_719))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_7-vocabulary-Writ_729))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_7-vocabulary-Writ_730))+(Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Write)\n",
      "Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex10]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex10-Write-WriteImpl-Extract_1074))+(ref_PCollection_PCollection_625/Write)\n",
      "Running ((ref_PCollection_PCollection_619/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex10-Write-WriteImpl-PreFinalize_1075))+(ref_PCollection_PCollection_626/Write)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 0 files in 0.033089399337768555 seconds.\n",
      "Running ((Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_7-vocabulary-Writ_732))+(ref_PCollection_PCollection_407/Write)\n",
      "Running (((((((((GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/Group/Read)+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/Merge))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/ExtractOutputs))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-G_1090))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/Precombine))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-G_1099))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/Group/Write))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-G_1101))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Precombine))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Group/Write)\n",
      "Running (((((ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex7-Write-WriteImpl-DoOnce-Impulse_1017)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex7-Write-WriteImpl-DoOnce-FlatMap-_1018))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex7-Write-WriteImpl-DoOnce-Map-deco_1020))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex7-Write-WriteImpl-InitializeWrite_1021))+(ref_PCollection_PCollection_586/Write))+(ref_PCollection_PCollection_587/Write)\n",
      "Running (((((((((((Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_5#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Group/Read)+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_5#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Merge))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_5#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/ExtractOutputs))+(Analyze/FlattenCache[VocabularyMerge[compute_and_apply_vocabulary_5#vocabulary]]/Flatten/Transcode/0))+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_5-vocabu_840))+(ref_AppliedPTransform_Analyze-FlattenCache-VocabularyMerge-compute_and_apply_vocabulary_5-vocabulary_529))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_5#vocabulary]/MergeCountPerToken/Precombine))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_5#vocabulary]/MergeCountPerToken/Group/Write))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex7-Write-WriteImpl-WindowInto-Wind_1022))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex7-Write-WriteImpl-WriteBundles_1023))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex7-Write-WriteImpl-Pair_1024))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex7]/Write/WriteImpl/GroupByKey/Write)\n",
      "Running (((((((((((Analyze/VocabularyMerge[compute_and_apply_vocabulary_5#vocabulary]/MergeCountPerToken/Group/Read)+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_5#vocabulary]/MergeCountPerToken/Merge))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_5#vocabulary]/MergeCountPerToken/ExtractOutputs))+(ref_AppliedPTransform_Analyze-VocabularyMerge-compute_and_apply_vocabulary_5-vocabulary-SwapTokensAn_536))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_5-vocabulary-TotalVocabSi_540))+(ref_AppliedPTransform_Analyze-VocabularyPrune-compute_and_apply_vocabulary_5-vocabulary-KeepOnlyVali_556))+(Analyze/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Precombine))+(Analyze/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Write))+(ref_AppliedPTransform_Analyze-VocabularyPrune-compute_and_apply_vocabulary_5-vocabulary-ApplyThresho_558))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_5-vocabulary-Batc_562))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_5-vocabulary-Batc_563))+(ref_PCollection_PCollection_311/Write)\n",
      "Running ((ref_PCollection_PCollection_402/Read)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_7-vocabulary-Writ_733))+(ref_PCollection_PCollection_408/Write)\n",
      "Running (((ref_AppliedPTransform_Analyze-InstrumentAPI-CreateSoleAPIUse-Impulse_37)+(ref_AppliedPTransform_Analyze-InstrumentAPI-CreateSoleAPIUse-FlatMap-lambda-at-core-py-3222-_38))+(ref_AppliedPTransform_Analyze-InstrumentAPI-CreateSoleAPIUse-Map-decode-_40))+(ref_AppliedPTransform_Analyze-InstrumentAPI-CountAPIUse_41)\n",
      "Running (((((ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-WriteSchema-Write-WriteImpl-DoOnce-Impu_1169)+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-WriteSchema-Write-WriteImpl-DoOnce-Flat_1170))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-WriteSchema-Write-WriteImpl-DoOnce-Map-_1172))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-WriteSchema-Write-WriteImpl-InitializeW_1173))+(ref_PCollection_PCollection_683/Write))+(ref_PCollection_PCollection_684/Write)\n",
      "Running (((((ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-CreateSchema-Impulse_1161)+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-CreateSchema-FlatMap-lambda-at-core-py-_1162))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-CreateSchema-Map-decode-_1164))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-WriteSchema-Write-WriteImpl-Map-lambda-_1174))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-WriteSchema-Write-WriteImpl-WindowInto-_1175))+(GenerateStats[FlattenedAnalysisDataset]/WriteSchema/Write/WriteImpl/GroupByKey/Write)\n",
      "Running ((GenerateStats[FlattenedAnalysisDataset]/WriteSchema/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-WriteSchema-Write-WriteImpl-WriteBundle_1177))+(ref_PCollection_PCollection_688/Write)\n",
      "Running ((ref_PCollection_PCollection_683/Read)+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-WriteSchema-Write-WriteImpl-PreFinalize_1178))+(ref_PCollection_PCollection_689/Write)\n",
      "Running ((ref_PCollection_PCollection_402/Read)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_7-vocabulary-Writ_734))+(ref_PCollection_PCollection_409/Write)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.042356014251708984 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Running (((((ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex2-Write-WriteImpl-DoOnce-Impulse_937)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex2-Write-WriteImpl-DoOnce-FlatMap-_938))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex2-Write-WriteImpl-DoOnce-Map-deco_940))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex2-Write-WriteImpl-InitializeWrite_941))+(ref_PCollection_PCollection_531/Write))+(ref_PCollection_PCollection_532/Write)\n",
      "Running (((((((((((Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_2#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Group/Read)+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_2#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Merge))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_2#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/ExtractOutputs))+(Analyze/FlattenCache[VocabularyMerge[compute_and_apply_vocabulary_2#vocabulary]]/Flatten/Transcode/0))+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_2-vocabu_795))+(ref_AppliedPTransform_Analyze-FlattenCache-VocabularyMerge-compute_and_apply_vocabulary_2-vocabulary_304))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_2#vocabulary]/MergeCountPerToken/Precombine))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_2#vocabulary]/MergeCountPerToken/Group/Write))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex2-Write-WriteImpl-WindowInto-Wind_942))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex2-Write-WriteImpl-WriteBundles_943))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex2-Write-WriteImpl-Pair_944))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex2]/Write/WriteImpl/GroupByKey/Write)\n",
      "Running (((((((((((Analyze/VocabularyMerge[compute_and_apply_vocabulary_2#vocabulary]/MergeCountPerToken/Group/Read)+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_2#vocabulary]/MergeCountPerToken/Merge))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_2#vocabulary]/MergeCountPerToken/ExtractOutputs))+(ref_AppliedPTransform_Analyze-VocabularyMerge-compute_and_apply_vocabulary_2-vocabulary-SwapTokensAn_311))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_2-vocabulary-TotalVocabSi_315))+(ref_AppliedPTransform_Analyze-VocabularyPrune-compute_and_apply_vocabulary_2-vocabulary-ApplyThresho_332))+(Analyze/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Precombine))+(Analyze/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Write))+(ref_AppliedPTransform_Analyze-VocabularyPrune-compute_and_apply_vocabulary_2-vocabulary-ApplyThresho_333))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_2-vocabulary-Batc_337))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_2-vocabulary-Batc_338))+(ref_PCollection_PCollection_185/Write)\n",
      "Running ((((Analyze/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Read)+(Analyze/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Merge))+(Analyze/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_2-vocabulary-TotalVocabSi_320))+(ref_PCollection_PCollection_175/Write)\n",
      "Running (((ref_AppliedPTransform_Analyze-CreateSavedModelForAnalyzerInputs-Phase0-tf_v2_only-Count-CreateSole-I_51)+(ref_AppliedPTransform_Analyze-CreateSavedModelForAnalyzerInputs-Phase0-tf_v2_only-Count-CreateSole-F_52))+(ref_AppliedPTransform_Analyze-CreateSavedModelForAnalyzerInputs-Phase0-tf_v2_only-Count-CreateSole-M_54))+(ref_AppliedPTransform_Analyze-CreateSavedModelForAnalyzerInputs-Phase0-tf_v2_only-Count-Count_55)\n",
      "Running ((((ref_AppliedPTransform_TFXIOReadAndDecode-TransformIndex1-RawRecordBeamSource-ReadRawRecords-ReadFrom_1214)+(ref_AppliedPTransform_TFXIOReadAndDecode-TransformIndex1-RawRecordBeamSource-ReadRawRecords-ReadFrom_1215))+(TFXIOReadAndDecode[TransformIndex1]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(TFXIOReadAndDecode[TransformIndex1]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_707_split/Write)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.04165148735046387 seconds.\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.039998769760131836 seconds.\n",
      "Running (((((ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_3-vocabulary-Writ_424)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_3-vocabulary-Writ_425))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_3-vocabulary-Writ_427))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_3-vocabulary-Writ_428))+(ref_PCollection_PCollection_234/Write))+(ref_PCollection_PCollection_235/Write)\n",
      "Running (((((ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex3-Write-WriteImpl-DoOnce-Impulse_953)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex3-Write-WriteImpl-DoOnce-FlatMap-_954))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex3-Write-WriteImpl-DoOnce-Map-deco_956))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex3-Write-WriteImpl-InitializeWrite_957))+(ref_PCollection_PCollection_542/Write))+(ref_PCollection_PCollection_543/Write)\n",
      "Running (((((((((((Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_3#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Group/Read)+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_3#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Merge))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_3#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/ExtractOutputs))+(Analyze/FlattenCache[VocabularyMerge[compute_and_apply_vocabulary_3#vocabulary]]/Flatten/Transcode/0))+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_3-vocabu_804))+(ref_AppliedPTransform_Analyze-FlattenCache-VocabularyMerge-compute_and_apply_vocabulary_3-vocabulary_379))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_3#vocabulary]/MergeCountPerToken/Precombine))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_3#vocabulary]/MergeCountPerToken/Group/Write))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex3-Write-WriteImpl-WindowInto-Wind_958))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex3-Write-WriteImpl-WriteBundles_959))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex3-Write-WriteImpl-Pair_960))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex3]/Write/WriteImpl/GroupByKey/Write)\n",
      "Running (((((((((((Analyze/VocabularyMerge[compute_and_apply_vocabulary_3#vocabulary]/MergeCountPerToken/Group/Read)+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_3#vocabulary]/MergeCountPerToken/Merge))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_3#vocabulary]/MergeCountPerToken/ExtractOutputs))+(ref_AppliedPTransform_Analyze-VocabularyMerge-compute_and_apply_vocabulary_3-vocabulary-SwapTokensAn_386))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_3-vocabulary-TotalVocabSi_390))+(ref_AppliedPTransform_Analyze-VocabularyPrune-compute_and_apply_vocabulary_3-vocabulary-ApplyThresho_407))+(Analyze/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Precombine))+(Analyze/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Write))+(ref_AppliedPTransform_Analyze-VocabularyPrune-compute_and_apply_vocabulary_3-vocabulary-ApplyThresho_408))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_3-vocabulary-Batc_412))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_3-vocabulary-Batc_413))+(ref_PCollection_PCollection_227/Write)\n",
      "Running ((((((ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_3-vocabulary-Prep_415)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_3-vocabulary-Prep_416))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_3-vocabulary-Prep_418))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_3-vocabulary-Orde_419))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_3-vocabulary-Writ_429))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_3-vocabulary-Writ_430))+(Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Write)\n",
      "Running ((Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_3-vocabulary-Writ_432))+(ref_PCollection_PCollection_239/Write)\n",
      "Running ((ref_PCollection_PCollection_234/Read)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_3-vocabulary-Writ_433))+(ref_PCollection_PCollection_240/Write)\n",
      "Running ((ref_PCollection_PCollection_234/Read)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_3-vocabulary-Writ_434))+(ref_PCollection_PCollection_241/Write)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.04347968101501465 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Running (((((ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_3-vocabulary-Crea_436)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_3-vocabulary-Crea_437))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_3-vocabulary-Crea_439))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_3-vocabulary-Wait_440))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-compute_and_apply_vocabulary_3-vocabulary-temporar_442))+(ref_PCollection_PCollection_246/Write)\n",
      "Running (((((ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_4-vocabulary-Writ_499)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_4-vocabulary-Writ_500))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_4-vocabulary-Writ_502))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_4-vocabulary-Writ_503))+(ref_PCollection_PCollection_276/Write))+(ref_PCollection_PCollection_277/Write)\n",
      "Running (((((ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex6-Write-WriteImpl-DoOnce-Impulse_1001)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex6-Write-WriteImpl-DoOnce-FlatMap-_1002))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex6-Write-WriteImpl-DoOnce-Map-deco_1004))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex6-Write-WriteImpl-InitializeWrite_1005))+(ref_PCollection_PCollection_575/Write))+(ref_PCollection_PCollection_576/Write)\n",
      "Running (((((((((((Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_4#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Group/Read)+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_4#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Merge))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_4#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/ExtractOutputs))+(Analyze/FlattenCache[VocabularyMerge[compute_and_apply_vocabulary_4#vocabulary]]/Flatten/Transcode/0))+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_4-vocabu_831))+(ref_AppliedPTransform_Analyze-FlattenCache-VocabularyMerge-compute_and_apply_vocabulary_4-vocabulary_454))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_4#vocabulary]/MergeCountPerToken/Precombine))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_4#vocabulary]/MergeCountPerToken/Group/Write))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex6-Write-WriteImpl-WindowInto-Wind_1006))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex6-Write-WriteImpl-WriteBundles_1007))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex6-Write-WriteImpl-Pair_1008))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex6]/Write/WriteImpl/GroupByKey/Write)\n",
      "Running (((((((((((Analyze/VocabularyMerge[compute_and_apply_vocabulary_4#vocabulary]/MergeCountPerToken/Group/Read)+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_4#vocabulary]/MergeCountPerToken/Merge))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_4#vocabulary]/MergeCountPerToken/ExtractOutputs))+(ref_AppliedPTransform_Analyze-VocabularyMerge-compute_and_apply_vocabulary_4-vocabulary-SwapTokensAn_461))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_4-vocabulary-TotalVocabSi_465))+(ref_AppliedPTransform_Analyze-VocabularyPrune-compute_and_apply_vocabulary_4-vocabulary-KeepOnlyVali_481))+(Analyze/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Precombine))+(Analyze/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Write))+(ref_AppliedPTransform_Analyze-VocabularyPrune-compute_and_apply_vocabulary_4-vocabulary-ApplyThresho_483))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_4-vocabulary-Batc_487))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_4-vocabulary-Batc_488))+(ref_PCollection_PCollection_269/Write)\n",
      "Running ((((((ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_4-vocabulary-Prep_490)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_4-vocabulary-Prep_491))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_4-vocabulary-Prep_493))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_4-vocabulary-Orde_494))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_4-vocabulary-Writ_504))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_4-vocabulary-Writ_505))+(Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Write)\n",
      "Running ((Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_4-vocabulary-Writ_507))+(ref_PCollection_PCollection_281/Write)\n",
      "Running ((ref_PCollection_PCollection_276/Read)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_4-vocabulary-Writ_508))+(ref_PCollection_PCollection_282/Write)\n",
      "Running ((ref_PCollection_PCollection_276/Read)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_4-vocabulary-Writ_509))+(ref_PCollection_PCollection_283/Write)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.04717278480529785 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.40 seconds.\n",
      "Running (((((ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_4-vocabulary-Crea_511)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_4-vocabulary-Crea_512))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_4-vocabulary-Crea_514))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_4-vocabulary-Wait_515))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-compute_and_apply_vocabulary_4-vocabulary-temporar_517))+(ref_PCollection_PCollection_288/Write)\n",
      "Running (((((ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary-vocabulary-WriteT_199)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary-vocabulary-WriteT_200))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary-vocabulary-WriteT_202))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary-vocabulary-WriteT_203))+(ref_PCollection_PCollection_108/Write))+(ref_PCollection_PCollection_109/Write)\n",
      "Running (((((ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex0-Write-WriteImpl-DoOnce-Impulse_905)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex0-Write-WriteImpl-DoOnce-FlatMap-_906))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex0-Write-WriteImpl-DoOnce-Map-deco_908))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex0-Write-WriteImpl-InitializeWrite_909))+(ref_PCollection_PCollection_509/Write))+(ref_PCollection_PCollection_510/Write)\n",
      "Running (((((((((((Analyze/VocabularyAccumulate[compute_and_apply_vocabulary#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Group/Read)+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Merge))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/ExtractOutputs))+(Analyze/FlattenCache[VocabularyMerge[compute_and_apply_vocabulary#vocabulary]]/Flatten/Transcode/0))+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary-vocabula_777))+(ref_AppliedPTransform_Analyze-FlattenCache-VocabularyMerge-compute_and_apply_vocabulary-vocabulary-F_154))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary#vocabulary]/MergeCountPerToken/Precombine))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary#vocabulary]/MergeCountPerToken/Group/Write))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex0-Write-WriteImpl-WindowInto-Wind_910))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex0-Write-WriteImpl-WriteBundles_911))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex0-Write-WriteImpl-Pair_912))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex0]/Write/WriteImpl/GroupByKey/Write)\n",
      "Running (((((((((((Analyze/VocabularyMerge[compute_and_apply_vocabulary#vocabulary]/MergeCountPerToken/Group/Read)+(Analyze/VocabularyMerge[compute_and_apply_vocabulary#vocabulary]/MergeCountPerToken/Merge))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary#vocabulary]/MergeCountPerToken/ExtractOutputs))+(ref_AppliedPTransform_Analyze-VocabularyMerge-compute_and_apply_vocabulary-vocabulary-SwapTokensAndC_161))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary-vocabulary-TotalVocabSize_165))+(ref_AppliedPTransform_Analyze-VocabularyPrune-compute_and_apply_vocabulary-vocabulary-ApplyThreshold_182))+(Analyze/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Precombine))+(Analyze/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Write))+(ref_AppliedPTransform_Analyze-VocabularyPrune-compute_and_apply_vocabulary-vocabulary-ApplyThreshold_183))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary-vocabulary-BatchA_187))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary-vocabulary-BatchA_188))+(ref_PCollection_PCollection_101/Write)\n",
      "Running ((((((ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary-vocabulary-Prepar_190)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary-vocabulary-Prepar_191))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary-vocabulary-Prepar_193))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary-vocabulary-OrderE_194))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary-vocabulary-WriteT_204))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary-vocabulary-WriteT_205))+(Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Write)\n",
      "Running ((Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary-vocabulary-WriteT_207))+(ref_PCollection_PCollection_113/Write)\n",
      "Running ((ref_PCollection_PCollection_108/Read)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary-vocabulary-WriteT_208))+(ref_PCollection_PCollection_114/Write)\n",
      "Running ((ref_PCollection_PCollection_108/Read)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary-vocabulary-WriteT_209))+(ref_PCollection_PCollection_115/Write)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.037926673889160156 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Running (((((ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary-vocabulary-Create_211)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary-vocabulary-Create_212))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary-vocabulary-Create_214))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary-vocabulary-WaitFo_215))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-compute_and_apply_vocabulary-vocabulary-temporary__217))+(ref_PCollection_PCollection_120/Write)\n",
      "Running ((((((ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_5-vocabulary-Prep_565)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_5-vocabulary-Prep_566))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_5-vocabulary-Prep_568))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_5-vocabulary-Orde_569))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_5-vocabulary-Writ_579))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_5-vocabulary-Writ_580))+(Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Write)\n",
      "Running (((((ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_5-vocabulary-Writ_574)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_5-vocabulary-Writ_575))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_5-vocabulary-Writ_577))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_5-vocabulary-Writ_578))+(ref_PCollection_PCollection_318/Write))+(ref_PCollection_PCollection_319/Write)\n",
      "Running ((Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_5-vocabulary-Writ_582))+(ref_PCollection_PCollection_323/Write)\n",
      "Running ((ref_PCollection_PCollection_318/Read)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_5-vocabulary-Writ_583))+(ref_PCollection_PCollection_324/Write)\n",
      "Running ((ref_PCollection_PCollection_318/Read)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_5-vocabulary-Writ_584))+(ref_PCollection_PCollection_325/Write)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.05375266075134277 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Running (((((ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_5-vocabulary-Crea_586)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_5-vocabulary-Crea_587))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_5-vocabulary-Crea_589))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_5-vocabulary-Wait_590))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-compute_and_apply_vocabulary_5-vocabulary-temporar_592))+(ref_PCollection_PCollection_330/Write)\n",
      "Running (((((ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex8-Write-WriteImpl-DoOnce-Impulse_1033)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex8-Write-WriteImpl-DoOnce-FlatMap-_1034))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex8-Write-WriteImpl-DoOnce-Map-deco_1036))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex8-Write-WriteImpl-InitializeWrite_1037))+(ref_PCollection_PCollection_597/Write))+(ref_PCollection_PCollection_598/Write)\n",
      "Running (((((((((((Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_6#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Group/Read)+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_6#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Merge))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_6#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/ExtractOutputs))+(Analyze/FlattenCache[VocabularyMerge[compute_and_apply_vocabulary_6#vocabulary]]/Flatten/Transcode/0))+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_6-vocabu_849))+(ref_AppliedPTransform_Analyze-FlattenCache-VocabularyMerge-compute_and_apply_vocabulary_6-vocabulary_604))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_6#vocabulary]/MergeCountPerToken/Precombine))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_6#vocabulary]/MergeCountPerToken/Group/Write))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex8-Write-WriteImpl-WindowInto-Wind_1038))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex8-Write-WriteImpl-WriteBundles_1039))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex8-Write-WriteImpl-Pair_1040))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex8]/Write/WriteImpl/GroupByKey/Write)\n",
      "Running (((((((((((Analyze/VocabularyMerge[compute_and_apply_vocabulary_6#vocabulary]/MergeCountPerToken/Group/Read)+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_6#vocabulary]/MergeCountPerToken/Merge))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_6#vocabulary]/MergeCountPerToken/ExtractOutputs))+(ref_AppliedPTransform_Analyze-VocabularyMerge-compute_and_apply_vocabulary_6-vocabulary-SwapTokensAn_611))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_6-vocabulary-TotalVocabSi_615))+(ref_AppliedPTransform_Analyze-VocabularyPrune-compute_and_apply_vocabulary_6-vocabulary-KeepOnlyVali_631))+(Analyze/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Precombine))+(Analyze/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Write))+(ref_AppliedPTransform_Analyze-VocabularyPrune-compute_and_apply_vocabulary_6-vocabulary-ApplyThresho_633))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_6-vocabulary-Batc_637))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_6-vocabulary-Batc_638))+(ref_PCollection_PCollection_353/Write)\n",
      "Running ((((((ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_6-vocabulary-Prep_640)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_6-vocabulary-Prep_641))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_6-vocabulary-Prep_643))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_6-vocabulary-Orde_644))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_6-vocabulary-Writ_654))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_6-vocabulary-Writ_655))+(Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Write)\n",
      "Running (((((ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_6-vocabulary-Writ_649)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_6-vocabulary-Writ_650))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_6-vocabulary-Writ_652))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_6-vocabulary-Writ_653))+(ref_PCollection_PCollection_360/Write))+(ref_PCollection_PCollection_361/Write)\n",
      "Running ((Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_6-vocabulary-Writ_657))+(ref_PCollection_PCollection_365/Write)\n",
      "Running ((ref_PCollection_PCollection_360/Read)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_6-vocabulary-Writ_658))+(ref_PCollection_PCollection_366/Write)\n",
      "Running ((ref_PCollection_PCollection_360/Read)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_6-vocabulary-Writ_659))+(ref_PCollection_PCollection_367/Write)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.05272102355957031 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.40 seconds.\n",
      "Running (((((ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_6-vocabulary-Crea_661)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_6-vocabulary-Crea_662))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_6-vocabulary-Crea_664))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_6-vocabulary-Wait_665))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-compute_and_apply_vocabulary_6-vocabulary-temporar_667))+(ref_PCollection_PCollection_372/Write)\n",
      "Running (((((ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex1-Write-WriteImpl-DoOnce-Impulse_921)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex1-Write-WriteImpl-DoOnce-FlatMap-_922))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex1-Write-WriteImpl-DoOnce-Map-deco_924))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex1-Write-WriteImpl-InitializeWrite_925))+(ref_PCollection_PCollection_520/Write))+(ref_PCollection_PCollection_521/Write)\n",
      "Running (((((((((((Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_1#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Group/Read)+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_1#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Merge))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_1#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/ExtractOutputs))+(Analyze/FlattenCache[VocabularyMerge[compute_and_apply_vocabulary_1#vocabulary]]/Flatten/Transcode/0))+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_1-vocabu_786))+(ref_AppliedPTransform_Analyze-FlattenCache-VocabularyMerge-compute_and_apply_vocabulary_1-vocabulary_229))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_1#vocabulary]/MergeCountPerToken/Precombine))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_1#vocabulary]/MergeCountPerToken/Group/Write))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex1-Write-WriteImpl-WindowInto-Wind_926))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex1-Write-WriteImpl-WriteBundles_927))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex1-Write-WriteImpl-Pair_928))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex1]/Write/WriteImpl/GroupByKey/Write)\n",
      "Running (((((((((((Analyze/VocabularyMerge[compute_and_apply_vocabulary_1#vocabulary]/MergeCountPerToken/Group/Read)+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_1#vocabulary]/MergeCountPerToken/Merge))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_1#vocabulary]/MergeCountPerToken/ExtractOutputs))+(ref_AppliedPTransform_Analyze-VocabularyMerge-compute_and_apply_vocabulary_1-vocabulary-SwapTokensAn_236))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_1-vocabulary-TotalVocabSi_240))+(ref_AppliedPTransform_Analyze-VocabularyPrune-compute_and_apply_vocabulary_1-vocabulary-ApplyThresho_257))+(Analyze/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Precombine))+(Analyze/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Write))+(ref_AppliedPTransform_Analyze-VocabularyPrune-compute_and_apply_vocabulary_1-vocabulary-ApplyThresho_258))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_1-vocabulary-Batc_262))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_1-vocabulary-Batc_263))+(ref_PCollection_PCollection_143/Write)\n",
      "Running ((((Analyze/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Read)+(Analyze/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Merge))+(Analyze/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_1-vocabulary-TotalVocabSi_245))+(ref_PCollection_PCollection_133/Write)\n",
      "Running ((((((ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_1-vocabulary-TotalVocabSi_247)+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_1-vocabulary-TotalVocabSi_248))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_1-vocabulary-TotalVocabSi_250))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_1-vocabulary-TotalVocabSi_251))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_1-vocabulary-ToInt64_252))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-compute_and_apply_vocabulary_1-vocabulary-temporar_254))+(ref_PCollection_PCollection_139/Write)\n",
      "Running ((((Analyze/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Read)+(Analyze/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Merge))+(Analyze/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_3-vocabulary-TotalVocabSi_395))+(ref_PCollection_PCollection_217/Write)\n",
      "Running ((((((ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_3-vocabulary-TotalVocabSi_397)+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_3-vocabulary-TotalVocabSi_398))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_3-vocabulary-TotalVocabSi_400))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_3-vocabulary-TotalVocabSi_401))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_3-vocabulary-ToInt64_402))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-compute_and_apply_vocabulary_3-vocabulary-temporar_404))+(ref_PCollection_PCollection_223/Write)\n",
      "Running ((((Analyze/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Read)+(Analyze/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Merge))+(Analyze/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary-vocabulary-TotalVocabSize_170))+(ref_PCollection_PCollection_91/Write)\n",
      "Running ((((((ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary-vocabulary-TotalVocabSize_172)+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary-vocabulary-TotalVocabSize_173))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary-vocabulary-TotalVocabSize_175))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary-vocabulary-TotalVocabSize_176))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary-vocabulary-ToInt64_177))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-compute_and_apply_vocabulary-vocabulary-temporary__179))+(ref_PCollection_PCollection_97/Write)\n",
      "Running ((((Analyze/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Read)+(Analyze/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Merge))+(Analyze/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_4-vocabulary-TotalVocabSi_470))+(ref_PCollection_PCollection_259/Write)\n",
      "Running ((((((ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_4-vocabulary-TotalVocabSi_472)+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_4-vocabulary-TotalVocabSi_473))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_4-vocabulary-TotalVocabSi_475))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_4-vocabulary-TotalVocabSi_476))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_4-vocabulary-ToInt64_477))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-compute_and_apply_vocabulary_4-vocabulary-temporar_479))+(ref_PCollection_PCollection_265/Write)\n",
      "Running (((Analyze/FlattenInputForPackedCombineMerge[3]/Flatten/Read)+(ref_AppliedPTransform_Analyze-PackedCombineMerge-3-MergePackedCombinesGlobally-KeyWithVoid_115))+(Analyze/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/Precombine))+(Analyze/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/Group/Write)\n",
      "Running ((((Analyze/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/Group/Read)+(Analyze/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/Merge))+(Analyze/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_Analyze-PackedCombineMerge-3-MergePackedCombinesGlobally-UnKey_120))+(ref_PCollection_PCollection_64/Write)\n",
      "Running (((((((((((((((((((((ref_AppliedPTransform_Analyze-PackedCombineMerge-3-MergePackedCombinesGlobally-DoOnce-Impulse_122)+(ref_AppliedPTransform_Analyze-PackedCombineMerge-3-MergePackedCombinesGlobally-DoOnce-FlatMap-lambda_123))+(ref_AppliedPTransform_Analyze-PackedCombineMerge-3-MergePackedCombinesGlobally-DoOnce-Map-decode-_125))+(ref_AppliedPTransform_Analyze-PackedCombineMerge-3-MergePackedCombinesGlobally-InjectDefault_126))+(ref_AppliedPTransform_Analyze-ExtractFromDict-CacheableCombineMerge-scale_to_z_score_2-mean_and_var-_135))+(ref_AppliedPTransform_Analyze-ExtractFromDict-CacheableCombineMerge-scale_to_z_score-mean_and_var-Ex_744))+(ref_AppliedPTransform_Analyze-ExtractFromDict-CacheableCombineMerge-scale_to_z_score_1-mean_and_var-_753))+(ref_AppliedPTransform_Analyze-ExtractPackedCombineMergeOutputs-CacheableCombineMerge-scale_to_z_scor_138))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_z_score_2-mean_and_var-temporary_analyzer_142))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_z_score_2-mean_and_var-temporary_analyzer_140))+(ref_PCollection_PCollection_77/Write))+(ref_PCollection_PCollection_78/Write))+(ref_AppliedPTransform_Analyze-ExtractPackedCombineMergeOutputs-CacheableCombineMerge-scale_to_z_scor_747))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_z_score-mean_and_var-temporary_analyzer_o_749))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_z_score-mean_and_var-temporary_analyzer_o_751))+(ref_PCollection_PCollection_419/Write))+(ref_PCollection_PCollection_420/Write))+(ref_AppliedPTransform_Analyze-ExtractPackedCombineMergeOutputs-CacheableCombineMerge-scale_to_z_scor_756))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_z_score_1-mean_and_var-temporary_analyzer_760))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_z_score_1-mean_and_var-temporary_analyzer_758))+(ref_PCollection_PCollection_425/Write))+(ref_PCollection_PCollection_426/Write)\n",
      "Running ((((((ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_2-vocabulary-TotalVocabSi_322)+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_2-vocabulary-TotalVocabSi_323))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_2-vocabulary-TotalVocabSi_325))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_2-vocabulary-TotalVocabSi_326))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_2-vocabulary-ToInt64_327))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-compute_and_apply_vocabulary_2-vocabulary-temporar_329))+(ref_PCollection_PCollection_181/Write)\n",
      "Running ((((((ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_2-vocabulary-Prep_340)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_2-vocabulary-Prep_341))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_2-vocabulary-Prep_343))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_2-vocabulary-Orde_344))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_2-vocabulary-Writ_354))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_2-vocabulary-Writ_355))+(Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Write)\n",
      "Running (((((ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_2-vocabulary-Writ_349)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_2-vocabulary-Writ_350))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_2-vocabulary-Writ_352))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_2-vocabulary-Writ_353))+(ref_PCollection_PCollection_192/Write))+(ref_PCollection_PCollection_193/Write)\n",
      "Running ((Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_2-vocabulary-Writ_357))+(ref_PCollection_PCollection_197/Write)\n",
      "Running ((ref_PCollection_PCollection_192/Read)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_2-vocabulary-Writ_358))+(ref_PCollection_PCollection_198/Write)\n",
      "Running ((ref_PCollection_PCollection_192/Read)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_2-vocabulary-Writ_359))+(ref_PCollection_PCollection_199/Write)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.043840646743774414 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Running (((((ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_2-vocabulary-Crea_361)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_2-vocabulary-Crea_362))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_2-vocabulary-Crea_364))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_2-vocabulary-Wait_365))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-compute_and_apply_vocabulary_2-vocabulary-temporar_367))+(ref_PCollection_PCollection_204/Write)\n",
      "Running ((((Analyze/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Read)+(Analyze/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Merge))+(Analyze/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_5-vocabulary-TotalVocabSi_545))+(ref_PCollection_PCollection_301/Write)\n",
      "Running ((((((ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_5-vocabulary-TotalVocabSi_547)+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_5-vocabulary-TotalVocabSi_548))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_5-vocabulary-TotalVocabSi_550))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_5-vocabulary-TotalVocabSi_551))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_5-vocabulary-ToInt64_552))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-compute_and_apply_vocabulary_5-vocabulary-temporar_554))+(ref_PCollection_PCollection_307/Write)\n",
      "Running ((((Analyze/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Read)+(Analyze/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Merge))+(Analyze/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_6-vocabulary-TotalVocabSi_620))+(ref_PCollection_PCollection_343/Write)\n",
      "Running ((((((ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_6-vocabulary-TotalVocabSi_622)+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_6-vocabulary-TotalVocabSi_623))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_6-vocabulary-TotalVocabSi_625))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_6-vocabulary-TotalVocabSi_626))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_6-vocabulary-ToInt64_627))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-compute_and_apply_vocabulary_6-vocabulary-temporar_629))+(ref_PCollection_PCollection_349/Write)\n",
      "Running ((((Analyze/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Read)+(Analyze/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Merge))+(Analyze/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_7-vocabulary-TotalVocabSi_695))+(ref_PCollection_PCollection_385/Write)\n",
      "Running ((((((ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_7-vocabulary-TotalVocabSi_697)+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_7-vocabulary-TotalVocabSi_698))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_7-vocabulary-TotalVocabSi_700))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_7-vocabulary-TotalVocabSi_701))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_7-vocabulary-ToInt64_702))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-compute_and_apply_vocabulary_7-vocabulary-temporar_704))+(ref_PCollection_PCollection_391/Write)\n",
      "Running (((((ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_7-vocabulary-Crea_736)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_7-vocabulary-Crea_737))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_7-vocabulary-Crea_739))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_7-vocabulary-Wait_740))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-compute_and_apply_vocabulary_7-vocabulary-temporar_742))+(ref_PCollection_PCollection_414/Write)\n",
      "Running ((((((ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_1-vocabulary-Prep_265)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_1-vocabulary-Prep_266))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_1-vocabulary-Prep_268))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_1-vocabulary-Orde_269))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_1-vocabulary-Writ_279))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_1-vocabulary-Writ_280))+(Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Write)\n",
      "Running (((((ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_1-vocabulary-Writ_274)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_1-vocabulary-Writ_275))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_1-vocabulary-Writ_277))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_1-vocabulary-Writ_278))+(ref_PCollection_PCollection_150/Write))+(ref_PCollection_PCollection_151/Write)\n",
      "Running ((Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_1-vocabulary-Writ_282))+(ref_PCollection_PCollection_155/Write)\n",
      "Running ((ref_PCollection_PCollection_150/Read)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_1-vocabulary-Writ_283))+(ref_PCollection_PCollection_156/Write)\n",
      "Running ((ref_PCollection_PCollection_150/Read)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_1-vocabulary-Writ_284))+(ref_PCollection_PCollection_157/Write)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.056833505630493164 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.40 seconds.\n",
      "Running (((((ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_1-vocabulary-Crea_286)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_1-vocabulary-Crea_287))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_1-vocabulary-Crea_289))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_1-vocabulary-Wait_290))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-compute_and_apply_vocabulary_1-vocabulary-temporar_292))+(ref_PCollection_PCollection_162/Write)\n",
      "Running ((((((((((((ref_AppliedPTransform_Analyze-CreateSavedModel-tf_v2_only-CreateSole-Impulse_763)+(ref_AppliedPTransform_Analyze-CreateSavedModel-tf_v2_only-CreateSole-FlatMap-lambda-at-core-py-3222-_764))+(ref_AppliedPTransform_Analyze-CreateSavedModel-tf_v2_only-CreateSole-Map-decode-_766))+(ref_AppliedPTransform_Analyze-CreateSavedModel-tf_v2_only-ReplaceWithConstants_767))+(ref_AppliedPTransform_Analyze-CreateSavedModel-tf_v2_only-CreateSavedModel_768))+(ref_AppliedPTransform_Analyze-ComputeDeferredMetadata-compat_v1-False-_875))+(ref_AppliedPTransform_Analyze-MakeCheapBarrier_876))+(ref_AppliedPTransform_WriteTransformFn-WriteTransformFnToTemp_893))+(ref_PCollection_PCollection_431/Write))+(ref_AppliedPTransform_WriteTransformFn-WriteMetadataToTemp-WriteMetadata_892))+(ref_PCollection_PCollection_492/Write))+(ref_PCollection_PCollection_501/Write))+(ref_PCollection_PCollection_502/Write)\n",
      "Assets written to: gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/DataTransformer/transform_graph/9/.temp_path/tftransform_tmp/c9b55b8256534c6997fdf7f23d205de8/assets\n",
      "Running ((((((((((((((((((ref_PCollection_PCollection_707_split/Read)+(TFXIOReadAndDecode[TransformIndex1]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_TFXIOReadAndDecode-TransformIndex1-RawRecordBeamSource-ReadRawRecords-FlattenP_1218))+(ref_AppliedPTransform_TFXIOReadAndDecode-TransformIndex1-RawRecordBeamSource-CollectRawRecordTelemet_1220))+(ref_AppliedPTransform_TFXIOReadAndDecode-TransformIndex1-RawRecordToRecordBatch-RawRecordToRecordBat_1224))+(ref_AppliedPTransform_TFXIOReadAndDecode-TransformIndex1-RawRecordToRecordBatch-RawRecordToRecordBat_1225))+(ref_AppliedPTransform_TFXIOReadAndDecode-TransformIndex1-RawRecordToRecordBatch-CollectRecordBatchTe_1227))+(ref_AppliedPTransform_Transform-TransformIndex1-Transform_1229))+(ref_AppliedPTransform_Transform-TransformIndex1-ConvertToRecordBatch_1230))+(ref_AppliedPTransform_Transform-TransformIndex1-MakeCheapBarrier_1231))+(ref_AppliedPTransform_ExtractRecordBatches-TransformIndex1-Keys_1241))+(ref_AppliedPTransform_EncodeAndSerialize-TransformIndex1-_1382))+(ref_PCollection_PCollection_716/Write))+(FlattenTransformedDatasets/Write/1))+(ref_AppliedPTransform_Materialize-TransformIndex1-Values-Values_1385))+(ref_AppliedPTransform_Materialize-TransformIndex1-Write-Write-WriteImpl-WindowInto-WindowIntoFn-_1395))+(ref_AppliedPTransform_Materialize-TransformIndex1-Write-Write-WriteImpl-WriteBundles_1396))+(ref_AppliedPTransform_Materialize-TransformIndex1-Write-Write-WriteImpl-Pair_1397))+(Materialize[TransformIndex1]/Write/Write/WriteImpl/GroupByKey/Write)\n",
      "Running ((Materialize[TransformIndex1]/Write/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_Materialize-TransformIndex1-Write-Write-WriteImpl-Extract_1399))+(ref_PCollection_PCollection_821/Write)\n",
      "Running ((((ref_AppliedPTransform_WriteTransformFn-CreateSole-Impulse_895)+(ref_AppliedPTransform_WriteTransformFn-CreateSole-FlatMap-lambda-at-core-py-3222-_896))+(ref_AppliedPTransform_WriteTransformFn-CreateSole-Map-decode-_898))+(ref_AppliedPTransform_WriteTransformFn-PublishMetadataAndTransformFn_899))+(ref_PCollection_PCollection_506/Write)\n",
      "Running ((((ref_AppliedPTransform_TFXIOReadAndDecode-TransformIndex0-RawRecordBeamSource-ReadRawRecords-ReadFrom_1185)+(ref_AppliedPTransform_TFXIOReadAndDecode-TransformIndex0-RawRecordBeamSource-ReadRawRecords-ReadFrom_1186))+(TFXIOReadAndDecode[TransformIndex0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(TFXIOReadAndDecode[TransformIndex0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_692_split/Write)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.03535962104797363 seconds.\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.03950095176696777 seconds.\n",
      "Running (((((ref_AppliedPTransform_Materialize-TransformIndex0-Write-Write-WriteImpl-DoOnce-Impulse_1370)+(ref_AppliedPTransform_Materialize-TransformIndex0-Write-Write-WriteImpl-DoOnce-FlatMap-lambda-at-cor_1371))+(ref_AppliedPTransform_Materialize-TransformIndex0-Write-Write-WriteImpl-DoOnce-Map-decode-_1373))+(ref_AppliedPTransform_Materialize-TransformIndex0-Write-Write-WriteImpl-InitializeWrite_1374))+(ref_PCollection_PCollection_802/Write))+(ref_PCollection_PCollection_803/Write)\n",
      "Running ((((((((((((((((((ref_PCollection_PCollection_692_split/Read)+(TFXIOReadAndDecode[TransformIndex0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_TFXIOReadAndDecode-TransformIndex0-RawRecordBeamSource-ReadRawRecords-FlattenP_1189))+(ref_AppliedPTransform_TFXIOReadAndDecode-TransformIndex0-RawRecordBeamSource-CollectRawRecordTelemet_1191))+(ref_AppliedPTransform_TFXIOReadAndDecode-TransformIndex0-RawRecordToRecordBatch-RawRecordToRecordBat_1195))+(ref_AppliedPTransform_TFXIOReadAndDecode-TransformIndex0-RawRecordToRecordBatch-RawRecordToRecordBat_1196))+(ref_AppliedPTransform_TFXIOReadAndDecode-TransformIndex0-RawRecordToRecordBatch-CollectRecordBatchTe_1198))+(ref_AppliedPTransform_Transform-TransformIndex0-Transform_1200))+(ref_AppliedPTransform_Transform-TransformIndex0-ConvertToRecordBatch_1201))+(ref_AppliedPTransform_Transform-TransformIndex0-MakeCheapBarrier_1202))+(ref_AppliedPTransform_ExtractRecordBatches-TransformIndex0-Keys_1239))+(ref_AppliedPTransform_EncodeAndSerialize-TransformIndex0-_1362))+(ref_PCollection_PCollection_701/Write))+(FlattenTransformedDatasets/Write/0))+(ref_AppliedPTransform_Materialize-TransformIndex0-Values-Values_1365))+(ref_AppliedPTransform_Materialize-TransformIndex0-Write-Write-WriteImpl-WindowInto-WindowIntoFn-_1375))+(ref_AppliedPTransform_Materialize-TransformIndex0-Write-Write-WriteImpl-WriteBundles_1376))+(ref_AppliedPTransform_Materialize-TransformIndex0-Write-Write-WriteImpl-Pair_1377))+(Materialize[TransformIndex0]/Write/Write/WriteImpl/GroupByKey/Write)\n",
      "Running ((((((((((((FlattenTransformedDatasets/Read)+(ref_AppliedPTransform_WaitForTransformWrite_1243))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-FilterInternalColumn_1245))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1248))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1251))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1276))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/Precombine))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/Group/Write))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/Flatten/Transcode/0))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1277))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PreCombineFn)/Precombine))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PreCombineFn)/Group/Write))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/Flatten/Write/0)\n",
      "Running (((((((((GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/Group/Read)+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/Merge))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/ExtractOutputs))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1256))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/Precombine))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1265))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/Group/Write))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1267))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Precombine))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Group/Write)\n",
      "Running ((((GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Group/Read)+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Merge))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1272))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesFeatureStatsProtos/Write/1)\n",
      "Running (((((GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/Group/Read)+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/Merge))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1262))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1263))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesFeatureStatsProtos/Write/0)\n",
      "Running ((GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesFeatureStatsProtos/Read)+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Transcode/0))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Write/0)\n",
      "Running ((((((GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PreCombineFn)/Group/Read)+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PreCombineFn)/Merge))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PreCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1282))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1283))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/Flatten/Transcode/1))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/Flatten/Write/1)\n",
      "Running ((GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/Flatten/Read)+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PostCombineFn)/Precombine))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PostCombineFn)/Group/Write)\n",
      "Running ((((GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PostCombineFn)/Group/Read)+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PostCombineFn)/Merge))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PostCombineFn)/ExtractOutputs))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Transcode/1))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Write/1)\n",
      "Running ((GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Read)+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Precombine))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Group/Write)\n",
      "Running ((((((GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Group/Read)+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Merge))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/ExtractOutputs))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1294))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1297))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Precombine))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Write)\n",
      "Running (((((GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/Group/Read)+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/Merge))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-G_1096))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-G_1097))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesFeatureStatsProtos/Write/0)\n",
      "Running (ref_PCollection_PCollection_683/Read)+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-WriteSchema-Write-WriteImpl-FinalizeWri_1179)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.042551279067993164 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.40 seconds.\n",
      "Running ((ref_PCollection_PCollection_815/Read)+(ref_AppliedPTransform_Materialize-TransformIndex1-Write-Write-WriteImpl-PreFinalize_1400))+(ref_PCollection_PCollection_822/Write)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 0 files in 0.04219698905944824 seconds.\n",
      "Running (ref_PCollection_PCollection_815/Read)+(ref_AppliedPTransform_Materialize-TransformIndex1-Write-Write-WriteImpl-FinalizeWrite_1401)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.038500070571899414 seconds.\n",
      "Starting the size estimation of the input\n",
      "Finished listing 0 files in 0.034113407135009766 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Running (ref_PCollection_PCollection_608/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex9-Write-WriteImpl-FinalizeWrite_1060)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.04501914978027344 seconds.\n",
      "Starting the size estimation of the input\n",
      "Finished listing 0 files in 0.04462385177612305 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Running ((((GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Read)+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Merge))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1302))+(ref_PCollection_PCollection_758/Write)\n",
      "Running (((ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_4-vocabu_834)+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_4-vocabu_835))+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_4-vocabu_837))+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_4-vocabu_838)\n",
      "Running (ref_PCollection_PCollection_619/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex10-Write-WriteImpl-FinalizeWrite_1076)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.0395967960357666 seconds.\n",
      "Starting the size estimation of the input\n",
      "Finished listing 0 files in 0.03884148597717285 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Running (((ref_AppliedPTransform_Analyze-CreateSavedModel-tf_v2_only-Count-CreateSole-Impulse_771)+(ref_AppliedPTransform_Analyze-CreateSavedModel-tf_v2_only-Count-CreateSole-FlatMap-lambda-at-core-py_772))+(ref_AppliedPTransform_Analyze-CreateSavedModel-tf_v2_only-Count-CreateSole-Map-decode-_774))+(ref_AppliedPTransform_Analyze-CreateSavedModel-tf_v2_only-Count-Count_775)\n",
      "Running ((((GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Group/Read)+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Merge))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-G_1106))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesFeatureStatsProtos/Write/1)\n",
      "Running (((ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_1-vocabu_789)+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_1-vocabu_790))+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_1-vocabu_792))+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_1-vocabu_793)\n",
      "Running (((ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary-vocabula_780)+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary-vocabula_781))+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary-vocabula_783))+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary-vocabula_784)\n",
      "Running (((ref_AppliedPTransform_Analyze-PackedCombineAccumulate-ApplySavedModel-Phase0-AnalysisIndex0-Count-Cr_88)+(ref_AppliedPTransform_Analyze-PackedCombineAccumulate-ApplySavedModel-Phase0-AnalysisIndex0-Count-Cr_89))+(ref_AppliedPTransform_Analyze-PackedCombineAccumulate-ApplySavedModel-Phase0-AnalysisIndex0-Count-Cr_91))+(ref_AppliedPTransform_Analyze-PackedCombineAccumulate-ApplySavedModel-Phase0-AnalysisIndex0-Count-Co_92)\n",
      "Running ((GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesFeatureStatsProtos/Read)+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Transcode/0))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Write/0)\n",
      "Running ((((((GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PreCombineFn)/Group/Read)+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PreCombineFn)/Merge))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PreCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-G_1116))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-G_1117))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/Flatten/Transcode/1))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/Flatten/Write/1)\n",
      "Running ((GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/Flatten/Read)+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PostCombineFn)/Precombine))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PostCombineFn)/Group/Write)\n",
      "Running ((((GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PostCombineFn)/Group/Read)+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PostCombineFn)/Merge))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PostCombineFn)/ExtractOutputs))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Transcode/1))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Write/1)\n",
      "Running ((GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Read)+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Precombine))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Group/Write)\n",
      "Running ((((((GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Group/Read)+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Merge))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/ExtractOutputs))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-G_1128))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-G_1131))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Precombine))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Write)\n",
      "Running ((((GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Read)+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Merge))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-G_1136))+(ref_PCollection_PCollection_662/Write)\n",
      "Running (((((((ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-G_1138)+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-G_1139))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-G_1141))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-G_1142))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-G_1143))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-WriteStats-WriteStats-Write-WriteImpl-M_1154))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-WriteStats-WriteStats-Write-WriteImpl-W_1155))+(GenerateStats[FlattenedAnalysisDataset]/WriteStats/WriteStats/Write/WriteImpl/GroupByKey/Write)\n",
      "Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex5]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex5-Write-WriteImpl-Extract_994))+(ref_PCollection_PCollection_570/Write)\n",
      "Running ((ref_PCollection_PCollection_564/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex5-Write-WriteImpl-PreFinalize_995))+(ref_PCollection_PCollection_571/Write)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 0 files in 0.04065561294555664 seconds.\n",
      "Running (ref_PCollection_PCollection_564/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex5-Write-WriteImpl-FinalizeWrite_996)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.04053068161010742 seconds.\n",
      "Starting the size estimation of the input\n",
      "Finished listing 0 files in 0.04221940040588379 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex4]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex4-Write-WriteImpl-Extract_978))+(ref_PCollection_PCollection_559/Write)\n",
      "Running ((ref_PCollection_PCollection_553/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex4-Write-WriteImpl-PreFinalize_979))+(ref_PCollection_PCollection_560/Write)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 0 files in 0.03561139106750488 seconds.\n",
      "Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex3]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex3-Write-WriteImpl-Extract_962))+(ref_PCollection_PCollection_548/Write)\n",
      "Running ((ref_PCollection_PCollection_542/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex3-Write-WriteImpl-PreFinalize_963))+(ref_PCollection_PCollection_549/Write)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 0 files in 0.040873050689697266 seconds.\n",
      "Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex1]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex1-Write-WriteImpl-Extract_930))+(ref_PCollection_PCollection_526/Write)\n",
      "Running ((ref_PCollection_PCollection_520/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex1-Write-WriteImpl-PreFinalize_931))+(ref_PCollection_PCollection_527/Write)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 0 files in 0.0369412899017334 seconds.\n",
      "Running (ref_PCollection_PCollection_520/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex1-Write-WriteImpl-FinalizeWrite_932)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.04008889198303223 seconds.\n",
      "Starting the size estimation of the input\n",
      "Finished listing 0 files in 0.03795361518859863 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Running (((((ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-WriteStats-WriteStats-Write-WriteImpl-D_1149)+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-WriteStats-WriteStats-Write-WriteImpl-D_1150))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-WriteStats-WriteStats-Write-WriteImpl-D_1152))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-WriteStats-WriteStats-Write-WriteImpl-I_1153))+(ref_PCollection_PCollection_670/Write))+(ref_PCollection_PCollection_671/Write)\n",
      "Running (ref_PCollection_PCollection_542/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex3-Write-WriteImpl-FinalizeWrite_964)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.043616294860839844 seconds.\n",
      "Starting the size estimation of the input\n",
      "Finished listing 0 files in 0.04780697822570801 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Running ((GenerateStats[FlattenedAnalysisDataset]/WriteStats/WriteStats/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-WriteStats-WriteStats-Write-WriteImpl-W_1157))+(ref_PCollection_PCollection_675/Write)\n",
      "Running ((ref_PCollection_PCollection_670/Read)+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-WriteStats-WriteStats-Write-WriteImpl-P_1158))+(ref_PCollection_PCollection_676/Write)\n",
      "Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex0]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex0-Write-WriteImpl-Extract_914))+(ref_PCollection_PCollection_515/Write)\n",
      "Running ((ref_AppliedPTransform_OptimizeRun-WorkaroundForBug170304777-Impulse_4)+(ref_AppliedPTransform_OptimizeRun-WorkaroundForBug170304777-FlatMap-lambda-at-core-py-3222-_5))+(ref_AppliedPTransform_OptimizeRun-WorkaroundForBug170304777-Map-decode-_7)\n",
      "Running (ref_PCollection_PCollection_553/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex4-Write-WriteImpl-FinalizeWrite_980)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.07191205024719238 seconds.\n",
      "Starting the size estimation of the input\n",
      "Finished listing 0 files in 0.034021615982055664 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Running (ref_PCollection_PCollection_670/Read)+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-WriteStats-WriteStats-Write-WriteImpl-F_1159)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.04250597953796387 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.40 seconds.\n",
      "Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex6]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex6-Write-WriteImpl-Extract_1010))+(ref_PCollection_PCollection_581/Write)\n",
      "Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex8]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex8-Write-WriteImpl-Extract_1042))+(ref_PCollection_PCollection_603/Write)\n",
      "Running ((ref_PCollection_PCollection_597/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex8-Write-WriteImpl-PreFinalize_1043))+(ref_PCollection_PCollection_604/Write)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 0 files in 0.04468393325805664 seconds.\n",
      "Running (ref_PCollection_PCollection_597/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex8-Write-WriteImpl-FinalizeWrite_1044)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.04566550254821777 seconds.\n",
      "Starting the size estimation of the input\n",
      "Finished listing 0 files in 0.04285240173339844 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Running ((ref_PCollection_PCollection_575/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex6-Write-WriteImpl-PreFinalize_1011))+(ref_PCollection_PCollection_582/Write)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 0 files in 0.040654897689819336 seconds.\n",
      "Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex7]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex7-Write-WriteImpl-Extract_1026))+(ref_PCollection_PCollection_592/Write)\n",
      "Running ((ref_PCollection_PCollection_586/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex7-Write-WriteImpl-PreFinalize_1027))+(ref_PCollection_PCollection_593/Write)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 0 files in 0.04152870178222656 seconds.\n",
      "Running (((((ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteStats-WriteStats-Wr_1315)+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteStats-WriteStats-Wr_1316))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteStats-WriteStats-Wr_1318))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteStats-WriteStats-Wr_1319))+(ref_PCollection_PCollection_766/Write))+(ref_PCollection_PCollection_767/Write)\n",
      "Running (((((((((((ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1304)+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1305))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1307))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1308))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1309))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteStats-WriteStats-Wr_1320))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-ValidateStatistics_1346))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteStats-WriteStats-Wr_1321))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/WriteStats/WriteStats/Write/WriteImpl/GroupByKey/Write))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteValidation-Write-Wr_1356))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteValidation-Write-Wr_1357))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/WriteValidation/Write/WriteImpl/GroupByKey/Write)\n",
      "Running ((GenerateAndValidateStats[FlattenedTransformedDatasets]/WriteStats/WriteStats/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteStats-WriteStats-Wr_1323))+(ref_PCollection_PCollection_771/Write)\n",
      "Running ((ref_PCollection_PCollection_766/Read)+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteStats-WriteStats-Wr_1324))+(ref_PCollection_PCollection_772/Write)\n",
      "Running (((ref_AppliedPTransform_Analyze-PackedCombineMerge-3-Count-CreateSole-Impulse_129)+(ref_AppliedPTransform_Analyze-PackedCombineMerge-3-Count-CreateSole-FlatMap-lambda-at-core-py-3222-_130))+(ref_AppliedPTransform_Analyze-PackedCombineMerge-3-Count-CreateSole-Map-decode-_132))+(ref_AppliedPTransform_Analyze-PackedCombineMerge-3-Count-Count_133)\n",
      "Running (((((ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteSchema-Write-WriteI_1335)+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteSchema-Write-WriteI_1336))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteSchema-Write-WriteI_1338))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteSchema-Write-WriteI_1339))+(ref_PCollection_PCollection_779/Write))+(ref_PCollection_PCollection_780/Write)\n",
      "Running (((((ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-CreateSchema-Impulse_1327)+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-CreateSchema-FlatMap-lam_1328))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-CreateSchema-Map-decode-_1330))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteSchema-Write-WriteI_1340))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteSchema-Write-WriteI_1341))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/WriteSchema/Write/WriteImpl/GroupByKey/Write)\n",
      "Running ((GenerateAndValidateStats[FlattenedTransformedDatasets]/WriteSchema/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteSchema-Write-WriteI_1343))+(ref_PCollection_PCollection_784/Write)\n",
      "Running ((ref_PCollection_PCollection_779/Read)+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteSchema-Write-WriteI_1344))+(ref_PCollection_PCollection_785/Write)\n",
      "Running (ref_PCollection_PCollection_779/Read)+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteSchema-Write-WriteI_1345)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.042197465896606445 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Running (ref_PCollection_PCollection_575/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex6-Write-WriteImpl-FinalizeWrite_1012)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.04347944259643555 seconds.\n",
      "Starting the size estimation of the input\n",
      "Finished listing 0 files in 0.04364013671875 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Running (ref_PCollection_PCollection_586/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex7-Write-WriteImpl-FinalizeWrite_1028)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.040047645568847656 seconds.\n",
      "Starting the size estimation of the input\n",
      "Finished listing 0 files in 0.03971529006958008 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Running (ref_PCollection_PCollection_766/Read)+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteStats-WriteStats-Wr_1325)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.04657316207885742 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Running (((((ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteValidation-Write-Wr_1351)+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteValidation-Write-Wr_1352))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteValidation-Write-Wr_1354))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteValidation-Write-Wr_1355))+(ref_PCollection_PCollection_790/Write))+(ref_PCollection_PCollection_791/Write)\n",
      "Running ((GenerateAndValidateStats[FlattenedTransformedDatasets]/WriteValidation/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteValidation-Write-Wr_1359))+(ref_PCollection_PCollection_795/Write)\n",
      "Running ((ref_PCollection_PCollection_790/Read)+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteValidation-Write-Wr_1360))+(ref_PCollection_PCollection_796/Write)\n",
      "Running (ref_PCollection_PCollection_790/Read)+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteValidation-Write-Wr_1361)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.039797306060791016 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Running (((ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_z_score-mean_and_var-A_816)+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_z_score-mean_and_var-A_817))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_z_score-mean_and_var-A_819))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_z_score-mean_and_var-A_820)\n",
      "Running (((ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_2-vocabu_798)+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_2-vocabu_799))+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_2-vocabu_801))+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_2-vocabu_802)\n",
      "Running ((ref_PCollection_PCollection_509/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex0-Write-WriteImpl-PreFinalize_915))+(ref_PCollection_PCollection_516/Write)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 0 files in 0.043862342834472656 seconds.\n",
      "Running (ref_PCollection_PCollection_509/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex0-Write-WriteImpl-FinalizeWrite_916)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.0508875846862793 seconds.\n",
      "Starting the size estimation of the input\n",
      "Finished listing 0 files in 0.03902316093444824 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Running ((Materialize[TransformIndex0]/Write/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_Materialize-TransformIndex0-Write-Write-WriteImpl-Extract_1379))+(ref_PCollection_PCollection_808/Write)\n",
      "Running (((ref_AppliedPTransform_Analyze-PrepareToClearSharedKeepAlives-Impulse_878)+(ref_AppliedPTransform_Analyze-PrepareToClearSharedKeepAlives-FlatMap-lambda-at-core-py-3222-_879))+(ref_AppliedPTransform_Analyze-PrepareToClearSharedKeepAlives-Map-decode-_881))+(ref_AppliedPTransform_Analyze-WaitAndClearSharedKeepAlives_882)\n",
      "Running ((ref_PCollection_PCollection_802/Read)+(ref_AppliedPTransform_Materialize-TransformIndex0-Write-Write-WriteImpl-PreFinalize_1380))+(ref_PCollection_PCollection_809/Write)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 0 files in 0.04416489601135254 seconds.\n",
      "Running (((ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_z_score_1-mean_and_var_825)+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_z_score_1-mean_and_var_826))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_z_score_1-mean_and_var_828))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_z_score_1-mean_and_var_829)\n",
      "Running (((ref_AppliedPTransform_WriteMetadata-Create-Impulse_885)+(ref_AppliedPTransform_WriteMetadata-Create-FlatMap-lambda-at-core-py-3222-_886))+(ref_AppliedPTransform_WriteMetadata-Create-Map-decode-_888))+(ref_AppliedPTransform_WriteMetadata-WriteMetadata_889)\n",
      "Running (((ref_AppliedPTransform_Transform-TransformIndex0-PrepareToClearSharedKeepAlives-Impulse_1204)+(ref_AppliedPTransform_Transform-TransformIndex0-PrepareToClearSharedKeepAlives-FlatMap-lambda-at-cor_1205))+(ref_AppliedPTransform_Transform-TransformIndex0-PrepareToClearSharedKeepAlives-Map-decode-_1207))+(ref_AppliedPTransform_Transform-TransformIndex0-WaitAndClearSharedKeepAlives_1208)\n",
      "Running (((ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_3-vocabu_807)+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_3-vocabu_808))+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_3-vocabu_810))+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_3-vocabu_811)\n",
      "Running (((ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_z_score_2-mean_and_var_861)+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_z_score_2-mean_and_var_862))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_z_score_2-mean_and_var_864))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_z_score_2-mean_and_var_865)\n",
      "Running (((ref_AppliedPTransform_Transform-TransformIndex1-PrepareToClearSharedKeepAlives-Impulse_1233)+(ref_AppliedPTransform_Transform-TransformIndex1-PrepareToClearSharedKeepAlives-FlatMap-lambda-at-cor_1234))+(ref_AppliedPTransform_Transform-TransformIndex1-PrepareToClearSharedKeepAlives-Map-decode-_1236))+(ref_AppliedPTransform_Transform-TransformIndex1-WaitAndClearSharedKeepAlives_1237)\n",
      "Running (((ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_5-vocabu_843)+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_5-vocabu_844))+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_5-vocabu_846))+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_5-vocabu_847)\n",
      "Running (((ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_7-vocabu_870)+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_7-vocabu_871))+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_7-vocabu_873))+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_7-vocabu_874)\n",
      "Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex2]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex2-Write-WriteImpl-Extract_946))+(ref_PCollection_PCollection_537/Write)\n",
      "Running ((ref_PCollection_PCollection_531/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex2-Write-WriteImpl-PreFinalize_947))+(ref_PCollection_PCollection_538/Write)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 0 files in 0.043515920639038086 seconds.\n",
      "Running (((ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_6-vocabu_852)+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_6-vocabu_853))+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_6-vocabu_855))+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_6-vocabu_856)\n",
      "Running (((ref_AppliedPTransform_IncrementPipelineMetrics-CreateSole-Impulse_10)+(ref_AppliedPTransform_IncrementPipelineMetrics-CreateSole-FlatMap-lambda-at-core-py-3222-_11))+(ref_AppliedPTransform_IncrementPipelineMetrics-CreateSole-Map-decode-_13))+(ref_AppliedPTransform_IncrementPipelineMetrics-Count_14)\n",
      "Running (ref_PCollection_PCollection_802/Read)+(ref_AppliedPTransform_Materialize-TransformIndex0-Write-Write-WriteImpl-FinalizeWrite_1381)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.047007083892822266 seconds.\n",
      "Starting the size estimation of the input\n",
      "Finished listing 0 files in 0.04879951477050781 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Running (ref_PCollection_PCollection_531/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex2-Write-WriteImpl-FinalizeWrite_948)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.04149484634399414 seconds.\n",
      "Starting the size estimation of the input\n",
      "Finished listing 0 files in 0.04155135154724121 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "stateful_working_dir /home/jupyter/20220318_Training/gcp-partner-training-mlops/gs:/grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/DataTransformer/.system/stateful_working_dir is not found, not going to delete it.\n",
      "I0330 14:48:41.912820   104 rdbms_metadata_access_object.cc:686] No property is defined for the Type\n",
      "I0330 14:48:41.921489   104 rdbms_metadata_access_object.cc:686] No property is defined for the Type\n",
      "Artifact type Model is not found in MLMD.\n",
      "I0330 14:48:42.018571   104 rdbms_metadata_access_object.cc:686] No property is defined for the Type\n",
      "Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
      "Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
      "Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
      "E0330 14:48:46.138605021     104 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "Processing /tmp/tmprn87gnmc/tfx_user_code_ModelTrainer-0.0+45151d477b837ba3e9c07eca0c4beadd51fa08b10afc0503d6ce3837f470c259-py3-none-any.whl\n",
      "Installing collected packages: tfx-user-code-ModelTrainer\n",
      "Successfully installed tfx-user-code-ModelTrainer-0.0+45151d477b837ba3e9c07eca0c4beadd51fa08b10afc0503d6ce3837f470c259\n",
      "Runner started...\n",
      "fn_args: FnArgs(working_dir=None, train_files=['gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/DataTransformer/transformed_examples/9/Split-train/*'], eval_files=['gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/DataTransformer/transformed_examples/9/Split-eval/*'], train_steps=None, eval_steps=None, schema_path='src/raw_schema/schema.pbtxt', schema_file='src/raw_schema/schema.pbtxt', transform_graph_path='gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/DataTransformer/transform_graph/9', transform_output='gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/DataTransformer/transform_graph/9', data_accessor=DataAccessor(tf_dataset_factory=<function get_tf_dataset_factory_from_artifact.<locals>.dataset_factory at 0x7f84d8a297a0>, record_batch_factory=<function get_record_batch_factory_from_artifact.<locals>.record_batch_factory at 0x7f84d8960cb0>, data_view_decode_fn=None), serving_model_dir='gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/ModelTrainer/model/10/Format-Serving', eval_model_dir='gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/ModelTrainer/model/10/Format-TFMA', model_run_dir='gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/ModelTrainer/model_run/10', base_model=None, hyperparameters={'num_epochs': 1, 'batch_size': 512, 'learning_rate': 0.001, 'hidden_units': [128, 128]}, custom_config=None)\n",
      "\n",
      "Hyperparameter:\n",
      "{'num_epochs': 1, 'batch_size': 512, 'learning_rate': 0.001, 'hidden_units': [128, 128]}\n",
      "\n",
      "Runner executing trainer...\n",
      "Loading tft output from gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/DataTransformer/transform_graph/9\n",
      "2022-03-30 14:48:49.573903: I tensorflow/core/profiler/lib/profiler_session.cc:126] Profiler session initializing.\n",
      "2022-03-30 14:48:49.573975: I tensorflow/core/profiler/lib/profiler_session.cc:141] Profiler session started.\n",
      "2022-03-30 14:48:49.574081: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session tear down.\n",
      "TensorFlow in TF remote image: 2.5.3\n",
      "max_tokens is deprecated, please use num_tokens instead.\n",
      "max_tokens is deprecated, please use num_tokens instead.\n",
      "Model training started...\n",
      "      1/Unknown - 1s 1s/step - loss: 0.6496 - accuracy: 0.90432022-03-30 14:48:53.820942: I tensorflow/core/profiler/lib/profiler_session.cc:126] Profiler session initializing.\n",
      "2022-03-30 14:48:53.821021: I tensorflow/core/profiler/lib/profiler_session.cc:141] Profiler session started.\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6496 - accuracy: 0.9043\n",
      "Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "2022-03-30 14:48:55.572199: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "2022-03-30 14:48:55.578064: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session tear down.\n",
      "2022-03-30 14:48:56.720826: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/ModelTrainer/model_run/10/train/plugins/profile/2022_03_30_14_48_55\n",
      "2022-03-30 14:48:56.926940: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for trace.json.gz to gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/ModelTrainer/model_run/10/train/plugins/profile/2022_03_30_14_48_55/vm-508f776f-5512-42a9-b529-8989959ded1b.trace.json.gz\n",
      "2022-03-30 14:48:57.013589: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/ModelTrainer/model_run/10/train/plugins/profile/2022_03_30_14_48_55\n",
      "2022-03-30 14:48:57.179940: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for memory_profile.json.gz to gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/ModelTrainer/model_run/10/train/plugins/profile/2022_03_30_14_48_55/vm-508f776f-5512-42a9-b529-8989959ded1b.memory_profile.json.gz\n",
      "2022-03-30 14:48:58.157474: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/ModelTrainer/model_run/10/train/plugins/profile/2022_03_30_14_48_55Dumped tool data for xplane.pb to gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/ModelTrainer/model_run/10/train/plugins/profile/2022_03_30_14_48_55/vm-508f776f-5512-42a9-b529-8989959ded1b.xplane.pb\n",
      "Dumped tool data for overview_page.pb to gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/ModelTrainer/model_run/10/train/plugins/profile/2022_03_30_14_48_55/vm-508f776f-5512-42a9-b529-8989959ded1b.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/ModelTrainer/model_run/10/train/plugins/profile/2022_03_30_14_48_55/vm-508f776f-5512-42a9-b529-8989959ded1b.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/ModelTrainer/model_run/10/train/plugins/profile/2022_03_30_14_48_55/vm-508f776f-5512-42a9-b529-8989959ded1b.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/ModelTrainer/model_run/10/train/plugins/profile/2022_03_30_14_48_55/vm-508f776f-5512-42a9-b529-8989959ded1b.kernel_stats.pb\n",
      "\n",
      "Model training completed.\n",
      "Runner executing exporter...\n",
      "Model export started...\n",
      "Assets written to: gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/ModelTrainer/model/10/Format-Serving/assets\n",
      "Model export completed.\n",
      "Runner completed.\n",
      "stateful_working_dir /home/jupyter/20220318_Training/gcp-partner-training-mlops/gs:/grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/ModelTrainer/.system/stateful_working_dir is not found, not going to delete it.\n",
      "I0330 14:49:19.077921   104 rdbms_metadata_access_object.cc:686] No property is defined for the Type\n",
      "I0330 14:49:19.101500   104 rdbms_metadata_access_object.cc:686] No property is defined for the Type\n",
      "I0330 14:49:19.194156   104 rdbms_metadata_access_object.cc:686] No property is defined for the Type\n",
      "E0330 14:49:23.108945554     104 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "There are change thresholds, but the baseline is missing. This is allowed only when rubber stamping (first run).\n",
      "Exception ignored in: <function CapturableResource.__del__ at 0x7f85471805f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py\", line 277, in __del__\n",
      "    self._destroy_resource()\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 889, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 924, in _call\n",
      "    results = self._stateful_fn(*args, **kwds)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3022, in __call__\n",
      "    filtered_flat_args) = self._maybe_define_function(args, kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3444, in _maybe_define_function\n",
      "    graph_function = self._create_graph_function(args, kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3289, in _create_graph_function\n",
      "    capture_by_value=self._capture_by_value),\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 999, in func_graph_from_py_func\n",
      "    func_outputs = python_func(*func_args, **func_kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 672, in wrapped_fn\n",
      "    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n",
      "AttributeError: 'NoneType' object has no attribute '__wrapped__'\n",
      "Exception ignored in: <function CapturableResource.__del__ at 0x7f85471805f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py\", line 277, in __del__\n",
      "    self._destroy_resource()\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 889, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 924, in _call\n",
      "    results = self._stateful_fn(*args, **kwds)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3022, in __call__\n",
      "    filtered_flat_args) = self._maybe_define_function(args, kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3444, in _maybe_define_function\n",
      "    graph_function = self._create_graph_function(args, kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3289, in _create_graph_function\n",
      "    capture_by_value=self._capture_by_value),\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 999, in func_graph_from_py_func\n",
      "    func_outputs = python_func(*func_args, **func_kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 672, in wrapped_fn\n",
      "    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n",
      "AttributeError: 'NoneType' object has no attribute '__wrapped__'\n",
      "Exception ignored in: <function CapturableResource.__del__ at 0x7f85471805f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py\", line 277, in __del__\n",
      "    self._destroy_resource()\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 889, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 924, in _call\n",
      "    results = self._stateful_fn(*args, **kwds)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3022, in __call__\n",
      "    filtered_flat_args) = self._maybe_define_function(args, kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3444, in _maybe_define_function\n",
      "    graph_function = self._create_graph_function(args, kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3289, in _create_graph_function\n",
      "    capture_by_value=self._capture_by_value),\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 999, in func_graph_from_py_func\n",
      "    func_outputs = python_func(*func_args, **func_kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 672, in wrapped_fn\n",
      "    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n",
      "AttributeError: 'NoneType' object has no attribute '__wrapped__'\n",
      "Exception ignored in: <function CapturableResource.__del__ at 0x7f85471805f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py\", line 277, in __del__\n",
      "    self._destroy_resource()\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 889, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 924, in _call\n",
      "    results = self._stateful_fn(*args, **kwds)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3022, in __call__\n",
      "    filtered_flat_args) = self._maybe_define_function(args, kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3444, in _maybe_define_function\n",
      "    graph_function = self._create_graph_function(args, kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3289, in _create_graph_function\n",
      "    capture_by_value=self._capture_by_value),\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 999, in func_graph_from_py_func\n",
      "    func_outputs = python_func(*func_args, **func_kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 672, in wrapped_fn\n",
      "    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n",
      "AttributeError: 'NoneType' object has no attribute '__wrapped__'\n",
      "Exception ignored in: <function CapturableResource.__del__ at 0x7f85471805f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py\", line 277, in __del__\n",
      "    self._destroy_resource()\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 889, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 924, in _call\n",
      "    results = self._stateful_fn(*args, **kwds)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3022, in __call__\n",
      "    filtered_flat_args) = self._maybe_define_function(args, kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3444, in _maybe_define_function\n",
      "    graph_function = self._create_graph_function(args, kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3289, in _create_graph_function\n",
      "    capture_by_value=self._capture_by_value),\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 999, in func_graph_from_py_func\n",
      "    func_outputs = python_func(*func_args, **func_kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 672, in wrapped_fn\n",
      "    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n",
      "AttributeError: 'NoneType' object has no attribute '__wrapped__'\n",
      "Exception ignored in: <function CapturableResource.__del__ at 0x7f85471805f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py\", line 277, in __del__\n",
      "    self._destroy_resource()\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 889, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 924, in _call\n",
      "    results = self._stateful_fn(*args, **kwds)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3022, in __call__\n",
      "    filtered_flat_args) = self._maybe_define_function(args, kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3444, in _maybe_define_function\n",
      "    graph_function = self._create_graph_function(args, kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3289, in _create_graph_function\n",
      "    capture_by_value=self._capture_by_value),\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 999, in func_graph_from_py_func\n",
      "    func_outputs = python_func(*func_args, **func_kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 672, in wrapped_fn\n",
      "    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n",
      "AttributeError: 'NoneType' object has no attribute '__wrapped__'\n",
      "Exception ignored in: <function CapturableResource.__del__ at 0x7f85471805f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py\", line 277, in __del__\n",
      "    self._destroy_resource()\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 889, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 924, in _call\n",
      "    results = self._stateful_fn(*args, **kwds)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3022, in __call__\n",
      "    filtered_flat_args) = self._maybe_define_function(args, kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3444, in _maybe_define_function\n",
      "    graph_function = self._create_graph_function(args, kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3289, in _create_graph_function\n",
      "    capture_by_value=self._capture_by_value),\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 999, in func_graph_from_py_func\n",
      "    func_outputs = python_func(*func_args, **func_kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 672, in wrapped_fn\n",
      "    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n",
      "AttributeError: 'NoneType' object has no attribute '__wrapped__'\n",
      "Exception ignored in: <function CapturableResource.__del__ at 0x7f85471805f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py\", line 277, in __del__\n",
      "    self._destroy_resource()\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 889, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 924, in _call\n",
      "    results = self._stateful_fn(*args, **kwds)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3022, in __call__\n",
      "    filtered_flat_args) = self._maybe_define_function(args, kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3444, in _maybe_define_function\n",
      "    graph_function = self._create_graph_function(args, kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3289, in _create_graph_function\n",
      "    capture_by_value=self._capture_by_value),\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 999, in func_graph_from_py_func\n",
      "    func_outputs = python_func(*func_args, **func_kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 672, in wrapped_fn\n",
      "    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n",
      "AttributeError: 'NoneType' object has no attribute '__wrapped__'\n",
      "Exception ignored in: <function CapturableResource.__del__ at 0x7f85471805f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py\", line 277, in __del__\n",
      "    self._destroy_resource()\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 889, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 924, in _call\n",
      "    results = self._stateful_fn(*args, **kwds)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3022, in __call__\n",
      "    filtered_flat_args) = self._maybe_define_function(args, kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3444, in _maybe_define_function\n",
      "    graph_function = self._create_graph_function(args, kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3289, in _create_graph_function\n",
      "    capture_by_value=self._capture_by_value),\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 999, in func_graph_from_py_func\n",
      "    func_outputs = python_func(*func_args, **func_kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 672, in wrapped_fn\n",
      "    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n",
      "AttributeError: 'NoneType' object has no attribute '__wrapped__'\n",
      "Exception ignored in: <function CapturableResource.__del__ at 0x7f85471805f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py\", line 277, in __del__\n",
      "    self._destroy_resource()\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 889, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 924, in _call\n",
      "    results = self._stateful_fn(*args, **kwds)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3022, in __call__\n",
      "    filtered_flat_args) = self._maybe_define_function(args, kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3444, in _maybe_define_function\n",
      "    graph_function = self._create_graph_function(args, kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3289, in _create_graph_function\n",
      "    capture_by_value=self._capture_by_value),\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 999, in func_graph_from_py_func\n",
      "    func_outputs = python_func(*func_args, **func_kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 672, in wrapped_fn\n",
      "    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n",
      "AttributeError: 'NoneType' object has no attribute '__wrapped__'\n",
      "Exception ignored in: <function CapturableResource.__del__ at 0x7f85471805f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py\", line 277, in __del__\n",
      "    self._destroy_resource()\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 889, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 924, in _call\n",
      "    results = self._stateful_fn(*args, **kwds)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3022, in __call__\n",
      "    filtered_flat_args) = self._maybe_define_function(args, kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3444, in _maybe_define_function\n",
      "    graph_function = self._create_graph_function(args, kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3289, in _create_graph_function\n",
      "    capture_by_value=self._capture_by_value),\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 999, in func_graph_from_py_func\n",
      "    func_outputs = python_func(*func_args, **func_kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 672, in wrapped_fn\n",
      "    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n",
      "AttributeError: 'NoneType' object has no attribute '__wrapped__'\n",
      "Exception ignored in: <function CapturableResource.__del__ at 0x7f85471805f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py\", line 277, in __del__\n",
      "    self._destroy_resource()\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 889, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 924, in _call\n",
      "    results = self._stateful_fn(*args, **kwds)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3022, in __call__\n",
      "    filtered_flat_args) = self._maybe_define_function(args, kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3444, in _maybe_define_function\n",
      "    graph_function = self._create_graph_function(args, kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3289, in _create_graph_function\n",
      "    capture_by_value=self._capture_by_value),\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 999, in func_graph_from_py_func\n",
      "    func_outputs = python_func(*func_args, **func_kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 672, in wrapped_fn\n",
      "    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n",
      "AttributeError: 'NoneType' object has no attribute '__wrapped__'\n",
      "Exception ignored in: <function CapturableResource.__del__ at 0x7f85471805f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py\", line 277, in __del__\n",
      "    self._destroy_resource()\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 889, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 924, in _call\n",
      "    results = self._stateful_fn(*args, **kwds)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3022, in __call__\n",
      "    filtered_flat_args) = self._maybe_define_function(args, kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3444, in _maybe_define_function\n",
      "    graph_function = self._create_graph_function(args, kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3289, in _create_graph_function\n",
      "    capture_by_value=self._capture_by_value),\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 999, in func_graph_from_py_func\n",
      "    func_outputs = python_func(*func_args, **func_kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 672, in wrapped_fn\n",
      "    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n",
      "AttributeError: 'NoneType' object has no attribute '__wrapped__'\n",
      "Exception ignored in: <function CapturableResource.__del__ at 0x7f85471805f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py\", line 277, in __del__\n",
      "    self._destroy_resource()\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 889, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 924, in _call\n",
      "    results = self._stateful_fn(*args, **kwds)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3022, in __call__\n",
      "    filtered_flat_args) = self._maybe_define_function(args, kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3444, in _maybe_define_function\n",
      "    graph_function = self._create_graph_function(args, kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3289, in _create_graph_function\n",
      "    capture_by_value=self._capture_by_value),\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 999, in func_graph_from_py_func\n",
      "    func_outputs = python_func(*func_args, **func_kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 672, in wrapped_fn\n",
      "    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n",
      "AttributeError: 'NoneType' object has no attribute '__wrapped__'\n",
      "Exception ignored in: <function CapturableResource.__del__ at 0x7f85471805f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py\", line 277, in __del__\n",
      "    self._destroy_resource()\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 889, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 924, in _call\n",
      "    results = self._stateful_fn(*args, **kwds)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3022, in __call__\n",
      "    filtered_flat_args) = self._maybe_define_function(args, kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3444, in _maybe_define_function\n",
      "    graph_function = self._create_graph_function(args, kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3289, in _create_graph_function\n",
      "    capture_by_value=self._capture_by_value),\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 999, in func_graph_from_py_func\n",
      "    func_outputs = python_func(*func_args, **func_kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 672, in wrapped_fn\n",
      "    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n",
      "AttributeError: 'NoneType' object has no attribute '__wrapped__'\n",
      "Exception ignored in: <function CapturableResource.__del__ at 0x7f85471805f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py\", line 277, in __del__\n",
      "    self._destroy_resource()\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 889, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 924, in _call\n",
      "    results = self._stateful_fn(*args, **kwds)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3022, in __call__\n",
      "    filtered_flat_args) = self._maybe_define_function(args, kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3444, in _maybe_define_function\n",
      "    graph_function = self._create_graph_function(args, kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3289, in _create_graph_function\n",
      "    capture_by_value=self._capture_by_value),\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 999, in func_graph_from_py_func\n",
      "    func_outputs = python_func(*func_args, **func_kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 672, in wrapped_fn\n",
      "    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n",
      "AttributeError: 'NoneType' object has no attribute '__wrapped__'\n",
      "Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tensorflow.python.keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7f84c6048c50> and <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f85007d0a50>).\n",
      "Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.06286931037902832 seconds.\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tensorflow.python.keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7f84d82bcd90> and <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f84c5f5e390>).\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Type[typing.Union[tensorflow_model_analysis.metrics.metric_types.MetricKey, tensorflow_model_analysis.metrics.metric_types.PlotKey, tensorflow_model_analysis.metrics.metric_types.AttributionsKey]]\n",
      "Using Any for unsupported type: typing.Type[typing.Union[tensorflow_model_analysis.metrics.metric_types.MetricKey, tensorflow_model_analysis.metrics.metric_types.PlotKey, tensorflow_model_analysis.metrics.metric_types.AttributionsKey]]\n",
      "Using Any for unsupported type: typing.Type[typing.Union[tensorflow_model_analysis.metrics.metric_types.MetricKey, tensorflow_model_analysis.metrics.metric_types.PlotKey, tensorflow_model_analysis.metrics.metric_types.AttributionsKey]]\n",
      "Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "Default Python SDK image for environment is apache/beam_python3.7_sdk:2.34.0\n",
      "==================== <function annotate_downstream_side_inputs at 0x7f8505d1f4d0> ====================\n",
      "==================== <function fix_side_input_pcoll_coders at 0x7f8505d1f5f0> ====================\n",
      "==================== <function pack_combiners at 0x7f8505d1fb00> ====================\n",
      "==================== <function lift_combiners at 0x7f8505d1fb90> ====================\n",
      "==================== <function expand_sdf at 0x7f8505d1fd40> ====================\n",
      "==================== <function expand_gbk at 0x7f8505d1fdd0> ====================\n",
      "==================== <function sink_flattens at 0x7f8505d1fef0> ====================\n",
      "==================== <function greedily_fuse at 0x7f8505d1ff80> ====================\n",
      "==================== <function read_to_impulse at 0x7f8505d1e050> ====================\n",
      "==================== <function impulse_to_input at 0x7f8505d1e0e0> ====================\n",
      "==================== <function sort_stages at 0x7f8505d1e320> ====================\n",
      "==================== <function setup_timer_mapping at 0x7f8505d1e290> ====================\n",
      "==================== <function populate_data_channel_coders at 0x7f8505d1e3b0> ====================\n",
      "Creating state cache with size 100\n",
      "Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7f84c4dac310> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
      "Running ((((ref_AppliedPTransform_ReadFromTFRecordToArrow-test-0-RawRecordBeamSource-ReadRawRecords-ReadFromTFRe_7)+(ref_AppliedPTransform_ReadFromTFRecordToArrow-test-0-RawRecordBeamSource-ReadRawRecords-ReadFromTFRe_8))+(ReadFromTFRecordToArrow[test][0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(ReadFromTFRecordToArrow[test][0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_2_split/Write)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.05930495262145996 seconds.\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.037963151931762695 seconds.\n",
      "Running (((((((((((((((((((((((((((((((ref_PCollection_PCollection_2_split/Read)+(ReadFromTFRecordToArrow[test][0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_ReadFromTFRecordToArrow-test-0-RawRecordBeamSource-ReadRawRecords-FlattenPColl_11))+(ref_AppliedPTransform_ReadFromTFRecordToArrow-test-0-RawRecordBeamSource-CollectRawRecordTelemetry-P_13))+(ref_AppliedPTransform_ReadFromTFRecordToArrow-test-0-RawRecordToRecordBatch-RawRecordToRecordBatch-B_17))+(ref_AppliedPTransform_ReadFromTFRecordToArrow-test-0-RawRecordToRecordBatch-RawRecordToRecordBatch-D_18))+(ref_AppliedPTransform_ReadFromTFRecordToArrow-test-0-RawRecordToRecordBatch-CollectRecordBatchTeleme_20))+(ref_AppliedPTransform_FlattenExamples_21))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-BatchedInputsToExtracts-AddArrowRecordBatchKey_24))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-ExtractFeatures-ExtractFeatu_27))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-ExtractTransformedFeatures-P_29))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-ExtractLabels-ExtractLabels_31))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-ExtractExampleWeights-Extrac_33))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-ExtractPredictions-Predict_35))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-ExtractUnbatchedInputs-Unbat_37))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-ExtractSliceKeys-ParDo-Extra_39))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_42))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_44))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_47))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_72))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_92))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_49))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/FanoutSlices/TrackDistinctSliceKeys/RemoveDuplicates/Group/Precombine))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/FanoutSlices/TrackDistinctSliceKeys/RemoveDuplicates/Group/Group/Write))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_74))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CountPerSliceKey/CombinePerKey(CountCombineFn)/Precombine))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CountPerSliceKey/CombinePerKey(CountCombineFn)/Group/Write))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_93))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CombineMetricsPerSlice/Flatten/Transcode/0))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CombineMetricsPerSlice/CombinePerKey(PreCombineFn)/Precombine))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CombineMetricsPerSlice/CombinePerKey(PreCombineFn)/Group/Write))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CombineMetricsPerSlice/Flatten/Write/0)\n",
      "Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tensorflow.python.keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7f84c5e18690> and <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f8501fc6650>).\n",
      "Running ((((ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CountPerSliceKey/CombinePerKey(CountCombineFn)/Group/Read)+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CountPerSliceKey/CombinePerKey(CountCombineFn)/Merge))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CountPerSliceKey/CombinePerKey(CountCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_88))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_89)\n",
      "Running (((((ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteEvalConfig-WriteEvalConfig-Wr_124)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteEvalConfig-WriteEvalConfig-Wr_125))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteEvalConfig-WriteEvalConfig-Wr_127))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteEvalConfig-WriteEvalConfig-Wr_128))+(ref_PCollection_PCollection_65/Write))+(ref_PCollection_PCollection_66/Write)\n",
      "Running (((((ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteEvalConfig-CreateEvalConfig-I_116)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteEvalConfig-CreateEvalConfig-F_117))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteEvalConfig-CreateEvalConfig-M_119))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteEvalConfig-WriteEvalConfig-Wr_129))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteEvalConfig-WriteEvalConfig-Wr_130))+(ExtractEvaluateAndWriteResults/WriteResults/WriteEvalConfig/WriteEvalConfig/Write/WriteImpl/GroupByKey/Write)\n",
      "Running ((ExtractEvaluateAndWriteResults/WriteResults/WriteEvalConfig/WriteEvalConfig/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteEvalConfig-WriteEvalConfig-Wr_132))+(ref_PCollection_PCollection_70/Write)\n",
      "Running (((((ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteValidati_201)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteValidati_202))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteValidati_204))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteValidati_205))+(ref_PCollection_PCollection_116/Write))+(ref_PCollection_PCollection_117/Write)\n",
      "Running ((((((ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CombineMetricsPerSlice/CombinePerKey(PreCombineFn)/Group/Read)+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CombineMetricsPerSlice/CombinePerKey(PreCombineFn)/Merge))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CombineMetricsPerSlice/CombinePerKey(PreCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_98))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_99))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CombineMetricsPerSlice/Flatten/Transcode/1))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CombineMetricsPerSlice/Flatten/Write/1)\n",
      "Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tensorflow.python.keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7f84c4dbb510> and <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f84d9049e10>).\n",
      "Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tensorflow.python.keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7f84aae93990> and <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f84aae930d0>).\n",
      "Running ((ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CombineMetricsPerSlice/Flatten/Read)+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CombineMetricsPerSlice/CombinePerKey(PostCombineFn)/Precombine))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CombineMetricsPerSlice/CombinePerKey(PostCombineFn)/Group/Write)\n",
      "Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tensorflow.python.keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7f8493355550> and <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f84933935d0>).\n",
      "Running (((((((((((((((((((((((ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CombineMetricsPerSlice/CombinePerKey(PostCombineFn)/Group/Read)+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CombineMetricsPerSlice/CombinePerKey(PostCombineFn)/Merge))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CombineMetricsPerSlice/CombinePerKey(PostCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_106))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_108))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_109))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_110))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_111))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Vali_112))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-ConvertSliceM_136))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-ConvertSliceP_152))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-ConvertSliceA_168))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-MergeValidati_185))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteMetrics-_146))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteMetrics-_147))+(ExtractEvaluateAndWriteResults/WriteResults/WriteMetricsAndPlots/WriteMetrics/Write/WriteImpl/GroupByKey/Write))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WritePlotsToT_162))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WritePlotsToT_163))+(ExtractEvaluateAndWriteResults/WriteResults/WriteMetricsAndPlots/WritePlotsToTFRecord/Write/WriteImpl/GroupByKey/Write))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteAttribut_178))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteAttribut_179))+(ExtractEvaluateAndWriteResults/WriteResults/WriteMetricsAndPlots/WriteAttributionsToTFRecord/Write/WriteImpl/GroupByKey/Write))+(ExtractEvaluateAndWriteResults/WriteResults/WriteMetricsAndPlots/MergeValidationResults/CombinePerKey/Precombine))+(ExtractEvaluateAndWriteResults/WriteResults/WriteMetricsAndPlots/MergeValidationResults/CombinePerKey/Group/Write)\n",
      "Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tensorflow.python.keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7f8492907e10> and <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f8492980ed0>).\n",
      "Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<tensorflow.python.keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7f8491f95bd0> and <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f8492925c90>).\n",
      "Running ((((ExtractEvaluateAndWriteResults/WriteResults/WriteMetricsAndPlots/MergeValidationResults/CombinePerKey/Group/Read)+(ExtractEvaluateAndWriteResults/WriteResults/WriteMetricsAndPlots/MergeValidationResults/CombinePerKey/Merge))+(ExtractEvaluateAndWriteResults/WriteResults/WriteMetricsAndPlots/MergeValidationResults/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-MergeValidati_190))+(ref_PCollection_PCollection_109/Write)\n",
      "Running ((((((ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-MergeValidati_192)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-MergeValidati_193))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-MergeValidati_195))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-MergeValidati_196))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteValidati_206))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteValidati_207))+(ExtractEvaluateAndWriteResults/WriteResults/WriteMetricsAndPlots/WriteValidationsToTFRecord/Write/WriteImpl/GroupByKey/Write)\n",
      "Running ((ExtractEvaluateAndWriteResults/WriteResults/WriteMetricsAndPlots/WriteValidationsToTFRecord/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteValidati_209))+(ref_PCollection_PCollection_121/Write)\n",
      "Running (((((ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WritePlotsToT_157)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WritePlotsToT_158))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WritePlotsToT_160))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WritePlotsToT_161))+(ref_PCollection_PCollection_87/Write))+(ref_PCollection_PCollection_88/Write)\n",
      "Running (((((ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteMetrics-_141)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteMetrics-_142))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteMetrics-_144))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteMetrics-_145))+(ref_PCollection_PCollection_76/Write))+(ref_PCollection_PCollection_77/Write)\n",
      "Running ((ExtractEvaluateAndWriteResults/WriteResults/WriteMetricsAndPlots/WriteMetrics/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteMetrics-_149))+(ref_PCollection_PCollection_81/Write)\n",
      "Running ((ref_PCollection_PCollection_76/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteMetrics-_150))+(ref_PCollection_PCollection_82/Write)\n",
      "Running ((ExtractEvaluateAndWriteResults/WriteResults/WriteMetricsAndPlots/WritePlotsToTFRecord/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WritePlotsToT_165))+(ref_PCollection_PCollection_92/Write)\n",
      "Running ((ref_PCollection_PCollection_87/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WritePlotsToT_166))+(ref_PCollection_PCollection_93/Write)\n",
      "Running (ref_PCollection_PCollection_87/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WritePlotsToT_167)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.048070430755615234 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.40 seconds.\n",
      "Running (((ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_81)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_82))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_84))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_85)\n",
      "Running ((ref_PCollection_PCollection_65/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteEvalConfig-WriteEvalConfig-Wr_133))+(ref_PCollection_PCollection_71/Write)\n",
      "Running (ref_PCollection_PCollection_65/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteEvalConfig-WriteEvalConfig-Wr_134)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.04296088218688965 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Running (((((ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteAttribut_173)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteAttribut_174))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteAttribut_176))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteAttribut_177))+(ref_PCollection_PCollection_98/Write))+(ref_PCollection_PCollection_99/Write)\n",
      "Running ((ExtractEvaluateAndWriteResults/WriteResults/WriteMetricsAndPlots/WriteAttributionsToTFRecord/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteAttribut_181))+(ref_PCollection_PCollection_103/Write)\n",
      "Running ((ref_PCollection_PCollection_98/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteAttribut_182))+(ref_PCollection_PCollection_104/Write)\n",
      "Running ((((((ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/FanoutSlices/TrackDistinctSliceKeys/RemoveDuplicates/Group/Group/Read)+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/FanoutSlices/TrackDistinctSliceKeys/RemoveDuplicates/Group/Merge))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/FanoutSlices/TrackDistinctSliceKeys/RemoveDuplicates/Group/ExtractOutputs))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_55))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_58))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/FanoutSlices/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/CombinePerKey/Precombine))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/FanoutSlices/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Write)\n",
      "Running ((((ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/FanoutSlices/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Read)+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/FanoutSlices/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/CombinePerKey/Merge))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/FanoutSlices/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_63))+(ref_PCollection_PCollection_28/Write)\n",
      "Running ((((ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_65)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_66))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_68))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_69))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_70)\n",
      "Running (ref_PCollection_PCollection_98/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteAttribut_183)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.03893637657165527 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.60 seconds.\n",
      "Running ((ref_PCollection_PCollection_116/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteValidati_210))+(ref_PCollection_PCollection_122/Write)\n",
      "Running (ref_PCollection_PCollection_116/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteValidati_211)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.03990316390991211 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Running (ref_PCollection_PCollection_76/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteMetrics-_151)\n",
      "Starting the size estimation of the input\n",
      "Finished listing 1 files in 0.044161081314086914 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "From /home/jupyter/.local/lib/python3.7/site-packages/tensorflow_model_analysis/writers/metrics_plots_and_validations_writer.py:113: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "stateful_working_dir /home/jupyter/20220318_Training/gcp-partner-training-mlops/gs:/grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/ModelEvaluator/.system/stateful_working_dir is not found, not going to delete it.\n",
      "I0330 14:51:14.311641   104 rdbms_metadata_access_object.cc:686] No property is defined for the Type\n",
      "I0330 14:51:14.332365   104 rdbms_metadata_access_object.cc:686] No property is defined for the Type\n",
      "I0330 14:51:14.399489   104 rdbms_metadata_access_object.cc:686] No property is defined for the Type\n",
      "stateful_working_dir /home/jupyter/20220318_Training/gcp-partner-training-mlops/gs:/grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/ModelPusher/.system/stateful_working_dir is not found, not going to delete it.\n",
      "I0330 14:51:34.092697   104 rdbms_metadata_access_object.cc:686] No property is defined for the Type\n",
      "Model output: gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/model_registry/chicago-taxi-tips-classifier-v01\n",
      ".\n",
      "\n",
      "=============================== warnings summary ===============================\n",
      "../../.local/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py:22\n",
      "  /home/jupyter/.local/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py:22: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "    import imp\n",
      "\n",
      "../../.local/lib/python3.7/site-packages/apache_beam/typehints/typehints.py:696\n",
      "  /home/jupyter/.local/lib/python3.7/site-packages/apache_beam/typehints/typehints.py:696: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "    if not isinstance(type_params, collections.Iterable):\n",
      "\n",
      "../../.local/lib/python3.7/site-packages/apache_beam/typehints/typehints.py:535\n",
      "  /home/jupyter/.local/lib/python3.7/site-packages/apache_beam/typehints/typehints.py:535: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "    if not isinstance(type_params, (collections.Sequence, set)):\n",
      "\n",
      "src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "  /home/jupyter/.local/lib/python3.7/site-packages/tfx/utils/deprecation_utils.py:188: TfxDeprecationWarning: From /home/jupyter/.local/lib/python3.7/site-packages/tfx/orchestration/portable/resolver_node_handler.py:66: resolve_input_artifacts (from tfx.orchestration.portable.inputs_utils) is deprecated and will be removed after 2021-06-01. Instructions for updating:\n",
      "  Use resolve_input_artifacts_v2() instead.\n",
      "    warnings.warn(msg, TfxDeprecationWarning)\n",
      "\n",
      "src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "  /home/jupyter/.local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery.py:2395: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "    temp_location = pcoll.pipeline.options.view_as(\n",
      "\n",
      "src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "  /home/jupyter/.local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery.py:2397: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "    job_name = pcoll.pipeline.options.view_as(GoogleCloudOptions).job_name\n",
      "\n",
      "src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "  /home/jupyter/.local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery.py:2428: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "    | _PassThroughThenCleanup(files_to_remove_pcoll))\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "================== 1 passed, 10 warnings in 468.93s (0:07:48) ==================\n",
      "Unresolved object in checkpoint: (root).layer-0._init_input_shape\n",
      "A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "Unresolved object in checkpoint: (root).layer-0._init_input_shape\n",
      "A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "Unresolved object in checkpoint: (root).layer-0._init_input_shape\n",
      "A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "Unresolved object in checkpoint: (root).layer-0._init_input_shape\n",
      "A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "Unresolved object in checkpoint: (root).layer-0._init_input_shape\n",
      "A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "Unresolved object in checkpoint: (root).layer-0._init_input_shape\n",
      "A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "Unresolved object in checkpoint: (root).layer-0._init_input_shape\n",
      "A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest src/tests/pipeline_deployment_tests.py::test_e2e_pipeline -s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5704bcb",
   "metadata": {},
   "source": [
    "## 2. Run the training pipeline using Vertex Pipelines\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d7db74",
   "metadata": {},
   "source": [
    "### Set the pipeline configurations for the Vertex AI run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e2fe69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"DATASET_DISPLAY_NAME\"] = DATASET_DISPLAY_NAME\n",
    "os.environ[\"MODEL_DISPLAY_NAME\"] = MODEL_DISPLAY_NAME\n",
    "os.environ[\"PIPELINE_NAME\"] = PIPELINE_NAME\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"GCS_LOCATION\"] = f\"gs://{BUCKET}/{DATASET_DISPLAY_NAME}\"\n",
    "os.environ[\"TRAIN_LIMIT\"] = \"85000\"\n",
    "os.environ[\"TEST_LIMIT\"] = \"15000\"\n",
    "os.environ[\"BEAM_RUNNER\"] = \"DataflowRunner\"\n",
    "os.environ[\"TRAINING_RUNNER\"] = \"vertex\"\n",
    "os.environ[\"TFX_IMAGE_URI\"] = f\"gcr.io/{PROJECT}/{DATASET_DISPLAY_NAME}:{VERSION}\"\n",
    "os.environ[\"ENABLE_CACHE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d83ef31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT: grandelli-demo-295810\n",
      "REGION: us-central1\n",
      "GCS_LOCATION: gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips\n",
      "ARTIFACT_STORE_URI: gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/tfx_artifacts\n",
      "MODEL_REGISTRY_URI: gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/model_registry\n",
      "DATASET_DISPLAY_NAME: chicago-taxi-tips\n",
      "MODEL_DISPLAY_NAME: chicago-taxi-tips-classifier-v01\n",
      "PIPELINE_NAME: chicago-taxi-tips-classifier-v01-train-pipeline\n",
      "ML_USE_COLUMN: ml_use\n",
      "EXCLUDE_COLUMNS: trip_start_timestamp\n",
      "TRAIN_LIMIT: 85000\n",
      "TEST_LIMIT: 15000\n",
      "SERVE_LIMIT: 0\n",
      "NUM_TRAIN_SPLITS: 4\n",
      "NUM_EVAL_SPLITS: 1\n",
      "ACCURACY_THRESHOLD: 0.1\n",
      "USE_KFP_SA: False\n",
      "TFX_IMAGE_URI: gcr.io/grandelli-demo-295810/chicago-taxi-tips:v01\n",
      "BEAM_RUNNER: DataflowRunner\n",
      "BEAM_DIRECT_PIPELINE_ARGS: ['--project=grandelli-demo-295810', '--temp_location=gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/temp']\n",
      "BEAM_DATAFLOW_PIPELINE_ARGS: ['--project=grandelli-demo-295810', '--temp_location=gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/temp', '--region=us-central1', '--runner=DataflowRunner']\n",
      "TRAINING_RUNNER: vertex\n",
      "VERTEX_TRAINING_ARGS: {'project': 'grandelli-demo-295810', 'worker_pool_specs': [{'machine_spec': {'machine_type': 'n1-standard-4'}, 'replica_count': 1, 'container_spec': {'image_uri': 'gcr.io/grandelli-demo-295810/chicago-taxi-tips:v01'}}]}\n",
      "VERTEX_TRAINING_CONFIG: {'ai_platform_training_enable_ucaip': True, 'ai_platform_training_ucaip_region': 'us-central1', 'ai_platform_training_args': {'project': 'grandelli-demo-295810', 'worker_pool_specs': [{'machine_spec': {'machine_type': 'n1-standard-4'}, 'replica_count': 1, 'container_spec': {'image_uri': 'gcr.io/grandelli-demo-295810/chicago-taxi-tips:v01'}}]}, 'use_gpu': False}\n",
      "SERVING_RUNTIME: tf2-cpu.2-5\n",
      "SERVING_IMAGE_URI: us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-5:latest\n",
      "BATCH_PREDICTION_BQ_DATASET_NAME: playground_us\n",
      "BATCH_PREDICTION_BQ_TABLE_NAME: chicago_taxitrips_prep\n",
      "BATCH_PREDICTION_BEAM_ARGS: {'runner': 'DataflowRunner', 'temporary_dir': 'gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/temp', 'gcs_location': 'gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/temp', 'project': 'grandelli-demo-295810', 'region': 'us-central1', 'setup_file': './setup.py'}\n",
      "BATCH_PREDICTION_JOB_RESOURCES: {'machine_type': 'n1-standard-2', 'starting_replica_count': 1, 'max_replica_count': 10}\n",
      "DATASTORE_PREDICTION_KIND: chicago-taxi-tips-classifier-v01-predictions\n",
      "ENABLE_CACHE: 1\n",
      "UPLOAD_MODEL: 0\n"
     ]
    }
   ],
   "source": [
    "from src.tfx_pipelines import config\n",
    "import importlib\n",
    "importlib.reload(config)\n",
    "\n",
    "for key, value in config.__dict__.items():\n",
    "    if key.isupper(): print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f3164f",
   "metadata": {},
   "source": [
    "### Build the ML container image\n",
    "\n",
    "This is the `TFX` runtime environment for the training pipeline steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a0e729b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcr.io/grandelli-demo-295810/chicago-taxi-tips:v01\n"
     ]
    }
   ],
   "source": [
    "!echo $TFX_IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3087da4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 59 file(s) totalling 1.6 MiB before compression.\n",
      "Some files were not included in the source upload.\n",
      "\n",
      "Check the gcloud log [/home/jupyter/.config/gcloud/logs/2022.03.30/14.52.15.947140.log] to see which files and the contents of the\n",
      "default gcloudignore file used (see `$ gcloud topic gcloudignore` to learn\n",
      "more).\n",
      "\n",
      "Uploading tarball of [.] to [gs://grandelli-demo-295810_cloudbuild/source/1648651937.232844-62032b8b010940f09a1a067690a2ecca.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/grandelli-demo-295810/locations/global/builds/40669306-866f-4f2d-ab9b-85589edef8f9].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/40669306-866f-4f2d-ab9b-85589edef8f9?project=155283586619].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"40669306-866f-4f2d-ab9b-85589edef8f9\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://grandelli-demo-295810_cloudbuild/source/1648651937.232844-62032b8b010940f09a1a067690a2ecca.tgz#1648651937885840\n",
      "Copying gs://grandelli-demo-295810_cloudbuild/source/1648651937.232844-62032b8b010940f09a1a067690a2ecca.tgz#1648651937885840...\n",
      "/ [1 files][261.8 KiB/261.8 KiB]                                                \n",
      "Operation completed over 1 objects/261.8 KiB.                                    \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  1.716MB\n",
      "Step 1/5 : FROM gcr.io/tfx-oss-public/tfx:1.2.0\n",
      "1.2.0: Pulling from tfx-oss-public/tfx\n",
      "25fa05cd42bd: Pulling fs layer\n",
      "2d6e353a95ec: Pulling fs layer\n",
      "14d7996407de: Pulling fs layer\n",
      "0c9c6fc70f16: Pulling fs layer\n",
      "c3c76be11512: Pulling fs layer\n",
      "ab6e5a9c78ee: Pulling fs layer\n",
      "7bc1690abd59: Pulling fs layer\n",
      "f5b4dd7682bc: Pulling fs layer\n",
      "d6897660f71d: Pulling fs layer\n",
      "174d792fb622: Pulling fs layer\n",
      "5f8143275aca: Pulling fs layer\n",
      "56646f115483: Pulling fs layer\n",
      "798922b52524: Pulling fs layer\n",
      "e2699a9f592b: Pulling fs layer\n",
      "f43e7d1c07e4: Pulling fs layer\n",
      "1e71d5e9923d: Pulling fs layer\n",
      "bf6ae2a2e250: Pulling fs layer\n",
      "e49679b748d5: Pulling fs layer\n",
      "80208bd6f7fb: Pulling fs layer\n",
      "b83c16bef138: Pulling fs layer\n",
      "9d1427033824: Pulling fs layer\n",
      "c0028679f003: Pulling fs layer\n",
      "09c222e7ff04: Pulling fs layer\n",
      "ae6048a3aec1: Pulling fs layer\n",
      "1ced637de50b: Pulling fs layer\n",
      "762ff1eb7f16: Pulling fs layer\n",
      "f6f8f4265c8c: Pulling fs layer\n",
      "595b1c49222a: Pulling fs layer\n",
      "b6c7eb38f366: Pulling fs layer\n",
      "520be5017b4d: Pulling fs layer\n",
      "0a1aacb7e387: Pulling fs layer\n",
      "8f605134caf9: Pulling fs layer\n",
      "189ba322d030: Pulling fs layer\n",
      "af92447b9198: Pulling fs layer\n",
      "0c9c6fc70f16: Waiting\n",
      "c3c76be11512: Waiting\n",
      "ab6e5a9c78ee: Waiting\n",
      "7bc1690abd59: Waiting\n",
      "f5b4dd7682bc: Waiting\n",
      "d6897660f71d: Waiting\n",
      "174d792fb622: Waiting\n",
      "5f8143275aca: Waiting\n",
      "56646f115483: Waiting\n",
      "798922b52524: Waiting\n",
      "1ced637de50b: Waiting\n",
      "e2699a9f592b: Waiting\n",
      "f43e7d1c07e4: Waiting\n",
      "762ff1eb7f16: Waiting\n",
      "1e71d5e9923d: Waiting\n",
      "f6f8f4265c8c: Waiting\n",
      "bf6ae2a2e250: Waiting\n",
      "e49679b748d5: Waiting\n",
      "595b1c49222a: Waiting\n",
      "80208bd6f7fb: Waiting\n",
      "b83c16bef138: Waiting\n",
      "9d1427033824: Waiting\n",
      "c0028679f003: Waiting\n",
      "09c222e7ff04: Waiting\n",
      "b6c7eb38f366: Waiting\n",
      "ae6048a3aec1: Waiting\n",
      "af92447b9198: Waiting\n",
      "0a1aacb7e387: Waiting\n",
      "8f605134caf9: Waiting\n",
      "189ba322d030: Waiting\n",
      "520be5017b4d: Waiting\n",
      "14d7996407de: Verifying Checksum\n",
      "14d7996407de: Download complete\n",
      "2d6e353a95ec: Download complete\n",
      "0c9c6fc70f16: Verifying Checksum\n",
      "0c9c6fc70f16: Download complete\n",
      "25fa05cd42bd: Verifying Checksum\n",
      "25fa05cd42bd: Download complete\n",
      "c3c76be11512: Verifying Checksum\n",
      "c3c76be11512: Download complete\n",
      "7bc1690abd59: Verifying Checksum\n",
      "7bc1690abd59: Download complete\n",
      "d6897660f71d: Verifying Checksum\n",
      "d6897660f71d: Download complete\n",
      "25fa05cd42bd: Pull complete\n",
      "2d6e353a95ec: Pull complete\n",
      "14d7996407de: Pull complete\n",
      "0c9c6fc70f16: Pull complete\n",
      "c3c76be11512: Pull complete\n",
      "174d792fb622: Download complete\n",
      "5f8143275aca: Verifying Checksum\n",
      "5f8143275aca: Download complete\n",
      "f5b4dd7682bc: Verifying Checksum\n",
      "f5b4dd7682bc: Download complete\n",
      "798922b52524: Verifying Checksum\n",
      "798922b52524: Download complete\n",
      "e2699a9f592b: Verifying Checksum\n",
      "e2699a9f592b: Download complete\n",
      "f43e7d1c07e4: Verifying Checksum\n",
      "f43e7d1c07e4: Download complete\n",
      "56646f115483: Verifying Checksum\n",
      "56646f115483: Download complete\n",
      "bf6ae2a2e250: Verifying Checksum\n",
      "bf6ae2a2e250: Download complete\n",
      "e49679b748d5: Verifying Checksum\n",
      "e49679b748d5: Download complete\n",
      "80208bd6f7fb: Verifying Checksum\n",
      "80208bd6f7fb: Download complete\n",
      "ab6e5a9c78ee: Verifying Checksum\n",
      "ab6e5a9c78ee: Download complete\n",
      "b83c16bef138: Verifying Checksum\n",
      "b83c16bef138: Download complete\n",
      "9d1427033824: Verifying Checksum\n",
      "9d1427033824: Download complete\n",
      "c0028679f003: Verifying Checksum\n",
      "c0028679f003: Download complete\n",
      "09c222e7ff04: Verifying Checksum\n",
      "09c222e7ff04: Download complete\n",
      "ae6048a3aec1: Verifying Checksum\n",
      "ae6048a3aec1: Download complete\n",
      "1e71d5e9923d: Verifying Checksum\n",
      "1e71d5e9923d: Download complete\n",
      "f6f8f4265c8c: Verifying Checksum\n",
      "f6f8f4265c8c: Download complete\n",
      "1ced637de50b: Verifying Checksum\n",
      "1ced637de50b: Download complete\n",
      "595b1c49222a: Verifying Checksum\n",
      "595b1c49222a: Download complete\n",
      "520be5017b4d: Verifying Checksum\n",
      "520be5017b4d: Download complete\n",
      "0a1aacb7e387: Verifying Checksum\n",
      "0a1aacb7e387: Download complete\n",
      "8f605134caf9: Download complete\n",
      "189ba322d030: Verifying Checksum\n",
      "189ba322d030: Download complete\n",
      "b6c7eb38f366: Verifying Checksum\n",
      "b6c7eb38f366: Download complete\n",
      "af92447b9198: Verifying Checksum\n",
      "af92447b9198: Download complete\n",
      "ab6e5a9c78ee: Pull complete\n",
      "7bc1690abd59: Pull complete\n",
      "762ff1eb7f16: Verifying Checksum\n",
      "762ff1eb7f16: Download complete\n",
      "f5b4dd7682bc: Pull complete\n",
      "d6897660f71d: Pull complete\n",
      "174d792fb622: Pull complete\n",
      "5f8143275aca: Pull complete\n",
      "56646f115483: Pull complete\n",
      "798922b52524: Pull complete\n",
      "e2699a9f592b: Pull complete\n",
      "f43e7d1c07e4: Pull complete\n",
      "1e71d5e9923d: Pull complete\n",
      "bf6ae2a2e250: Pull complete\n",
      "e49679b748d5: Pull complete\n",
      "80208bd6f7fb: Pull complete\n",
      "b83c16bef138: Pull complete\n",
      "9d1427033824: Pull complete\n",
      "c0028679f003: Pull complete\n",
      "09c222e7ff04: Pull complete\n",
      "ae6048a3aec1: Pull complete\n",
      "1ced637de50b: Pull complete\n",
      "762ff1eb7f16: Pull complete\n",
      "f6f8f4265c8c: Pull complete\n",
      "595b1c49222a: Pull complete\n",
      "b6c7eb38f366: Pull complete\n",
      "520be5017b4d: Pull complete\n",
      "0a1aacb7e387: Pull complete\n",
      "8f605134caf9: Pull complete\n",
      "189ba322d030: Pull complete\n",
      "af92447b9198: Pull complete\n",
      "Digest: sha256:eba9e7b7d9131eb5b05434feaafc4676268ad805e4b97218f58994ad2714be67\n",
      "Status: Downloaded newer image for gcr.io/tfx-oss-public/tfx:1.2.0\n",
      " ---> 0e86fadcc60c\n",
      "Step 2/5 : COPY requirements.txt requirements.txt\n",
      " ---> 200ebfd7c7b8\n",
      "Step 3/5 : RUN pip install -r requirements.txt\n",
      " ---> Running in af74e5d920f3\n",
      "Collecting kfp==1.8.1\n",
      "  Downloading kfp-1.8.1.tar.gz (248 kB)\n",
      "Collecting google-cloud-bigquery==2.26.0\n",
      "  Downloading google_cloud_bigquery-2.26.0-py2.py3-none-any.whl (201 kB)\n",
      "Collecting google-cloud-bigquery-storage==2.7.0\n",
      "  Downloading google_cloud_bigquery_storage-2.7.0-py2.py3-none-any.whl (125 kB)\n",
      "Collecting google-cloud-aiplatform==1.4.3\n",
      "  Downloading google_cloud_aiplatform-1.4.3-py2.py3-none-any.whl (1.4 MB)\n",
      "Collecting cloudml-hypertune==0.1.0.dev6\n",
      "  Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "Collecting apache-beam==2.34.0\n",
      "  Downloading apache_beam-2.34.0-cp37-cp37m-manylinux2010_x86_64.whl (9.8 MB)\n",
      "Collecting pytest\n",
      "  Downloading pytest-7.1.1-py3-none-any.whl (297 kB)\n",
      "Collecting absl-py<=0.11,>=0.9\n",
      "  Downloading absl_py-0.11.0-py3-none-any.whl (127 kB)\n",
      "Requirement already satisfied: PyYAML<6,>=5.3 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.1->-r requirements.txt (line 1)) (5.4.1)\n",
      "Requirement already satisfied: google-cloud-storage<2,>=1.20.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.1->-r requirements.txt (line 1)) (1.41.1)\n",
      "Requirement already satisfied: kubernetes<19,>=8.0.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.1->-r requirements.txt (line 1)) (12.0.1)\n",
      "Requirement already satisfied: google-api-python-client<2,>=1.7.8 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.1->-r requirements.txt (line 1)) (1.12.8)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.1->-r requirements.txt (line 1)) (1.34.0)\n",
      "Collecting requests-toolbelt<1,>=0.8.0\n",
      "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "Requirement already satisfied: cloudpickle<2,>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.1->-r requirements.txt (line 1)) (1.6.0)\n",
      "Collecting kfp-server-api<2.0.0,>=1.1.2\n",
      "  Downloading kfp-server-api-1.8.1.tar.gz (54 kB)\n",
      "Requirement already satisfied: jsonschema<4,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.1->-r requirements.txt (line 1)) (3.2.0)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.1->-r requirements.txt (line 1)) (0.8.9)\n",
      "Requirement already satisfied: click<8,>=7.1.1 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.1->-r requirements.txt (line 1)) (7.1.2)\n",
      "Collecting Deprecated<2,>=1.2.7\n",
      "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting strip-hints<1,>=0.1.8\n",
      "  Downloading strip-hints-0.1.10.tar.gz (29 kB)\n",
      "Collecting docstring-parser<1,>=0.7.3\n",
      "  Downloading docstring_parser-0.13.tar.gz (23 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Collecting kfp-pipeline-spec<0.2.0,>=0.1.10\n",
      "  Downloading kfp_pipeline_spec-0.1.14-py3-none-any.whl (18 kB)\n",
      "Collecting fire<1,>=0.3.1\n",
      "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
      "Requirement already satisfied: protobuf<4,>=3.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.1->-r requirements.txt (line 1)) (3.16.0)\n",
      "Requirement already satisfied: pydantic<2,>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.1->-r requirements.txt (line 1)) (1.8.2)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.26.0->-r requirements.txt (line 2)) (2.25.1)\n",
      "Requirement already satisfied: proto-plus>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.26.0->-r requirements.txt (line 2)) (1.19.0)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.26.0->-r requirements.txt (line 2)) (1.3.2)\n",
      "Requirement already satisfied: google-api-core[grpc]<3.0.0dev,>=1.29.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.26.0->-r requirements.txt (line 2)) (1.31.1)\n",
      "Collecting grpcio<2.0dev,>=1.38.1\n",
      "  Downloading grpcio-1.44.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.26.0->-r requirements.txt (line 2)) (1.7.2)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.26.0->-r requirements.txt (line 2)) (20.9)\n",
      "Requirement already satisfied: libcst>=0.2.5 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery-storage==2.7.0->-r requirements.txt (line 3)) (0.3.19)\n",
      "Requirement already satisfied: pytz>=2018.3 in /opt/conda/lib/python3.7/site-packages (from apache-beam==2.34.0->-r requirements.txt (line 6)) (2021.1)\n",
      "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam==2.34.0->-r requirements.txt (line 6)) (2.6.0)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam==2.34.0->-r requirements.txt (line 6)) (2.8.2)\n",
      "Requirement already satisfied: pydot<2,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam==2.34.0->-r requirements.txt (line 6)) (1.4.2)\n",
      "Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam==2.34.0->-r requirements.txt (line 6)) (0.3.1.1)\n",
      "Collecting orjson<4.0\n",
      "  Downloading orjson-3.6.7-cp37-cp37m-manylinux_2_24_x86_64.whl (255 kB)\n",
      "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam==2.34.0->-r requirements.txt (line 6)) (3.12.0)\n",
      "Requirement already satisfied: crcmod<2.0,>=1.7 in /opt/conda/lib/python3.7/site-packages (from apache-beam==2.34.0->-r requirements.txt (line 6)) (1.7)\n",
      "Requirement already satisfied: numpy<1.21.0,>=1.14.3 in /opt/conda/lib/python3.7/site-packages (from apache-beam==2.34.0->-r requirements.txt (line 6)) (1.19.5)\n",
      "Requirement already satisfied: fastavro<2,>=0.21.4 in /opt/conda/lib/python3.7/site-packages (from apache-beam==2.34.0->-r requirements.txt (line 6)) (1.4.4)\n",
      "Requirement already satisfied: oauth2client<5,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam==2.34.0->-r requirements.txt (line 6)) (4.1.3)\n",
      "Requirement already satisfied: typing-extensions<4,>=3.7.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam==2.34.0->-r requirements.txt (line 6)) (3.7.4.3)\n",
      "Requirement already satisfied: pyarrow<6.0.0,>=0.15.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam==2.34.0->-r requirements.txt (line 6)) (2.0.0)\n",
      "Requirement already satisfied: avro-python3!=1.9.2,<1.10.0,>=1.8.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam==2.34.0->-r requirements.txt (line 6)) (1.9.2.1)\n",
      "Requirement already satisfied: future<1.0.0,>=0.18.2 in /opt/conda/lib/python3.7/site-packages (from apache-beam==2.34.0->-r requirements.txt (line 6)) (0.18.2)\n",
      "Requirement already satisfied: httplib2<0.20.0,>=0.8 in /opt/conda/lib/python3.7/site-packages (from apache-beam==2.34.0->-r requirements.txt (line 6)) (0.19.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.12 in /opt/conda/lib/python3.7/site-packages (from pytest->-r requirements.txt (line 7)) (4.6.3)\n",
      "Collecting py>=1.8.2\n",
      "  Downloading py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
      "Collecting iniconfig\n",
      "  Downloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from pytest->-r requirements.txt (line 7)) (1.1.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.7/site-packages (from pytest->-r requirements.txt (line 7)) (20.3.0)\n",
      "Collecting pluggy<2.0,>=0.12\n",
      "  Downloading pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from absl-py<=0.11,>=0.9->kfp==1.8.1->-r requirements.txt (line 1)) (1.15.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated<2,>=1.2.7->kfp==1.8.1->-r requirements.txt (line 1)) (1.12.1)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.7/site-packages (from fire<1,>=0.3.1->kfp==1.8.1->-r requirements.txt (line 1)) (1.1.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.29.0->google-cloud-bigquery==2.26.0->-r requirements.txt (line 2)) (1.53.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.29.0->google-cloud-bigquery==2.26.0->-r requirements.txt (line 2)) (49.6.0.post20210108)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp==1.8.1->-r requirements.txt (line 1)) (3.0.1)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp==1.8.1->-r requirements.txt (line 1)) (0.1.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.8.1->-r requirements.txt (line 1)) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.8.1->-r requirements.txt (line 1)) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.8.1->-r requirements.txt (line 1)) (0.2.7)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery==2.26.0->-r requirements.txt (line 2)) (1.1.2)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery==2.26.0->-r requirements.txt (line 2)) (1.14.6)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery==2.26.0->-r requirements.txt (line 2)) (2.20)\n",
      "Requirement already satisfied: docopt in /opt/conda/lib/python3.7/site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam==2.34.0->-r requirements.txt (line 6)) (0.6.2)\n",
      "Requirement already satisfied: pyparsing<3,>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from httplib2<0.20.0,>=0.8->apache-beam==2.34.0->-r requirements.txt (line 6)) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.12->pytest->-r requirements.txt (line 7)) (3.5.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp==1.8.1->-r requirements.txt (line 1)) (0.17.3)\n",
      "Requirement already satisfied: urllib3>=1.15 in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.8.1->-r requirements.txt (line 1)) (1.26.6)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.8.1->-r requirements.txt (line 1)) (2021.5.30)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<19,>=8.0.0->kfp==1.8.1->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<19,>=8.0.0->kfp==1.8.1->-r requirements.txt (line 1)) (0.57.0)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from libcst>=0.2.5->google-cloud-bigquery-storage==2.7.0->-r requirements.txt (line 3)) (0.7.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /opt/conda/lib/python3.7/site-packages (from oauth2client<5,>=2.0.1->apache-beam==2.34.0->-r requirements.txt (line 6)) (0.4.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery==2.26.0->-r requirements.txt (line 2)) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery==2.26.0->-r requirements.txt (line 2)) (4.0.0)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints<1,>=0.1.8->kfp==1.8.1->-r requirements.txt (line 1)) (0.36.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from typing-inspect>=0.4.0->libcst>=0.2.5->google-cloud-bigquery-storage==2.7.0->-r requirements.txt (line 3)) (0.4.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<19,>=8.0.0->kfp==1.8.1->-r requirements.txt (line 1)) (3.1.1)\n",
      "Building wheels for collected packages: kfp, cloudml-hypertune, docstring-parser, fire, kfp-server-api, strip-hints\n",
      "  Building wheel for kfp (setup.py): started\n",
      "  Building wheel for kfp (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp: filename=kfp-1.8.1-py3-none-any.whl size=345340 sha256=46f452492bfef4eebd278648e0a710d6532f81ae9bbcf60fe468c1428bf213a1\n",
      "  Stored in directory: /root/.cache/pip/wheels/02/dd/71/45fa445fd9ea0c60ab0bd00b31760b1102ef2999f8002ee892\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3988 sha256=2e8dab40020eb75046e14effe51d84135d2cfa0bfb212df1e232a503ac45742a\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/ff/87/e7bed0c2741fe219b3d6da67c2431d7f7fedb183032e00f81e\n",
      "  Building wheel for docstring-parser (PEP 517): started\n",
      "  Building wheel for docstring-parser (PEP 517): finished with status 'done'\n",
      "  Created wheel for docstring-parser: filename=docstring_parser-0.13-py3-none-any.whl size=31866 sha256=dc8e2dfd6ba62bdbab2d9b014c1640ae88c84f7889ae56d6f6b87deda964aa77\n",
      "  Stored in directory: /root/.cache/pip/wheels/bd/88/3c/d1aa049309f7945178cac9fbe6561a86424f432da57c18ca0f\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115928 sha256=b6edb578b982467576e64b69252f07c5ba402de7ef7e5678bc0f9e61c6154b92\n",
      "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
      "  Building wheel for kfp-server-api (setup.py): started\n",
      "  Building wheel for kfp-server-api (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp-server-api: filename=kfp_server_api-1.8.1-py3-none-any.whl size=95548 sha256=5b0e137624885bdfc1f7e38fb9e0b222e1b6f3744311976d7e9735f9a3872f9f\n",
      "  Stored in directory: /root/.cache/pip/wheels/f5/4e/2e/6795bd3ed456a43652e7de100aca275ec179c9a8dfbcc65626\n",
      "  Building wheel for strip-hints (setup.py): started\n",
      "  Building wheel for strip-hints (setup.py): finished with status 'done'\n",
      "  Created wheel for strip-hints: filename=strip_hints-0.1.10-py2.py3-none-any.whl size=22279 sha256=4f21a0107d556fbdb11ea45e46982a6ad43b53590a7bcf93274564f57ed2de5a\n",
      "  Stored in directory: /root/.cache/pip/wheels/5e/14/c3/6e44e9b2545f2d570b03f5b6d38c00b7534aa8abb376978363\n",
      "Successfully built kfp cloudml-hypertune docstring-parser fire kfp-server-api strip-hints\n",
      "Installing collected packages: grpcio, strip-hints, requests-toolbelt, py, pluggy, orjson, kfp-server-api, kfp-pipeline-spec, iniconfig, google-cloud-bigquery, fire, docstring-parser, Deprecated, absl-py, pytest, kfp, google-cloud-bigquery-storage, google-cloud-aiplatform, cloudml-hypertune, apache-beam\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.34.1\n",
      "    Uninstalling grpcio-1.34.1:\n",
      "      Successfully uninstalled grpcio-1.34.1\n",
      "  Attempting uninstall: kfp-pipeline-spec\n",
      "    Found existing installation: kfp-pipeline-spec 0.1.9\n",
      "    Uninstalling kfp-pipeline-spec-0.1.9:\n",
      "      Successfully uninstalled kfp-pipeline-spec-0.1.9\n",
      "  Attempting uninstall: google-cloud-bigquery\n",
      "    Found existing installation: google-cloud-bigquery 2.20.0\n",
      "    Uninstalling google-cloud-bigquery-2.20.0:\n",
      "      Successfully uninstalled google-cloud-bigquery-2.20.0\n",
      "  Attempting uninstall: absl-py\n",
      "    Found existing installation: absl-py 0.12.0\n",
      "    Uninstalling absl-py-0.12.0:\n",
      "      Successfully uninstalled absl-py-0.12.0\n",
      "  Attempting uninstall: google-cloud-bigquery-storage\n",
      "    Found existing installation: google-cloud-bigquery-storage 2.6.2\n",
      "    Uninstalling google-cloud-bigquery-storage-2.6.2:\n",
      "      Successfully uninstalled google-cloud-bigquery-storage-2.6.2\n",
      "  Attempting uninstall: google-cloud-aiplatform\n",
      "    Found existing installation: google-cloud-aiplatform 0.7.1\n",
      "    Uninstalling google-cloud-aiplatform-0.7.1:\n",
      "      Successfully uninstalled google-cloud-aiplatform-0.7.1\n",
      "  Attempting uninstall: apache-beam\n",
      "    Found existing installation: apache-beam 2.31.0\n",
      "    Uninstalling apache-beam-2.31.0:\n",
      "      Successfully uninstalled apache-beam-2.31.0\n",
      "Successfully installed Deprecated-1.2.13 absl-py-0.11.0 apache-beam-2.34.0 cloudml-hypertune-0.1.0.dev6 docstring-parser-0.13 fire-0.4.0 google-cloud-aiplatform-1.4.3 google-cloud-bigquery-2.26.0 google-cloud-bigquery-storage-2.7.0 grpcio-1.44.0 iniconfig-1.1.1 kfp-1.8.1 kfp-pipeline-spec-0.1.14 kfp-server-api-1.8.1 orjson-3.6.7 pluggy-1.0.0 py-1.11.0 pytest-7.1.1 requests-toolbelt-0.9.1 strip-hints-0.1.10\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-io 0.18.0 requires tensorflow-io-gcs-filesystem==0.18.0, which is not installed.\n",
      "tfx 1.2.0 requires google-cloud-aiplatform<0.8,>=0.5.0, but you have google-cloud-aiplatform 1.4.3 which is incompatible.\n",
      "tfx 1.2.0 requires google-cloud-bigquery<2.21,>=1.28.0, but you have google-cloud-bigquery 2.26.0 which is incompatible.\n",
      "tfx-bsl 1.2.0 requires google-cloud-bigquery<2.21,>=1.28.0, but you have google-cloud-bigquery 2.26.0 which is incompatible.\n",
      "tensorflow 2.5.0 requires grpcio~=1.34.0, but you have grpcio 1.44.0 which is incompatible.\n",
      "tensorflow-transform 1.2.0 requires google-cloud-bigquery<2.21,>=1.28.0, but you have google-cloud-bigquery 2.26.0 which is incompatible.\n",
      "tensorflow-model-analysis 0.33.0 requires google-cloud-bigquery<2.21,>=1.28.0, but you have google-cloud-bigquery 2.26.0 which is incompatible.\n",
      "tensorflow-data-validation 1.2.0 requires google-cloud-bigquery<2.21,>=1.28.0, but you have google-cloud-bigquery 2.26.0 which is incompatible.\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "WARNING: You are using pip version 21.2.4; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\n",
      "Removing intermediate container af74e5d920f3\n",
      " ---> d91771851d87\n",
      "Step 4/5 : COPY src/ src/\n",
      " ---> 6e9eb053b3ac\n",
      "Step 5/5 : ENV PYTHONPATH=\"/pipeline:${PYTHONPATH}\"\n",
      " ---> Running in afa0b9822be1\n",
      "Removing intermediate container afa0b9822be1\n",
      " ---> b8470ba5714c\n",
      "Successfully built b8470ba5714c\n",
      "Successfully tagged gcr.io/grandelli-demo-295810/chicago-taxi-tips:v01\n",
      "PUSH\n",
      "Pushing gcr.io/grandelli-demo-295810/chicago-taxi-tips:v01\n",
      "The push refers to repository [gcr.io/grandelli-demo-295810/chicago-taxi-tips]\n",
      "fd08bf71d746: Preparing\n",
      "7aac39fef2ef: Preparing\n",
      "89fc37ad761b: Preparing\n",
      "d42c05f73feb: Preparing\n",
      "3119d30f29a9: Preparing\n",
      "121c9dfc7bce: Preparing\n",
      "868786b3710b: Preparing\n",
      "5bb1aa5df10d: Preparing\n",
      "f028010939aa: Preparing\n",
      "dc99c4ea3a81: Preparing\n",
      "37b508c5711b: Preparing\n",
      "756ab564e194: Preparing\n",
      "2ae86808a3d1: Preparing\n",
      "1dccbdf9b557: Preparing\n",
      "cfcbdbc2b748: Preparing\n",
      "937ab8f29c2e: Preparing\n",
      "5d417b2f7486: Preparing\n",
      "d6a297a3e6e4: Preparing\n",
      "6474a5e8117f: Preparing\n",
      "fe498124ed57: Preparing\n",
      "d5454704bb3d: Preparing\n",
      "fb896ef24b4b: Preparing\n",
      "5087113f67c8: Preparing\n",
      "2a92857a1d48: Preparing\n",
      "0ded97864c52: Preparing\n",
      "b50bbaac3e32: Preparing\n",
      "262ea1af4c10: Preparing\n",
      "b420a468ca49: Preparing\n",
      "608c205798d1: Preparing\n",
      "0760cd6d4269: Preparing\n",
      "fb4755c89c2a: Preparing\n",
      "22cfb9034da6: Preparing\n",
      "8bec4fbfce85: Preparing\n",
      "3b129ca3db46: Preparing\n",
      "64cb1a1930ab: Preparing\n",
      "600ef5a43f1f: Preparing\n",
      "8f8f0266f834: Preparing\n",
      "121c9dfc7bce: Waiting\n",
      "fb896ef24b4b: Waiting\n",
      "1dccbdf9b557: Waiting\n",
      "5087113f67c8: Waiting\n",
      "cfcbdbc2b748: Waiting\n",
      "2a92857a1d48: Waiting\n",
      "937ab8f29c2e: Waiting\n",
      "0ded97864c52: Waiting\n",
      "0760cd6d4269: Waiting\n",
      "5d417b2f7486: Waiting\n",
      "868786b3710b: Waiting\n",
      "b50bbaac3e32: Waiting\n",
      "fb4755c89c2a: Waiting\n",
      "d6a297a3e6e4: Waiting\n",
      "22cfb9034da6: Waiting\n",
      "262ea1af4c10: Waiting\n",
      "6474a5e8117f: Waiting\n",
      "8bec4fbfce85: Waiting\n",
      "b420a468ca49: Waiting\n",
      "fe498124ed57: Waiting\n",
      "3b129ca3db46: Waiting\n",
      "5bb1aa5df10d: Waiting\n",
      "64cb1a1930ab: Waiting\n",
      "d5454704bb3d: Waiting\n",
      "608c205798d1: Waiting\n",
      "600ef5a43f1f: Waiting\n",
      "f028010939aa: Waiting\n",
      "dc99c4ea3a81: Waiting\n",
      "8f8f0266f834: Waiting\n",
      "2ae86808a3d1: Waiting\n",
      "d42c05f73feb: Layer already exists\n",
      "3119d30f29a9: Layer already exists\n",
      "868786b3710b: Layer already exists\n",
      "121c9dfc7bce: Layer already exists\n",
      "5bb1aa5df10d: Layer already exists\n",
      "f028010939aa: Layer already exists\n",
      "37b508c5711b: Layer already exists\n",
      "dc99c4ea3a81: Layer already exists\n",
      "756ab564e194: Layer already exists\n",
      "2ae86808a3d1: Layer already exists\n",
      "1dccbdf9b557: Layer already exists\n",
      "cfcbdbc2b748: Layer already exists\n",
      "937ab8f29c2e: Layer already exists\n",
      "5d417b2f7486: Layer already exists\n",
      "d6a297a3e6e4: Layer already exists\n",
      "6474a5e8117f: Layer already exists\n",
      "fe498124ed57: Layer already exists\n",
      "d5454704bb3d: Layer already exists\n",
      "fb896ef24b4b: Layer already exists\n",
      "5087113f67c8: Layer already exists\n",
      "2a92857a1d48: Layer already exists\n",
      "0ded97864c52: Layer already exists\n",
      "b50bbaac3e32: Layer already exists\n",
      "262ea1af4c10: Layer already exists\n",
      "b420a468ca49: Layer already exists\n",
      "608c205798d1: Layer already exists\n",
      "0760cd6d4269: Layer already exists\n",
      "fb4755c89c2a: Layer already exists\n",
      "22cfb9034da6: Layer already exists\n",
      "8bec4fbfce85: Layer already exists\n",
      "3b129ca3db46: Layer already exists\n",
      "64cb1a1930ab: Layer already exists\n",
      "600ef5a43f1f: Layer already exists\n",
      "8f8f0266f834: Layer already exists\n",
      "89fc37ad761b: Pushed\n",
      "fd08bf71d746: Pushed\n",
      "7aac39fef2ef: Pushed\n",
      "v01: digest: sha256:bb58d709252b6a28a4d4076e339a371e6849bc1ed47dd809e0db1094dc4c4ba7 size: 8102\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                               IMAGES                                              STATUS\n",
      "40669306-866f-4f2d-ab9b-85589edef8f9  2022-03-30T14:52:18+00:00  3M11S     gs://grandelli-demo-295810_cloudbuild/source/1648651937.232844-62032b8b010940f09a1a067690a2ecca.tgz  gcr.io/grandelli-demo-295810/chicago-taxi-tips:v01  SUCCESS\n",
      "\n",
      "\n",
      "To take a quick anonymous survey, run:\n",
      "  $ gcloud survey\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --tag $TFX_IMAGE_URI . --timeout=15m --machine-type=e2-highcpu-8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155568ca",
   "metadata": {},
   "source": [
    "### Compile pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c1d5ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tfx_pipelines import runner\n",
    "\n",
    "pipeline_definition_file = f'{config.PIPELINE_NAME}.json'\n",
    "pipeline_definition = runner.compile_training_pipeline(pipeline_definition_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6644c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://chicago-taxi-tips-classifier-v01-train-pipeline.json [Content-Type=application/json]...\n",
      "/ [1 files][ 26.6 KiB/ 26.6 KiB]                                                \n",
      "Operation completed over 1 objects/26.6 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "PIPELINES_STORE = f\"gs://{BUCKET}/{DATASET_DISPLAY_NAME}/compiled_pipelines/\"\n",
    "!gsutil cp {pipeline_definition_file} {PIPELINES_STORE}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcb943e",
   "metadata": {},
   "source": [
    "### Submit run to Vertex Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62c6b0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/kfp/v2/google/client/client.py:173: FutureWarning: AIPlatformClient will be deprecated in v2.0.0. Please use PipelineJob https://googleapis.dev/python/aiplatform/latest/_modules/google/cloud/aiplatform/pipeline_jobs.html in Vertex SDK. Install the SDK using \"pip install google-cloud-aiplatform\"\n",
      "  category=FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "See the Pipeline job <a href=\"https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/chicago-taxi-tips-classifier-v01-train-pipeline-20220330170552?project=grandelli-demo-295810\" target=\"_blank\" >here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from kfp.v2.google.client import AIPlatformClient\n",
    "\n",
    "pipeline_client = AIPlatformClient(\n",
    "    project_id=PROJECT, region=REGION)\n",
    "                 \n",
    "job = pipeline_client.create_run_from_job_spec(\n",
    "    job_spec_path=pipeline_definition_file,\n",
    "    parameter_values={\n",
    "        'learning_rate': 0.003,\n",
    "        'batch_size': 512,\n",
    "        'hidden_units': '128,128',\n",
    "        'num_epochs': 30,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888be1fd",
   "metadata": {},
   "source": [
    "### Extracting pipeline runs metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37ae4aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0330 17:06:28.880802721       1 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "E0330 17:06:30.743903842       1 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "E0330 17:06:32.576653479       1 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pipeline_name</th>\n",
       "      <td>chicago-taxi-tips-classifier-v01-train-pipeline</td>\n",
       "      <td>chicago-taxi-tips-classifier-v01-train-pipeline</td>\n",
       "      <td>chicago-taxi-tips-classifier-v01-train-pipeline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>run_name</th>\n",
       "      <td>chicago-taxi-tips-classifier-v01-train-pipelin...</td>\n",
       "      <td>chicago-taxi-tips-classifier-v01-train-pipelin...</td>\n",
       "      <td>chicago-taxi-tips-classifier-v01-train-pipelin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param.input:hidden_units</th>\n",
       "      <td>128,128</td>\n",
       "      <td>256,126</td>\n",
       "      <td>128,128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param.input:num_epochs</th>\n",
       "      <td>30</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param.input:batch_size</th>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param.input:learning_rate</th>\n",
       "      <td>0.003</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                           0  \\\n",
       "pipeline_name                chicago-taxi-tips-classifier-v01-train-pipeline   \n",
       "run_name                   chicago-taxi-tips-classifier-v01-train-pipelin...   \n",
       "param.input:hidden_units                                             128,128   \n",
       "param.input:num_epochs                                                    30   \n",
       "param.input:batch_size                                                   512   \n",
       "param.input:learning_rate                                              0.003   \n",
       "\n",
       "                                                                           1  \\\n",
       "pipeline_name                chicago-taxi-tips-classifier-v01-train-pipeline   \n",
       "run_name                   chicago-taxi-tips-classifier-v01-train-pipelin...   \n",
       "param.input:hidden_units                                             256,126   \n",
       "param.input:num_epochs                                                     7   \n",
       "param.input:batch_size                                                   512   \n",
       "param.input:learning_rate                                             0.0015   \n",
       "\n",
       "                                                                           2  \n",
       "pipeline_name                chicago-taxi-tips-classifier-v01-train-pipeline  \n",
       "run_name                   chicago-taxi-tips-classifier-v01-train-pipelin...  \n",
       "param.input:hidden_units                                             128,128  \n",
       "param.input:num_epochs                                                    30  \n",
       "param.input:batch_size                                                   512  \n",
       "param.input:learning_rate                                              0.003  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.cloud import aiplatform as vertex_ai\n",
    "\n",
    "pipeline_df = vertex_ai.get_pipeline_df(PIPELINE_NAME)\n",
    "pipeline_df = pipeline_df[pipeline_df.pipeline_name == PIPELINE_NAME]\n",
    "pipeline_df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b454fe9",
   "metadata": {},
   "source": [
    "## 3. Execute the pipeline deployment CI/CD steps in Cloud Build\n",
    "\n",
    "The CI/CD routine is defined in the [pipeline-deployment.yaml](pipeline-deployment.yaml) file, and consists of the following steps:\n",
    "1. Clone the repository to the build environment.\n",
    "2. Run unit tests.\n",
    "3. Run a local e2e test of the pipeline.\n",
    "4. Build the ML container image for pipeline steps.\n",
    "5. Compile the pipeline.\n",
    "6. Upload the pipeline to Cloud Storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29688d4d",
   "metadata": {},
   "source": [
    "### Build CI/CD container Image for Cloud Build\n",
    "\n",
    "This is the runtime environment where the steps of testing and deploying the pipeline will be executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4759b85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcr.io/grandelli-demo-295810/cicd:latest\n"
     ]
    }
   ],
   "source": [
    "!echo $CICD_IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4fc09c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 5 file(s) totalling 11.2 KiB before compression.\n",
      "Uploading tarball of [build/.] to [gs://grandelli-demo-295810_cloudbuild/source/1648660042.412306-fc957ae3c7f148d3ba43f9b975d14364.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/grandelli-demo-295810/locations/global/builds/f128fce0-bcfb-4d87-ae0b-eb72bb4d00cb].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/f128fce0-bcfb-4d87-ae0b-eb72bb4d00cb?project=155283586619].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"f128fce0-bcfb-4d87-ae0b-eb72bb4d00cb\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://grandelli-demo-295810_cloudbuild/source/1648660042.412306-fc957ae3c7f148d3ba43f9b975d14364.tgz#1648660042653301\n",
      "Copying gs://grandelli-demo-295810_cloudbuild/source/1648660042.412306-fc957ae3c7f148d3ba43f9b975d14364.tgz#1648660042653301...\n",
      "/ [1 files][  2.8 KiB/  2.8 KiB]                                                \n",
      "Operation completed over 1 objects/2.8 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  16.38kB\n",
      "Step 1/4 : FROM gcr.io/tfx-oss-public/tfx:1.2.0\n",
      "1.2.0: Pulling from tfx-oss-public/tfx\n",
      "25fa05cd42bd: Pulling fs layer\n",
      "2d6e353a95ec: Pulling fs layer\n",
      "14d7996407de: Pulling fs layer\n",
      "0c9c6fc70f16: Pulling fs layer\n",
      "c3c76be11512: Pulling fs layer\n",
      "ab6e5a9c78ee: Pulling fs layer\n",
      "7bc1690abd59: Pulling fs layer\n",
      "f5b4dd7682bc: Pulling fs layer\n",
      "d6897660f71d: Pulling fs layer\n",
      "174d792fb622: Pulling fs layer\n",
      "5f8143275aca: Pulling fs layer\n",
      "56646f115483: Pulling fs layer\n",
      "798922b52524: Pulling fs layer\n",
      "e2699a9f592b: Pulling fs layer\n",
      "f43e7d1c07e4: Pulling fs layer\n",
      "1e71d5e9923d: Pulling fs layer\n",
      "bf6ae2a2e250: Pulling fs layer\n",
      "e49679b748d5: Pulling fs layer\n",
      "80208bd6f7fb: Pulling fs layer\n",
      "b83c16bef138: Pulling fs layer\n",
      "9d1427033824: Pulling fs layer\n",
      "c0028679f003: Pulling fs layer\n",
      "09c222e7ff04: Pulling fs layer\n",
      "ae6048a3aec1: Pulling fs layer\n",
      "1ced637de50b: Pulling fs layer\n",
      "762ff1eb7f16: Pulling fs layer\n",
      "f6f8f4265c8c: Pulling fs layer\n",
      "d6897660f71d: Waiting\n",
      "174d792fb622: Waiting\n",
      "595b1c49222a: Pulling fs layer\n",
      "b6c7eb38f366: Pulling fs layer\n",
      "5f8143275aca: Waiting\n",
      "520be5017b4d: Pulling fs layer\n",
      "0a1aacb7e387: Pulling fs layer\n",
      "56646f115483: Waiting\n",
      "8f605134caf9: Pulling fs layer\n",
      "189ba322d030: Pulling fs layer\n",
      "798922b52524: Waiting\n",
      "af92447b9198: Pulling fs layer\n",
      "e2699a9f592b: Waiting\n",
      "f43e7d1c07e4: Waiting\n",
      "1e71d5e9923d: Waiting\n",
      "bf6ae2a2e250: Waiting\n",
      "e49679b748d5: Waiting\n",
      "0c9c6fc70f16: Waiting\n",
      "80208bd6f7fb: Waiting\n",
      "c3c76be11512: Waiting\n",
      "b83c16bef138: Waiting\n",
      "9d1427033824: Waiting\n",
      "7bc1690abd59: Waiting\n",
      "c0028679f003: Waiting\n",
      "1ced637de50b: Waiting\n",
      "762ff1eb7f16: Waiting\n",
      "b6c7eb38f366: Waiting\n",
      "f5b4dd7682bc: Waiting\n",
      "f6f8f4265c8c: Waiting\n",
      "520be5017b4d: Waiting\n",
      "0a1aacb7e387: Waiting\n",
      "09c222e7ff04: Waiting\n",
      "8f605134caf9: Waiting\n",
      "189ba322d030: Waiting\n",
      "ae6048a3aec1: Waiting\n",
      "2d6e353a95ec: Verifying Checksum\n",
      "2d6e353a95ec: Download complete\n",
      "14d7996407de: Verifying Checksum\n",
      "14d7996407de: Download complete\n",
      "25fa05cd42bd: Verifying Checksum\n",
      "25fa05cd42bd: Download complete\n",
      "0c9c6fc70f16: Verifying Checksum\n",
      "0c9c6fc70f16: Download complete\n",
      "7bc1690abd59: Download complete\n",
      "c3c76be11512: Verifying Checksum\n",
      "c3c76be11512: Download complete\n",
      "d6897660f71d: Download complete\n",
      "25fa05cd42bd: Pull complete\n",
      "2d6e353a95ec: Pull complete\n",
      "14d7996407de: Pull complete\n",
      "0c9c6fc70f16: Pull complete\n",
      "c3c76be11512: Pull complete\n",
      "174d792fb622: Verifying Checksum\n",
      "174d792fb622: Download complete\n",
      "5f8143275aca: Verifying Checksum\n",
      "5f8143275aca: Download complete\n",
      "f5b4dd7682bc: Download complete\n",
      "56646f115483: Verifying Checksum\n",
      "56646f115483: Download complete\n",
      "798922b52524: Verifying Checksum\n",
      "798922b52524: Download complete\n",
      "f43e7d1c07e4: Verifying Checksum\n",
      "f43e7d1c07e4: Download complete\n",
      "e2699a9f592b: Download complete\n",
      "bf6ae2a2e250: Verifying Checksum\n",
      "bf6ae2a2e250: Download complete\n",
      "e49679b748d5: Verifying Checksum\n",
      "e49679b748d5: Download complete\n",
      "80208bd6f7fb: Download complete\n",
      "b83c16bef138: Verifying Checksum\n",
      "b83c16bef138: Download complete\n",
      "9d1427033824: Verifying Checksum\n",
      "9d1427033824: Download complete\n",
      "c0028679f003: Verifying Checksum\n",
      "c0028679f003: Download complete\n",
      "09c222e7ff04: Verifying Checksum\n",
      "09c222e7ff04: Download complete\n",
      "ae6048a3aec1: Verifying Checksum\n",
      "ae6048a3aec1: Download complete\n",
      "1e71d5e9923d: Verifying Checksum\n",
      "1e71d5e9923d: Download complete\n",
      "ab6e5a9c78ee: Verifying Checksum\n",
      "ab6e5a9c78ee: Download complete\n",
      "1ced637de50b: Verifying Checksum\n",
      "1ced637de50b: Download complete\n",
      "f6f8f4265c8c: Verifying Checksum\n",
      "f6f8f4265c8c: Download complete\n",
      "b6c7eb38f366: Verifying Checksum\n",
      "b6c7eb38f366: Download complete\n",
      "520be5017b4d: Verifying Checksum\n",
      "520be5017b4d: Download complete\n",
      "0a1aacb7e387: Verifying Checksum\n",
      "0a1aacb7e387: Download complete\n",
      "8f605134caf9: Verifying Checksum\n",
      "8f605134caf9: Download complete\n",
      "189ba322d030: Verifying Checksum\n",
      "189ba322d030: Download complete\n",
      "af92447b9198: Download complete\n",
      "595b1c49222a: Verifying Checksum\n",
      "595b1c49222a: Download complete\n",
      "762ff1eb7f16: Verifying Checksum\n",
      "762ff1eb7f16: Download complete\n",
      "ab6e5a9c78ee: Pull complete\n",
      "7bc1690abd59: Pull complete\n",
      "f5b4dd7682bc: Pull complete\n",
      "d6897660f71d: Pull complete\n",
      "174d792fb622: Pull complete\n",
      "5f8143275aca: Pull complete\n",
      "56646f115483: Pull complete\n",
      "798922b52524: Pull complete\n",
      "e2699a9f592b: Pull complete\n",
      "f43e7d1c07e4: Pull complete\n",
      "1e71d5e9923d: Pull complete\n",
      "bf6ae2a2e250: Pull complete\n",
      "e49679b748d5: Pull complete\n",
      "80208bd6f7fb: Pull complete\n",
      "b83c16bef138: Pull complete\n",
      "9d1427033824: Pull complete\n",
      "c0028679f003: Pull complete\n",
      "09c222e7ff04: Pull complete\n",
      "ae6048a3aec1: Pull complete\n",
      "1ced637de50b: Pull complete\n",
      "762ff1eb7f16: Pull complete\n",
      "f6f8f4265c8c: Pull complete\n",
      "595b1c49222a: Pull complete\n",
      "b6c7eb38f366: Pull complete\n",
      "520be5017b4d: Pull complete\n",
      "0a1aacb7e387: Pull complete\n",
      "8f605134caf9: Pull complete\n",
      "189ba322d030: Pull complete\n",
      "af92447b9198: Pull complete\n",
      "Digest: sha256:eba9e7b7d9131eb5b05434feaafc4676268ad805e4b97218f58994ad2714be67\n",
      "Status: Downloaded newer image for gcr.io/tfx-oss-public/tfx:1.2.0\n",
      " ---> 0e86fadcc60c\n",
      "Step 2/4 : RUN pip install -U pip\n",
      " ---> Running in ef293a17bf8b\n",
      "Requirement already satisfied: pip in /opt/conda/lib/python3.7/site-packages (21.2.4)\n",
      "Collecting pip\n",
      "  Downloading pip-22.0.4-py3-none-any.whl (2.1 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 21.2.4\n",
      "    Uninstalling pip-21.2.4:\n",
      "      Successfully uninstalled pip-21.2.4\n",
      "Successfully installed pip-22.0.4\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "Removing intermediate container ef293a17bf8b\n",
      " ---> c077894028e5\n",
      "Step 3/4 : RUN pip install google-cloud-aiplatform==1.4.2 google-cloud-aiplatform[tensorboard]\n",
      " ---> Running in b3808f7044f4\n",
      "Collecting google-cloud-aiplatform==1.4.2\n",
      "  Downloading google_cloud_aiplatform-1.4.2-py2.py3-none-any.whl (1.4 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/1.4 MB 35.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: google-cloud-aiplatform[tensorboard] in /opt/conda/lib/python3.7/site-packages (0.7.1)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.4.2) (20.9)\n",
      "Requirement already satisfied: google-cloud-storage<2.0.0dev,>=1.32.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.4.2) (1.41.1)\n",
      "Requirement already satisfied: google-api-core[grpc]<3.0.0dev,>=1.26.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.4.2) (1.31.1)\n",
      "Requirement already satisfied: proto-plus>=1.10.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.4.2) (1.19.0)\n",
      "Requirement already satisfied: google-cloud-bigquery<3.0.0dev,>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.4.2) (2.20.0)\n",
      "WARNING: google-cloud-aiplatform 0.7.1 does not provide the extra 'tensorboard'\n",
      "Collecting google-cloud-aiplatform[tensorboard]\n",
      "  Downloading google_cloud_aiplatform-1.11.0-py2.py3-none-any.whl (1.8 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.8/1.8 MB 55.3 MB/s eta 0:00:00\n",
      "  Downloading google_cloud_aiplatform-1.10.0-py2.py3-none-any.whl (1.7 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 65.5 MB/s eta 0:00:00\n",
      "  Downloading google_cloud_aiplatform-1.9.0-py2.py3-none-any.whl (1.7 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 70.2 MB/s eta 0:00:00\n",
      "  Downloading google_cloud_aiplatform-1.8.1-py2.py3-none-any.whl (1.7 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 71.7 MB/s eta 0:00:00\n",
      "  Downloading google_cloud_aiplatform-1.8.0-py2.py3-none-any.whl (1.7 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 73.6 MB/s eta 0:00:00\n",
      "  Downloading google_cloud_aiplatform-1.7.1-py2.py3-none-any.whl (1.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 73.5 MB/s eta 0:00:00\n",
      "  Downloading google_cloud_aiplatform-1.7.0-py2.py3-none-any.whl (1.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 77.5 MB/s eta 0:00:00\n",
      "  Downloading google_cloud_aiplatform-1.6.2-py2.py3-none-any.whl (1.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 77.0 MB/s eta 0:00:00\n",
      "  Downloading google_cloud_aiplatform-1.6.1-py2.py3-none-any.whl (1.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 78.6 MB/s eta 0:00:00\n",
      "  Downloading google_cloud_aiplatform-1.6.0-py2.py3-none-any.whl (1.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 77.7 MB/s eta 0:00:00\n",
      "  Downloading google_cloud_aiplatform-1.5.0-py2.py3-none-any.whl (1.4 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/1.4 MB 75.9 MB/s eta 0:00:00\n",
      "  Downloading google_cloud_aiplatform-1.4.3-py2.py3-none-any.whl (1.4 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/1.4 MB 71.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tensorflow<=2.5.0,>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.4.2) (2.5.0)\n",
      "Requirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.4.2) (1.15.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.4.2) (49.6.0.post20210108)\n",
      "Requirement already satisfied: google-auth<2.0dev,>=1.25.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.4.2) (1.34.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.4.2) (2021.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.4.2) (1.53.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.4.2) (2.25.1)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.4.2) (3.16.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.29.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.4.2) (1.34.1)\n",
      "Requirement already satisfied: google-resumable-media<2.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.4.2) (1.3.2)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.4.2) (1.7.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=14.3->google-cloud-aiplatform==1.4.2) (2.4.7)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<=2.5.0,>=2.3.0->google-cloud-aiplatform==1.4.2) (3.3.0)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<=2.5.0,>=2.3.0->google-cloud-aiplatform==1.4.2) (1.19.5)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /opt/conda/lib/python3.7/site-packages (from tensorflow<=2.5.0,>=2.3.0->google-cloud-aiplatform==1.4.2) (3.7.4.3)\n",
      "Requirement already satisfied: absl-py~=0.10 in /opt/conda/lib/python3.7/site-packages (from tensorflow<=2.5.0,>=2.3.0->google-cloud-aiplatform==1.4.2) (0.12.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow<=2.5.0,>=2.3.0->google-cloud-aiplatform==1.4.2) (0.36.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<=2.5.0,>=2.3.0->google-cloud-aiplatform==1.4.2) (2.5.0)\n",
      "Requirement already satisfied: keras-nightly~=2.5.0.dev in /opt/conda/lib/python3.7/site-packages (from tensorflow<=2.5.0,>=2.3.0->google-cloud-aiplatform==1.4.2) (2.5.0.dev2021032900)\n",
      "Requirement already satisfied: gast==0.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<=2.5.0,>=2.3.0->google-cloud-aiplatform==1.4.2) (0.4.0)\n",
      "Requirement already satisfied: tensorboard~=2.5 in /opt/conda/lib/python3.7/site-packages (from tensorflow<=2.5.0,>=2.3.0->google-cloud-aiplatform==1.4.2) (2.5.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<=2.5.0,>=2.3.0->google-cloud-aiplatform==1.4.2) (1.12.1)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<=2.5.0,>=2.3.0->google-cloud-aiplatform==1.4.2) (1.1.2)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow<=2.5.0,>=2.3.0->google-cloud-aiplatform==1.4.2) (1.6.3)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<=2.5.0,>=2.3.0->google-cloud-aiplatform==1.4.2) (0.2.0)\n",
      "Requirement already satisfied: h5py~=3.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<=2.5.0,>=2.3.0->google-cloud-aiplatform==1.4.2) (3.1.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<=2.5.0,>=2.3.0->google-cloud-aiplatform==1.4.2) (1.1.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<=2.5.0,>=2.3.0->google-cloud-aiplatform==1.4.2) (1.12)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.4.2) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.4.2) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.4.2) (0.2.7)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.4.2) (1.1.2)\n",
      "Requirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py~=3.1.0->tensorflow<=2.5.0,>=2.3.0->google-cloud-aiplatform==1.4.2) (1.5.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.4.2) (1.26.6)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.4.2) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.4.2) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.4.2) (2.10)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow<=2.5.0,>=2.3.0->google-cloud-aiplatform==1.4.2) (1.8.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow<=2.5.0,>=2.3.0->google-cloud-aiplatform==1.4.2) (0.4.5)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow<=2.5.0,>=2.3.0->google-cloud-aiplatform==1.4.2) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow<=2.5.0,>=2.3.0->google-cloud-aiplatform==1.4.2) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow<=2.5.0,>=2.3.0->google-cloud-aiplatform==1.4.2) (2.0.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow<=2.5.0,>=2.3.0->google-cloud-aiplatform==1.4.2) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.4.2) (1.14.6)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow<=2.5.0,>=2.3.0->google-cloud-aiplatform==1.4.2) (4.6.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.4.2) (0.4.8)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.4.2) (2.20)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow<=2.5.0,>=2.3.0->google-cloud-aiplatform==1.4.2) (3.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.5->tensorflow<=2.5.0,>=2.3.0->google-cloud-aiplatform==1.4.2) (3.5.0)\n",
      "Installing collected packages: google-cloud-aiplatform\n",
      "  Attempting uninstall: google-cloud-aiplatform\n",
      "    Found existing installation: google-cloud-aiplatform 0.7.1\n",
      "    Uninstalling google-cloud-aiplatform-0.7.1:\n",
      "      Successfully uninstalled google-cloud-aiplatform-0.7.1\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tfx 1.2.0 requires google-cloud-aiplatform<0.8,>=0.5.0, but you have google-cloud-aiplatform 1.4.2 which is incompatible.\n",
      "Successfully installed google-cloud-aiplatform-1.4.2\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "Removing intermediate container b3808f7044f4\n",
      " ---> b20cbed87e37\n",
      "Step 4/4 : RUN pip install pytest kfp==1.8.1 google-cloud-bigquery==2.26.0 google-cloud-bigquery-storage==2.7.0 google-cloud-aiplatform==1.4.2\n",
      " ---> Running in dbec00509247\n",
      "Collecting pytest\n",
      "  Downloading pytest-7.1.1-py3-none-any.whl (297 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 297.0/297.0 KB 20.9 MB/s eta 0:00:00\n",
      "Collecting kfp==1.8.1\n",
      "  Downloading kfp-1.8.1.tar.gz (248 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 248.5/248.5 KB 31.1 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting google-cloud-bigquery==2.26.0\n",
      "  Downloading google_cloud_bigquery-2.26.0-py2.py3-none-any.whl (201 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 201.2/201.2 KB 30.1 MB/s eta 0:00:00\n",
      "Collecting google-cloud-bigquery-storage==2.7.0\n",
      "  Downloading google_cloud_bigquery_storage-2.7.0-py2.py3-none-any.whl (125 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 125.3/125.3 KB 20.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: google-cloud-aiplatform==1.4.2 in /opt/conda/lib/python3.7/site-packages (1.4.2)\n",
      "Collecting absl-py<=0.11,>=0.9\n",
      "  Downloading absl_py-0.11.0-py3-none-any.whl (127 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 127.8/127.8 KB 21.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: PyYAML<6,>=5.3 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.1) (5.4.1)\n",
      "Requirement already satisfied: google-cloud-storage<2,>=1.20.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.1) (1.41.1)\n",
      "Requirement already satisfied: kubernetes<19,>=8.0.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.1) (12.0.1)\n",
      "Requirement already satisfied: google-api-python-client<2,>=1.7.8 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.1) (1.12.8)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.1) (1.34.0)\n",
      "Collecting requests-toolbelt<1,>=0.8.0\n",
      "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.3/54.3 KB 9.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: cloudpickle<2,>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.1) (1.6.0)\n",
      "Collecting kfp-server-api<2.0.0,>=1.1.2\n",
      "  Downloading kfp-server-api-1.8.1.tar.gz (54 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.2/54.2 KB 10.0 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: jsonschema<4,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.1) (3.2.0)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.1) (0.8.9)\n",
      "Requirement already satisfied: click<8,>=7.1.1 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.1) (7.1.2)\n",
      "Collecting Deprecated<2,>=1.2.7\n",
      "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting strip-hints<1,>=0.1.8\n",
      "  Downloading strip-hints-0.1.10.tar.gz (29 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting docstring-parser<1,>=0.7.3\n",
      "  Downloading docstring_parser-0.13.tar.gz (23 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting kfp-pipeline-spec<0.2.0,>=0.1.10\n",
      "  Downloading kfp_pipeline_spec-0.1.14-py3-none-any.whl (18 kB)\n",
      "Collecting fire<1,>=0.3.1\n",
      "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 87.7/87.7 KB 16.5 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: protobuf<4,>=3.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.1) (3.16.0)\n",
      "Requirement already satisfied: pydantic<2,>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.1) (1.8.2)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.26.0) (1.7.2)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.26.0) (2.25.1)\n",
      "Collecting grpcio<2.0dev,>=1.38.1\n",
      "  Downloading grpcio-1.44.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.3/4.3 MB 69.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.26.0) (20.9)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.26.0) (1.3.2)\n",
      "Requirement already satisfied: proto-plus>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.26.0) (1.19.0)\n",
      "Requirement already satisfied: google-api-core[grpc]<3.0.0dev,>=1.29.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.26.0) (1.31.1)\n",
      "Requirement already satisfied: libcst>=0.2.5 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery-storage==2.7.0) (0.3.19)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from pytest) (1.1.0)\n",
      "Collecting iniconfig\n",
      "  Downloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)\n",
      "Requirement already satisfied: importlib-metadata>=0.12 in /opt/conda/lib/python3.7/site-packages (from pytest) (4.6.3)\n",
      "Collecting pluggy<2.0,>=0.12\n",
      "  Downloading pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.7/site-packages (from pytest) (20.3.0)\n",
      "Collecting py>=1.8.2\n",
      "  Downloading py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.7/98.7 KB 16.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from absl-py<=0.11,>=0.9->kfp==1.8.1) (1.15.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated<2,>=1.2.7->kfp==1.8.1) (1.12.1)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.7/site-packages (from fire<1,>=0.3.1->kfp==1.8.1) (1.1.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.29.0->google-cloud-bigquery==2.26.0) (49.6.0.post20210108)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.29.0->google-cloud-bigquery==2.26.0) (2021.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.29.0->google-cloud-bigquery==2.26.0) (1.53.0)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp==1.8.1) (3.0.1)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp==1.8.1) (0.1.0)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp==1.8.1) (0.19.1)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.8.1) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.8.1) (0.2.7)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.8.1) (4.7.2)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery==2.26.0) (1.1.2)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.12->pytest) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.12->pytest) (3.5.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp==1.8.1) (0.17.3)\n",
      "Requirement already satisfied: urllib3>=1.15 in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.8.1) (1.26.6)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.8.1) (2021.5.30)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.8.1) (2.8.2)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<19,>=8.0.0->kfp==1.8.1) (0.57.0)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<19,>=8.0.0->kfp==1.8.1) (1.3.0)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from libcst>=0.2.5->google-cloud-bigquery-storage==2.7.0) (0.7.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=14.3->google-cloud-bigquery==2.26.0) (2.4.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery==2.26.0) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery==2.26.0) (2.10)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints<1,>=0.1.8->kfp==1.8.1) (0.36.2)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery==2.26.0) (1.14.6)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.1->kfp==1.8.1) (0.4.8)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from typing-inspect>=0.4.0->libcst>=0.2.5->google-cloud-bigquery-storage==2.7.0) (0.4.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<19,>=8.0.0->kfp==1.8.1) (3.1.1)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery==2.26.0) (2.20)\n",
      "Building wheels for collected packages: kfp, docstring-parser, fire, kfp-server-api, strip-hints\n",
      "  Building wheel for kfp (setup.py): started\n",
      "  Building wheel for kfp (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp: filename=kfp-1.8.1-py3-none-any.whl size=345340 sha256=8fd84f486e385448dbb94870eb19da99766d4b2ba108513e71340ff5c4dade85\n",
      "  Stored in directory: /root/.cache/pip/wheels/02/dd/71/45fa445fd9ea0c60ab0bd00b31760b1102ef2999f8002ee892\n",
      "  Building wheel for docstring-parser (pyproject.toml): started\n",
      "  Building wheel for docstring-parser (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for docstring-parser: filename=docstring_parser-0.13-py3-none-any.whl size=31866 sha256=a20836c494f593eeadb45507a4937e23e20d5cc20cb0103780a23becb73e4437\n",
      "  Stored in directory: /root/.cache/pip/wheels/bd/88/3c/d1aa049309f7945178cac9fbe6561a86424f432da57c18ca0f\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115928 sha256=6d54413874dfd586ce65acef8cbc7b1b956b9ee26652eb603d0611b8f244a93a\n",
      "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
      "  Building wheel for kfp-server-api (setup.py): started\n",
      "  Building wheel for kfp-server-api (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp-server-api: filename=kfp_server_api-1.8.1-py3-none-any.whl size=95548 sha256=369466386d72a5486cabb5b91a25e444f65a8e61f7270ca63a83dd30ebb4f0fe\n",
      "  Stored in directory: /root/.cache/pip/wheels/f5/4e/2e/6795bd3ed456a43652e7de100aca275ec179c9a8dfbcc65626\n",
      "  Building wheel for strip-hints (setup.py): started\n",
      "  Building wheel for strip-hints (setup.py): finished with status 'done'\n",
      "  Created wheel for strip-hints: filename=strip_hints-0.1.10-py2.py3-none-any.whl size=22279 sha256=022ef463d4a3678c90679d9d0f5cebf0f9c4b2baa4a2e0e9a7b175920df40336\n",
      "  Stored in directory: /root/.cache/pip/wheels/5e/14/c3/6e44e9b2545f2d570b03f5b6d38c00b7534aa8abb376978363\n",
      "Successfully built kfp docstring-parser fire kfp-server-api strip-hints\n",
      "Installing collected packages: iniconfig, strip-hints, py, grpcio, fire, docstring-parser, Deprecated, absl-py, requests-toolbelt, pluggy, kfp-server-api, kfp-pipeline-spec, pytest, google-cloud-bigquery-storage, google-cloud-bigquery, kfp\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.34.1\n",
      "    Uninstalling grpcio-1.34.1:\n",
      "      Successfully uninstalled grpcio-1.34.1\n",
      "  Attempting uninstall: absl-py\n",
      "    Found existing installation: absl-py 0.12.0\n",
      "    Uninstalling absl-py-0.12.0:\n",
      "      Successfully uninstalled absl-py-0.12.0\n",
      "  Attempting uninstall: kfp-pipeline-spec\n",
      "    Found existing installation: kfp-pipeline-spec 0.1.9\n",
      "    Uninstalling kfp-pipeline-spec-0.1.9:\n",
      "      Successfully uninstalled kfp-pipeline-spec-0.1.9\n",
      "  Attempting uninstall: google-cloud-bigquery-storage\n",
      "    Found existing installation: google-cloud-bigquery-storage 2.6.2\n",
      "    Uninstalling google-cloud-bigquery-storage-2.6.2:\n",
      "      Successfully uninstalled google-cloud-bigquery-storage-2.6.2\n",
      "  Attempting uninstall: google-cloud-bigquery\n",
      "    Found existing installation: google-cloud-bigquery 2.20.0\n",
      "    Uninstalling google-cloud-bigquery-2.20.0:\n",
      "      Successfully uninstalled google-cloud-bigquery-2.20.0\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-io 0.18.0 requires tensorflow-io-gcs-filesystem==0.18.0, which is not installed.\n",
      "tfx 1.2.0 requires google-cloud-aiplatform<0.8,>=0.5.0, but you have google-cloud-aiplatform 1.4.2 which is incompatible.\n",
      "tfx 1.2.0 requires google-cloud-bigquery<2.21,>=1.28.0, but you have google-cloud-bigquery 2.26.0 which is incompatible.\n",
      "tfx-bsl 1.2.0 requires google-cloud-bigquery<2.21,>=1.28.0, but you have google-cloud-bigquery 2.26.0 which is incompatible.\n",
      "tensorflow 2.5.0 requires grpcio~=1.34.0, but you have grpcio 1.44.0 which is incompatible.\n",
      "tensorflow-transform 1.2.0 requires google-cloud-bigquery<2.21,>=1.28.0, but you have google-cloud-bigquery 2.26.0 which is incompatible.\n",
      "tensorflow-model-analysis 0.33.0 requires google-cloud-bigquery<2.21,>=1.28.0, but you have google-cloud-bigquery 2.26.0 which is incompatible.\n",
      "tensorflow-data-validation 1.2.0 requires google-cloud-bigquery<2.21,>=1.28.0, but you have google-cloud-bigquery 2.26.0 which is incompatible.\n",
      "Successfully installed Deprecated-1.2.13 absl-py-0.11.0 docstring-parser-0.13 fire-0.4.0 google-cloud-bigquery-2.26.0 google-cloud-bigquery-storage-2.7.0 grpcio-1.44.0 iniconfig-1.1.1 kfp-1.8.1 kfp-pipeline-spec-0.1.14 kfp-server-api-1.8.1 pluggy-1.0.0 py-1.11.0 pytest-7.1.1 requests-toolbelt-0.9.1 strip-hints-0.1.10\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "Removing intermediate container dbec00509247\n",
      " ---> c266b000472f\n",
      "Successfully built c266b000472f\n",
      "Successfully tagged gcr.io/grandelli-demo-295810/cicd:latest\n",
      "PUSH\n",
      "Pushing gcr.io/grandelli-demo-295810/cicd:latest\n",
      "The push refers to repository [gcr.io/grandelli-demo-295810/cicd]\n",
      "355bf5286283: Preparing\n",
      "ad2cdece3373: Preparing\n",
      "a2639eb8663d: Preparing\n",
      "d42c05f73feb: Preparing\n",
      "3119d30f29a9: Preparing\n",
      "121c9dfc7bce: Preparing\n",
      "868786b3710b: Preparing\n",
      "5bb1aa5df10d: Preparing\n",
      "f028010939aa: Preparing\n",
      "dc99c4ea3a81: Preparing\n",
      "37b508c5711b: Preparing\n",
      "756ab564e194: Preparing\n",
      "2ae86808a3d1: Preparing\n",
      "1dccbdf9b557: Preparing\n",
      "cfcbdbc2b748: Preparing\n",
      "937ab8f29c2e: Preparing\n",
      "5d417b2f7486: Preparing\n",
      "d6a297a3e6e4: Preparing\n",
      "6474a5e8117f: Preparing\n",
      "fe498124ed57: Preparing\n",
      "d5454704bb3d: Preparing\n",
      "fb896ef24b4b: Preparing\n",
      "5087113f67c8: Preparing\n",
      "2a92857a1d48: Preparing\n",
      "0ded97864c52: Preparing\n",
      "b50bbaac3e32: Preparing\n",
      "262ea1af4c10: Preparing\n",
      "b420a468ca49: Preparing\n",
      "608c205798d1: Preparing\n",
      "0760cd6d4269: Preparing\n",
      "fb4755c89c2a: Preparing\n",
      "22cfb9034da6: Preparing\n",
      "8bec4fbfce85: Preparing\n",
      "3b129ca3db46: Preparing\n",
      "64cb1a1930ab: Preparing\n",
      "600ef5a43f1f: Preparing\n",
      "8f8f0266f834: Preparing\n",
      "756ab564e194: Waiting\n",
      "2ae86808a3d1: Waiting\n",
      "1dccbdf9b557: Waiting\n",
      "cfcbdbc2b748: Waiting\n",
      "937ab8f29c2e: Waiting\n",
      "5d417b2f7486: Waiting\n",
      "d6a297a3e6e4: Waiting\n",
      "6474a5e8117f: Waiting\n",
      "fe498124ed57: Waiting\n",
      "0ded97864c52: Waiting\n",
      "b50bbaac3e32: Waiting\n",
      "d5454704bb3d: Waiting\n",
      "fb896ef24b4b: Waiting\n",
      "262ea1af4c10: Waiting\n",
      "5087113f67c8: Waiting\n",
      "b420a468ca49: Waiting\n",
      "2a92857a1d48: Waiting\n",
      "608c205798d1: Waiting\n",
      "0760cd6d4269: Waiting\n",
      "f028010939aa: Waiting\n",
      "fb4755c89c2a: Waiting\n",
      "5bb1aa5df10d: Waiting\n",
      "22cfb9034da6: Waiting\n",
      "121c9dfc7bce: Waiting\n",
      "8bec4fbfce85: Waiting\n",
      "868786b3710b: Waiting\n",
      "dc99c4ea3a81: Waiting\n",
      "8f8f0266f834: Waiting\n",
      "3b129ca3db46: Waiting\n",
      "64cb1a1930ab: Waiting\n",
      "37b508c5711b: Waiting\n",
      "d42c05f73feb: Layer already exists\n",
      "3119d30f29a9: Layer already exists\n",
      "121c9dfc7bce: Layer already exists\n",
      "868786b3710b: Layer already exists\n",
      "f028010939aa: Layer already exists\n",
      "5bb1aa5df10d: Layer already exists\n",
      "37b508c5711b: Layer already exists\n",
      "dc99c4ea3a81: Layer already exists\n",
      "756ab564e194: Layer already exists\n",
      "2ae86808a3d1: Layer already exists\n",
      "1dccbdf9b557: Layer already exists\n",
      "cfcbdbc2b748: Layer already exists\n",
      "937ab8f29c2e: Layer already exists\n",
      "5d417b2f7486: Layer already exists\n",
      "d6a297a3e6e4: Layer already exists\n",
      "6474a5e8117f: Layer already exists\n",
      "fe498124ed57: Layer already exists\n",
      "d5454704bb3d: Layer already exists\n",
      "5087113f67c8: Layer already exists\n",
      "fb896ef24b4b: Layer already exists\n",
      "2a92857a1d48: Layer already exists\n",
      "0ded97864c52: Layer already exists\n",
      "b50bbaac3e32: Layer already exists\n",
      "262ea1af4c10: Layer already exists\n",
      "b420a468ca49: Layer already exists\n",
      "608c205798d1: Layer already exists\n",
      "0760cd6d4269: Layer already exists\n",
      "fb4755c89c2a: Layer already exists\n",
      "22cfb9034da6: Layer already exists\n",
      "3b129ca3db46: Layer already exists\n",
      "8bec4fbfce85: Layer already exists\n",
      "600ef5a43f1f: Layer already exists\n",
      "64cb1a1930ab: Layer already exists\n",
      "8f8f0266f834: Layer already exists\n",
      "a2639eb8663d: Pushed\n",
      "ad2cdece3373: Pushed\n",
      "355bf5286283: Pushed\n",
      "latest: digest: sha256:ae350d9b4e23b2db1d47425b6d21faad1d03105753226ecbd33d654137facaf2 size: 8109\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                               IMAGES                                       STATUS\n",
      "f128fce0-bcfb-4d87-ae0b-eb72bb4d00cb  2022-03-30T17:07:22+00:00  3M30S     gs://grandelli-demo-295810_cloudbuild/source/1648660042.412306-fc957ae3c7f148d3ba43f9b975d14364.tgz  gcr.io/grandelli-demo-295810/cicd (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --tag $CICD_IMAGE_URI build/. --timeout=15m --machine-type=e2-highcpu-8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc9b2af",
   "metadata": {},
   "source": [
    "### Run CI/CD from pipeline deployment using Cloud Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00b55593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_REPO_URL=https://github.com/GoogleCloudPlatform/mlops-with-vertex-ai.git,_BRANCH=main,_CICD_IMAGE_URI=gcr.io/grandelli-demo-295810/cicd:latest,_PROJECT=grandelli-demo-295810,_REGION=us-central1,_GCS_LOCATION=gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/,_TEST_GCS_LOCATION=gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests,_BQ_LOCATION=US,_BQ_DATASET_NAME=partner_training,_BQ_TABLE_NAME=chicago_taxitrips_prep,_DATASET_DISPLAY_NAME=chicago-taxi-tips,_MODEL_DISPLAY_NAME=chicago-taxi-tips-classifier-v01,_CI_TRAIN_LIMIT=1000,_CI_TEST_LIMIT=100,_CI_UPLOAD_MODEL=0,_CI_ACCURACY_THRESHOLD=0.1,_BEAM_RUNNER=DataflowRunner,_TRAINING_RUNNER=vertex,_TFX_IMAGE_URI=gcr.io/grandelli-demo-295810/chicago-taxi-tips:tfx-1.2,_PIPELINE_NAME=chicago-taxi-tips-classifier-v01-train-pipeline,_PIPELINES_STORE=gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/compiled_pipelines\n"
     ]
    }
   ],
   "source": [
    "REPO_URL = \"https://github.com/GoogleCloudPlatform/mlops-with-vertex-ai.git\" # Change to your github repo.\n",
    "BRANCH = \"main\"\n",
    "\n",
    "GCS_LOCATION = f\"gs://{BUCKET}/{DATASET_DISPLAY_NAME}/\"\n",
    "TEST_GCS_LOCATION = f\"gs://{BUCKET}/{DATASET_DISPLAY_NAME}/e2e_tests\"\n",
    "CI_TRAIN_LIMIT = 1000\n",
    "CI_TEST_LIMIT = 100\n",
    "CI_UPLOAD_MODEL = 0\n",
    "CI_ACCURACY_THRESHOLD = 0.1\n",
    "BEAM_RUNNER = \"DataflowRunner\"\n",
    "TRAINING_RUNNER = \"vertex\"\n",
    "VERSION = 'tfx-1.2'\n",
    "PIPELINE_NAME = f'{MODEL_DISPLAY_NAME}-train-pipeline'\n",
    "PIPELINES_STORE = os.path.join(GCS_LOCATION, \"compiled_pipelines\")\n",
    "\n",
    "TFX_IMAGE_URI = f\"gcr.io/{PROJECT}/{DATASET_DISPLAY_NAME}:{VERSION}\"\n",
    "\n",
    "SUBSTITUTIONS=f\"\"\"\\\n",
    "_REPO_URL='{REPO_URL}',\\\n",
    "_BRANCH={BRANCH},\\\n",
    "_CICD_IMAGE_URI={CICD_IMAGE_URI},\\\n",
    "_PROJECT={PROJECT},\\\n",
    "_REGION={REGION},\\\n",
    "_GCS_LOCATION={GCS_LOCATION},\\\n",
    "_TEST_GCS_LOCATION={TEST_GCS_LOCATION},\\\n",
    "_BQ_LOCATION={BQ_LOCATION},\\\n",
    "_BQ_DATASET_NAME={BQ_DATASET_NAME},\\\n",
    "_BQ_TABLE_NAME={BQ_TABLE_NAME},\\\n",
    "_DATASET_DISPLAY_NAME={DATASET_DISPLAY_NAME},\\\n",
    "_MODEL_DISPLAY_NAME={MODEL_DISPLAY_NAME},\\\n",
    "_CI_TRAIN_LIMIT={CI_TRAIN_LIMIT},\\\n",
    "_CI_TEST_LIMIT={CI_TEST_LIMIT},\\\n",
    "_CI_UPLOAD_MODEL={CI_UPLOAD_MODEL},\\\n",
    "_CI_ACCURACY_THRESHOLD={CI_ACCURACY_THRESHOLD},\\\n",
    "_BEAM_RUNNER={BEAM_RUNNER},\\\n",
    "_TRAINING_RUNNER={TRAINING_RUNNER},\\\n",
    "_TFX_IMAGE_URI={TFX_IMAGE_URI},\\\n",
    "_PIPELINE_NAME={PIPELINE_NAME},\\\n",
    "_PIPELINES_STORE={PIPELINES_STORE}\\\n",
    "\"\"\"\n",
    "\n",
    "!echo $SUBSTITUTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0bc589ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created [https://cloudbuild.googleapis.com/v1/projects/grandelli-demo-295810/locations/global/builds/13834e3f-3ac3-4096-9e01-cd058a1a30d0].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/13834e3f-3ac3-4096-9e01-cd058a1a30d0?project=155283586619].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"13834e3f-3ac3-4096-9e01-cd058a1a30d0\"\n",
      "\n",
      "FETCHSOURCE\n",
      "BUILD\n",
      "Starting Step #0 - \"Clone Repository\"\n",
      "Step #0 - \"Clone Repository\": Already have image (with digest): gcr.io/cloud-builders/git\n",
      "Step #0 - \"Clone Repository\": Cloning into 'mlops-with-vertex-ai'...\n",
      "Step #0 - \"Clone Repository\": POST git-upload-pack (352 bytes)\n",
      "Step #0 - \"Clone Repository\": POST git-upload-pack (194 bytes)\n",
      "Finished Step #0 - \"Clone Repository\"\n",
      "Starting Step #1 - \"Unit Test Datasource Utils\"\n",
      "Starting Step #2 - \"Unit Test Model\"\n",
      "Step #1 - \"Unit Test Datasource Utils\": Pulling image: gcr.io/grandelli-demo-295810/cicd:latest\n",
      "Step #2 - \"Unit Test Model\": Pulling image: gcr.io/grandelli-demo-295810/cicd:latest\n",
      "Step #2 - \"Unit Test Model\": latest: Pulling from grandelli-demo-295810/cicd\n",
      "Step #2 - \"Unit Test Model\": 25fa05cd42bd: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": 2d6e353a95ec: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": 14d7996407de: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": 0c9c6fc70f16: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": c3c76be11512: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": ab6e5a9c78ee: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": 7bc1690abd59: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": f5b4dd7682bc: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": d6897660f71d: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": 174d792fb622: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": 5f8143275aca: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": 56646f115483: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": 798922b52524: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": c3c76be11512: Waiting\n",
      "Step #2 - \"Unit Test Model\": e2699a9f592b: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": ab6e5a9c78ee: Waiting\n",
      "Step #2 - \"Unit Test Model\": 174d792fb622: Waiting\n",
      "Step #2 - \"Unit Test Model\": 7bc1690abd59: Waiting\n",
      "Step #2 - \"Unit Test Model\": 56646f115483: Waiting\n",
      "Step #2 - \"Unit Test Model\": d6897660f71d: Waiting\n",
      "Step #2 - \"Unit Test Model\": f5b4dd7682bc: Waiting\n",
      "Step #2 - \"Unit Test Model\": 798922b52524: Waiting\n",
      "Step #2 - \"Unit Test Model\": f43e7d1c07e4: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": 1e71d5e9923d: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": bf6ae2a2e250: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": e49679b748d5: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": 80208bd6f7fb: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": b83c16bef138: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": 9d1427033824: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": c0028679f003: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": 09c222e7ff04: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": ae6048a3aec1: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": 1ced637de50b: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": e2699a9f592b: Waiting\n",
      "Step #2 - \"Unit Test Model\": 762ff1eb7f16: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": f6f8f4265c8c: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": f43e7d1c07e4: Waiting\n",
      "Step #2 - \"Unit Test Model\": 595b1c49222a: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": 1e71d5e9923d: Waiting\n",
      "Step #2 - \"Unit Test Model\": b6c7eb38f366: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": 520be5017b4d: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": 0a1aacb7e387: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": 8f605134caf9: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": 189ba322d030: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": bf6ae2a2e250: Waiting\n",
      "Step #2 - \"Unit Test Model\": af92447b9198: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": 7e13e06b6d24: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": 7ca7c630ce44: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": 8ccde080a09e: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": e49679b748d5: Waiting\n",
      "Step #2 - \"Unit Test Model\": 80208bd6f7fb: Waiting\n",
      "Step #2 - \"Unit Test Model\": 9d1427033824: Waiting\n",
      "Step #2 - \"Unit Test Model\": ae6048a3aec1: Waiting\n",
      "Step #2 - \"Unit Test Model\": b83c16bef138: Waiting\n",
      "Step #2 - \"Unit Test Model\": 1ced637de50b: Waiting\n",
      "Step #2 - \"Unit Test Model\": 189ba322d030: Waiting\n",
      "Step #2 - \"Unit Test Model\": 762ff1eb7f16: Waiting\n",
      "Step #2 - \"Unit Test Model\": af92447b9198: Waiting\n",
      "Step #2 - \"Unit Test Model\": f6f8f4265c8c: Waiting\n",
      "Step #2 - \"Unit Test Model\": 8ccde080a09e: Waiting\n",
      "Step #2 - \"Unit Test Model\": 7ca7c630ce44: Waiting\n",
      "Step #2 - \"Unit Test Model\": 8f605134caf9: Waiting\n",
      "Step #2 - \"Unit Test Model\": 0a1aacb7e387: Waiting\n",
      "Step #2 - \"Unit Test Model\": b6c7eb38f366: Waiting\n",
      "Step #2 - \"Unit Test Model\": 520be5017b4d: Waiting\n",
      "Step #2 - \"Unit Test Model\": 7e13e06b6d24: Waiting\n",
      "Step #2 - \"Unit Test Model\": 595b1c49222a: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": latest: Pulling from grandelli-demo-295810/cicd\n",
      "Step #1 - \"Unit Test Datasource Utils\": 25fa05cd42bd: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": 2d6e353a95ec: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": 14d7996407de: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": 0c9c6fc70f16: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": c3c76be11512: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": ab6e5a9c78ee: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": 7bc1690abd59: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": f5b4dd7682bc: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": 0c9c6fc70f16: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": f5b4dd7682bc: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": c3c76be11512: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": ab6e5a9c78ee: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": 7bc1690abd59: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": d6897660f71d: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": 174d792fb622: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": 5f8143275aca: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": 56646f115483: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": 798922b52524: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": e2699a9f592b: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": f43e7d1c07e4: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": 1e71d5e9923d: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": bf6ae2a2e250: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": e49679b748d5: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": bf6ae2a2e250: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": 80208bd6f7fb: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": b83c16bef138: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": 9d1427033824: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": c0028679f003: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": 09c222e7ff04: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": ae6048a3aec1: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": 1ced637de50b: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": 762ff1eb7f16: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": f6f8f4265c8c: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": d6897660f71d: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": 595b1c49222a: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": 174d792fb622: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": b6c7eb38f366: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": e2699a9f592b: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": 798922b52524: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": f43e7d1c07e4: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": 520be5017b4d: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": 0a1aacb7e387: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": 0a1aacb7e387: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": e49679b748d5: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": 5f8143275aca: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": 80208bd6f7fb: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": 56646f115483: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": b83c16bef138: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": 1e71d5e9923d: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": 8f605134caf9: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": 189ba322d030: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": af92447b9198: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": 1ced637de50b: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": 9d1427033824: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": c0028679f003: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": 09c222e7ff04: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": ae6048a3aec1: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": 7e13e06b6d24: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": 595b1c49222a: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": 762ff1eb7f16: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": 7ca7c630ce44: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": f6f8f4265c8c: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": 8ccde080a09e: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": b6c7eb38f366: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": 520be5017b4d: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": 8ccde080a09e: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": 8f605134caf9: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": 189ba322d030: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": af92447b9198: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": 7e13e06b6d24: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": 7ca7c630ce44: Waiting\n",
      "Step #2 - \"Unit Test Model\": 2d6e353a95ec: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": 2d6e353a95ec: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 2d6e353a95ec: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": 2d6e353a95ec: Download complete\n",
      "Step #2 - \"Unit Test Model\": 14d7996407de: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": 14d7996407de: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 14d7996407de: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": 14d7996407de: Download complete\n",
      "Step #2 - \"Unit Test Model\": 25fa05cd42bd: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": 25fa05cd42bd: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": 25fa05cd42bd: Download complete\n",
      "Step #2 - \"Unit Test Model\": 25fa05cd42bd: Download complete\n",
      "Step #2 - \"Unit Test Model\": 0c9c6fc70f16: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 0c9c6fc70f16: Download complete\n",
      "Step #2 - \"Unit Test Model\": c3c76be11512: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": c3c76be11512: Download complete\n",
      "Step #2 - \"Unit Test Model\": 7bc1690abd59: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": 7bc1690abd59: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 7bc1690abd59: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": 7bc1690abd59: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": d6897660f71d: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": d6897660f71d: Download complete\n",
      "Step #2 - \"Unit Test Model\": d6897660f71d: Download complete\n",
      "Step #2 - \"Unit Test Model\": 25fa05cd42bd: Pull complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 25fa05cd42bd: Pull complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 2d6e353a95ec: Pull complete\n",
      "Step #2 - \"Unit Test Model\": 2d6e353a95ec: Pull complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 14d7996407de: Pull complete\n",
      "Step #2 - \"Unit Test Model\": 14d7996407de: Pull complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 0c9c6fc70f16: Pull complete\n",
      "Step #2 - \"Unit Test Model\": 0c9c6fc70f16: Pull complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": c3c76be11512: Pull complete\n",
      "Step #2 - \"Unit Test Model\": c3c76be11512: Pull complete\n",
      "Step #2 - \"Unit Test Model\": 174d792fb622: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": 174d792fb622: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 174d792fb622: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": 174d792fb622: Download complete\n",
      "Step #2 - \"Unit Test Model\": 5f8143275aca: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 5f8143275aca: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": 5f8143275aca: Download complete\n",
      "Step #2 - \"Unit Test Model\": ab6e5a9c78ee: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": ab6e5a9c78ee: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": ab6e5a9c78ee: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": ab6e5a9c78ee: Download complete\n",
      "Step #2 - \"Unit Test Model\": f5b4dd7682bc: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": f5b4dd7682bc: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": f5b4dd7682bc: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": f5b4dd7682bc: Download complete\n",
      "Step #2 - \"Unit Test Model\": 798922b52524: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": 798922b52524: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 798922b52524: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": 798922b52524: Download complete\n",
      "Step #2 - \"Unit Test Model\": f43e7d1c07e4: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": f43e7d1c07e4: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": f43e7d1c07e4: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": f43e7d1c07e4: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": e2699a9f592b: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": e2699a9f592b: Download complete\n",
      "Step #2 - \"Unit Test Model\": e2699a9f592b: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": e2699a9f592b: Download complete\n",
      "Step #2 - \"Unit Test Model\": bf6ae2a2e250: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": bf6ae2a2e250: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": bf6ae2a2e250: Download complete\n",
      "Step #2 - \"Unit Test Model\": bf6ae2a2e250: Download complete\n",
      "Step #2 - \"Unit Test Model\": 56646f115483: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 56646f115483: Download complete\n",
      "Step #2 - \"Unit Test Model\": e49679b748d5: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": e49679b748d5: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": e49679b748d5: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": e49679b748d5: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 80208bd6f7fb: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": 80208bd6f7fb: Download complete\n",
      "Step #2 - \"Unit Test Model\": 80208bd6f7fb: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": 80208bd6f7fb: Download complete\n",
      "Step #2 - \"Unit Test Model\": b83c16bef138: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": b83c16bef138: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": b83c16bef138: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": b83c16bef138: Download complete\n",
      "Step #2 - \"Unit Test Model\": 9d1427033824: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": 9d1427033824: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": 9d1427033824: Download complete\n",
      "Step #2 - \"Unit Test Model\": 9d1427033824: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": c0028679f003: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": c0028679f003: Download complete\n",
      "Step #2 - \"Unit Test Model\": c0028679f003: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": c0028679f003: Download complete\n",
      "Step #2 - \"Unit Test Model\": 09c222e7ff04: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": 09c222e7ff04: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 09c222e7ff04: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": 09c222e7ff04: Download complete\n",
      "Step #2 - \"Unit Test Model\": ae6048a3aec1: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": ae6048a3aec1: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": ae6048a3aec1: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": ae6048a3aec1: Download complete\n",
      "Step #2 - \"Unit Test Model\": 1e71d5e9923d: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": 1e71d5e9923d: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 1e71d5e9923d: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": f6f8f4265c8c: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": f6f8f4265c8c: Download complete\n",
      "Step #2 - \"Unit Test Model\": f6f8f4265c8c: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": f6f8f4265c8c: Download complete\n",
      "Step #2 - \"Unit Test Model\": 1ced637de50b: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": 1ced637de50b: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 1ced637de50b: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": 1ced637de50b: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": b6c7eb38f366: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": b6c7eb38f366: Download complete\n",
      "Step #2 - \"Unit Test Model\": b6c7eb38f366: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 520be5017b4d: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": 520be5017b4d: Download complete\n",
      "Step #2 - \"Unit Test Model\": 520be5017b4d: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": 520be5017b4d: Download complete\n",
      "Step #2 - \"Unit Test Model\": 0a1aacb7e387: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": 0a1aacb7e387: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 0a1aacb7e387: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": 0a1aacb7e387: Download complete\n",
      "Step #2 - \"Unit Test Model\": 595b1c49222a: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": 595b1c49222a: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 595b1c49222a: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": 595b1c49222a: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 8f605134caf9: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": 8f605134caf9: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": 8f605134caf9: Download complete\n",
      "Step #2 - \"Unit Test Model\": 8f605134caf9: Download complete\n",
      "Step #2 - \"Unit Test Model\": 189ba322d030: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 189ba322d030: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": 189ba322d030: Download complete\n",
      "Step #2 - \"Unit Test Model\": 7e13e06b6d24: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": 7e13e06b6d24: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 7e13e06b6d24: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": 7e13e06b6d24: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": af92447b9198: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": af92447b9198: Download complete\n",
      "Step #2 - \"Unit Test Model\": af92447b9198: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": af92447b9198: Download complete\n",
      "Step #2 - \"Unit Test Model\": 7ca7c630ce44: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": 7ca7c630ce44: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 7ca7c630ce44: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": 7ca7c630ce44: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 8ccde080a09e: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": 8ccde080a09e: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": 8ccde080a09e: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 8ccde080a09e: Download complete\n",
      "Step #2 - \"Unit Test Model\": 762ff1eb7f16: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": 762ff1eb7f16: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 762ff1eb7f16: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": 762ff1eb7f16: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": ab6e5a9c78ee: Pull complete\n",
      "Step #2 - \"Unit Test Model\": ab6e5a9c78ee: Pull complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 7bc1690abd59: Pull complete\n",
      "Step #2 - \"Unit Test Model\": 7bc1690abd59: Pull complete\n",
      "Step #2 - \"Unit Test Model\": f5b4dd7682bc: Pull complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": f5b4dd7682bc: Pull complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": d6897660f71d: Pull complete\n",
      "Step #2 - \"Unit Test Model\": d6897660f71d: Pull complete\n",
      "Step #2 - \"Unit Test Model\": 174d792fb622: Pull complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 174d792fb622: Pull complete\n",
      "Step #2 - \"Unit Test Model\": 5f8143275aca: Pull complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 5f8143275aca: Pull complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 56646f115483: Pull complete\n",
      "Step #2 - \"Unit Test Model\": 56646f115483: Pull complete\n",
      "Step #2 - \"Unit Test Model\": 798922b52524: Pull complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 798922b52524: Pull complete\n",
      "Step #2 - \"Unit Test Model\": e2699a9f592b: Pull complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": e2699a9f592b: Pull complete\n",
      "Step #2 - \"Unit Test Model\": f43e7d1c07e4: Pull complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": f43e7d1c07e4: Pull complete\n",
      "Step #2 - \"Unit Test Model\": 1e71d5e9923d: Pull complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 1e71d5e9923d: Pull complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": bf6ae2a2e250: Pull complete\n",
      "Step #2 - \"Unit Test Model\": bf6ae2a2e250: Pull complete\n",
      "Step #2 - \"Unit Test Model\": e49679b748d5: Pull complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": e49679b748d5: Pull complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 80208bd6f7fb: Pull complete\n",
      "Step #2 - \"Unit Test Model\": 80208bd6f7fb: Pull complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": b83c16bef138: Pull complete\n",
      "Step #2 - \"Unit Test Model\": b83c16bef138: Pull complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 9d1427033824: Pull complete\n",
      "Step #2 - \"Unit Test Model\": 9d1427033824: Pull complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": c0028679f003: Pull complete\n",
      "Step #2 - \"Unit Test Model\": c0028679f003: Pull complete\n",
      "Step #2 - \"Unit Test Model\": 09c222e7ff04: Pull complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 09c222e7ff04: Pull complete\n",
      "Step #2 - \"Unit Test Model\": ae6048a3aec1: Pull complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": ae6048a3aec1: Pull complete\n",
      "Step #2 - \"Unit Test Model\": 1ced637de50b: Pull complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 1ced637de50b: Pull complete\n",
      "Step #2 - \"Unit Test Model\": 762ff1eb7f16: Pull complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 762ff1eb7f16: Pull complete\n",
      "Step #2 - \"Unit Test Model\": f6f8f4265c8c: Pull complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": f6f8f4265c8c: Pull complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 595b1c49222a: Pull complete\n",
      "Step #2 - \"Unit Test Model\": 595b1c49222a: Pull complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": b6c7eb38f366: Pull complete\n",
      "Step #2 - \"Unit Test Model\": b6c7eb38f366: Pull complete\n",
      "Step #2 - \"Unit Test Model\": 520be5017b4d: Pull complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 520be5017b4d: Pull complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 0a1aacb7e387: Pull complete\n",
      "Step #2 - \"Unit Test Model\": 0a1aacb7e387: Pull complete\n",
      "Step #2 - \"Unit Test Model\": 8f605134caf9: Pull complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 8f605134caf9: Pull complete\n",
      "Step #2 - \"Unit Test Model\": 189ba322d030: Pull complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 189ba322d030: Pull complete\n",
      "Step #2 - \"Unit Test Model\": af92447b9198: Pull complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": af92447b9198: Pull complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 7e13e06b6d24: Pull complete\n",
      "Step #2 - \"Unit Test Model\": 7e13e06b6d24: Pull complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 7ca7c630ce44: Pull complete\n",
      "Step #2 - \"Unit Test Model\": 7ca7c630ce44: Pull complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 8ccde080a09e: Pull complete\n",
      "Step #2 - \"Unit Test Model\": 8ccde080a09e: Pull complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": Digest: sha256:ae350d9b4e23b2db1d47425b6d21faad1d03105753226ecbd33d654137facaf2\n",
      "Step #1 - \"Unit Test Datasource Utils\": Status: Downloaded newer image for gcr.io/grandelli-demo-295810/cicd:latest\n",
      "Step #2 - \"Unit Test Model\": Digest: sha256:ae350d9b4e23b2db1d47425b6d21faad1d03105753226ecbd33d654137facaf2\n",
      "Step #2 - \"Unit Test Model\": Status: Image is up to date for gcr.io/grandelli-demo-295810/cicd:latest\n",
      "Step #1 - \"Unit Test Datasource Utils\": gcr.io/grandelli-demo-295810/cicd:latest\n",
      "Step #2 - \"Unit Test Model\": gcr.io/grandelli-demo-295810/cicd:latest\n",
      "Step #2 - \"Unit Test Model\": 2022-03-30 17:17:24.505487: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "Step #1 - \"Unit Test Datasource Utils\": ============================= test session starts ==============================\n",
      "Step #1 - \"Unit Test Datasource Utils\": platform linux -- Python 3.7.10, pytest-7.1.1, pluggy-1.0.0\n",
      "Step #1 - \"Unit Test Datasource Utils\": rootdir: /workspace/mlops-with-vertex-ai\n",
      "Step #1 - \"Unit Test Datasource Utils\": plugins: anyio-3.3.0\n",
      "Step #1 - \"Unit Test Datasource Utils\": collected 2 items\n",
      "Step #1 - \"Unit Test Datasource Utils\": \n",
      "Step #1 - \"Unit Test Datasource Utils\": src/tests/datasource_utils_tests.py BigQuery Source: grandelli-demo-295810.partner_training.chicago_taxitrips_prep\n",
      "Step #2 - \"Unit Test Model\": ============================= test session starts ==============================\n",
      "Step #2 - \"Unit Test Model\": platform linux -- Python 3.7.10, pytest-7.1.1, pluggy-1.0.0\n",
      "Step #2 - \"Unit Test Model\": rootdir: /workspace/mlops-with-vertex-ai\n",
      "Step #2 - \"Unit Test Model\": plugins: anyio-3.3.0\n",
      "Step #2 - \"Unit Test Model\": collected 2 items\n",
      "Step #2 - \"Unit Test Model\": \n",
      "Step #2 - \"Unit Test Model\": 2022-03-30 17:17:26.430963: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "Step #2 - \"Unit Test Model\": 2022-03-30 17:17:26.431019: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "Step #2 - \"Unit Test Model\": 2022-03-30 17:17:26.431046: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (7baf71da4088): /proc/driver/nvidia/version does not exist\n",
      "Step #2 - \"Unit Test Model\": 2022-03-30 17:17:26.431439: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "Step #2 - \"Unit Test Model\": To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Step #2 - \"Unit Test Model\": src/tests/model_tests.py .max_tokens is deprecated, please use num_tokens instead.\n",
      "Step #2 - \"Unit Test Model\": max_tokens is deprecated, please use num_tokens instead.\n",
      "Step #2 - \"Unit Test Model\": .\n",
      "Step #2 - \"Unit Test Model\": \n",
      "Step #2 - \"Unit Test Model\": =============================== warnings summary ===============================\n",
      "Step #2 - \"Unit Test Model\": ../../opt/conda/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py:22\n",
      "Step #2 - \"Unit Test Model\":   /opt/conda/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py:22: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "Step #2 - \"Unit Test Model\":     import imp\n",
      "Step #2 - \"Unit Test Model\": \n",
      "Step #2 - \"Unit Test Model\": -- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "Step #2 - \"Unit Test Model\": ========================= 2 passed, 1 warning in 2.55s =========================\n",
      "Finished Step #2 - \"Unit Test Model\"\n",
      "Step #1 - \"Unit Test Datasource Utils\": .BigQuery Source: grandelli-demo-295810.partner_training.chicago_taxitrips_prep\n",
      "Step #1 - \"Unit Test Datasource Utils\": .\n",
      "Step #1 - \"Unit Test Datasource Utils\": \n",
      "Step #1 - \"Unit Test Datasource Utils\": ============================== 2 passed in 6.55s ===============================\n",
      "Finished Step #1 - \"Unit Test Datasource Utils\"\n",
      "Starting Step #3 - \"Local Test E2E Pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": Already have image (with digest): gcr.io/grandelli-demo-295810/cicd:latest\n",
      "Step #3 - \"Local Test E2E Pipeline\": 2022-03-30 17:17:32.477336: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "Step #3 - \"Local Test E2E Pipeline\": ============================= test session starts ==============================\n",
      "Step #3 - \"Local Test E2E Pipeline\": platform linux -- Python 3.7.10, pytest-7.1.1, pluggy-1.0.0\n",
      "Step #3 - \"Local Test E2E Pipeline\": rootdir: /workspace/mlops-with-vertex-ai\n",
      "Step #3 - \"Local Test E2E Pipeline\": plugins: anyio-3.3.0\n",
      "Step #3 - \"Local Test E2E Pipeline\": collected 1 item\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": src/tests/pipeline_deployment_tests.py upload_model: 0\n",
      "Step #3 - \"Local Test E2E Pipeline\": Pipeline e2e test artifacts stored in: gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests\n",
      "Step #3 - \"Local Test E2E Pipeline\": ML metadata store is ready.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Excluding no splits because exclude_splits is not set.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Excluding no splits because exclude_splits is not set.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Pipeline components: ['HyperparamsGen', 'TrainDataGen', 'TestDataGen', 'StatisticsGen', 'SchemaImporter', 'ExampleValidator', 'DataTransformer', 'WarmstartModelResolver', 'ModelTrainer', 'BaselineModelResolver', 'ModelEvaluator', 'ModelPusher']\n",
      "Step #3 - \"Local Test E2E Pipeline\": Beam pipeline args: ['--project=grandelli-demo-295810', '--temp_location=gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/temp']\n",
      "Step #3 - \"Local Test E2E Pipeline\": Generating ephemeral wheel package for '/workspace/mlops-with-vertex-ai/src/preprocessing/transformations.py' (including modules: ['etl', 'transformations']).\n",
      "Step #3 - \"Local Test E2E Pipeline\": User module package has hash fingerprint version de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Executing: ['/opt/conda/bin/python', '/tmp/tmppu51cl_8/_tfx_generated_setup.py', 'bdist_wheel', '--bdist-dir', '/tmp/tmp45vxzb0i', '--dist-dir', '/tmp/tmp6ihx3jri']\n",
      "Step #3 - \"Local Test E2E Pipeline\": running bdist_wheel\n",
      "Step #3 - \"Local Test E2E Pipeline\": running build\n",
      "Step #3 - \"Local Test E2E Pipeline\": running build_py\n",
      "Step #3 - \"Local Test E2E Pipeline\": creating build\n",
      "Step #3 - \"Local Test E2E Pipeline\": creating build/lib\n",
      "Step #3 - \"Local Test E2E Pipeline\": copying etl.py -> build/lib\n",
      "Step #3 - \"Local Test E2E Pipeline\": copying transformations.py -> build/lib\n",
      "Step #3 - \"Local Test E2E Pipeline\": installing to /tmp/tmp45vxzb0i\n",
      "Step #3 - \"Local Test E2E Pipeline\": running install\n",
      "Step #3 - \"Local Test E2E Pipeline\": running install_lib\n",
      "Step #3 - \"Local Test E2E Pipeline\": copying build/lib/etl.py -> /tmp/tmp45vxzb0i\n",
      "Step #3 - \"Local Test E2E Pipeline\": copying build/lib/transformations.py -> /tmp/tmp45vxzb0i\n",
      "Step #3 - \"Local Test E2E Pipeline\": running install_egg_info\n",
      "Step #3 - \"Local Test E2E Pipeline\": running egg_info\n",
      "Step #3 - \"Local Test E2E Pipeline\": creating tfx_user_code_DataTransformer.egg-info\n",
      "Step #3 - \"Local Test E2E Pipeline\": writing tfx_user_code_DataTransformer.egg-info/PKG-INFO\n",
      "Step #3 - \"Local Test E2E Pipeline\": writing dependency_links to tfx_user_code_DataTransformer.egg-info/dependency_links.txt\n",
      "Step #3 - \"Local Test E2E Pipeline\": writing top-level names to tfx_user_code_DataTransformer.egg-info/top_level.txt\n",
      "Step #3 - \"Local Test E2E Pipeline\": writing manifest file 'tfx_user_code_DataTransformer.egg-info/SOURCES.txt'\n",
      "Step #3 - \"Local Test E2E Pipeline\": reading manifest file 'tfx_user_code_DataTransformer.egg-info/SOURCES.txt'\n",
      "Step #3 - \"Local Test E2E Pipeline\": writing manifest file 'tfx_user_code_DataTransformer.egg-info/SOURCES.txt'\n",
      "Step #3 - \"Local Test E2E Pipeline\": Copying tfx_user_code_DataTransformer.egg-info to /tmp/tmp45vxzb0i/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3.7.egg-info\n",
      "Step #3 - \"Local Test E2E Pipeline\": running install_scripts\n",
      "Step #3 - \"Local Test E2E Pipeline\": creating /tmp/tmp45vxzb0i/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d.dist-info/WHEEL\n",
      "Step #3 - \"Local Test E2E Pipeline\": creating '/tmp/tmp6ihx3jri/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl' and adding '/tmp/tmp45vxzb0i' to it\n",
      "Step #3 - \"Local Test E2E Pipeline\": adding 'etl.py'\n",
      "Step #3 - \"Local Test E2E Pipeline\": adding 'transformations.py'\n",
      "Step #3 - \"Local Test E2E Pipeline\": adding 'tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d.dist-info/METADATA'\n",
      "Step #3 - \"Local Test E2E Pipeline\": adding 'tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d.dist-info/WHEEL'\n",
      "Step #3 - \"Local Test E2E Pipeline\": adding 'tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d.dist-info/top_level.txt'\n",
      "Step #3 - \"Local Test E2E Pipeline\": adding 'tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d.dist-info/RECORD'\n",
      "Step #3 - \"Local Test E2E Pipeline\": removing /tmp/tmp45vxzb0i\n",
      "Step #3 - \"Local Test E2E Pipeline\": Successfully built user code wheel distribution at 'gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/_wheels/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl'; target user module is 'transformations'.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Full user module path is 'transformations@gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/_wheels/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl'\n",
      "Step #3 - \"Local Test E2E Pipeline\": Generating ephemeral wheel package for '/workspace/mlops-with-vertex-ai/src/model_training/runner.py' (including modules: ['defaults', 'trainer', 'model', 'runner', 'data', 'task', 'exporter']).\n",
      "Step #3 - \"Local Test E2E Pipeline\": User module package has hash fingerprint version 5bd9ac13044cc46c88b21ccdcff2b74d85a1c15be76527e33008964fa7da7d12.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Executing: ['/opt/conda/bin/python', '/tmp/tmpzscdtv04/_tfx_generated_setup.py', 'bdist_wheel', '--bdist-dir', '/tmp/tmpe9d_eqio', '--dist-dir', '/tmp/tmppa853kpl']\n",
      "Step #3 - \"Local Test E2E Pipeline\": running bdist_wheel\n",
      "Step #3 - \"Local Test E2E Pipeline\": running build\n",
      "Step #3 - \"Local Test E2E Pipeline\": running build_py\n",
      "Step #3 - \"Local Test E2E Pipeline\": creating build\n",
      "Step #3 - \"Local Test E2E Pipeline\": creating build/lib\n",
      "Step #3 - \"Local Test E2E Pipeline\": copying defaults.py -> build/lib\n",
      "Step #3 - \"Local Test E2E Pipeline\": copying trainer.py -> build/lib\n",
      "Step #3 - \"Local Test E2E Pipeline\": copying model.py -> build/lib\n",
      "Step #3 - \"Local Test E2E Pipeline\": copying runner.py -> build/lib\n",
      "Step #3 - \"Local Test E2E Pipeline\": copying data.py -> build/lib\n",
      "Step #3 - \"Local Test E2E Pipeline\": copying task.py -> build/lib\n",
      "Step #3 - \"Local Test E2E Pipeline\": copying exporter.py -> build/lib\n",
      "Step #3 - \"Local Test E2E Pipeline\": installing to /tmp/tmpe9d_eqio\n",
      "Step #3 - \"Local Test E2E Pipeline\": running install\n",
      "Step #3 - \"Local Test E2E Pipeline\": running install_lib\n",
      "Step #3 - \"Local Test E2E Pipeline\": copying build/lib/defaults.py -> /tmp/tmpe9d_eqio\n",
      "Step #3 - \"Local Test E2E Pipeline\": copying build/lib/trainer.py -> /tmp/tmpe9d_eqio\n",
      "Step #3 - \"Local Test E2E Pipeline\": copying build/lib/model.py -> /tmp/tmpe9d_eqio\n",
      "Step #3 - \"Local Test E2E Pipeline\": copying build/lib/runner.py -> /tmp/tmpe9d_eqio\n",
      "Step #3 - \"Local Test E2E Pipeline\": copying build/lib/data.py -> /tmp/tmpe9d_eqio\n",
      "Step #3 - \"Local Test E2E Pipeline\": copying build/lib/task.py -> /tmp/tmpe9d_eqio\n",
      "Step #3 - \"Local Test E2E Pipeline\": copying build/lib/exporter.py -> /tmp/tmpe9d_eqio\n",
      "Step #3 - \"Local Test E2E Pipeline\": running install_egg_info\n",
      "Step #3 - \"Local Test E2E Pipeline\": running egg_info\n",
      "Step #3 - \"Local Test E2E Pipeline\": creating tfx_user_code_ModelTrainer.egg-info\n",
      "Step #3 - \"Local Test E2E Pipeline\": writing tfx_user_code_ModelTrainer.egg-info/PKG-INFO\n",
      "Step #3 - \"Local Test E2E Pipeline\": writing dependency_links to tfx_user_code_ModelTrainer.egg-info/dependency_links.txt\n",
      "Step #3 - \"Local Test E2E Pipeline\": writing top-level names to tfx_user_code_ModelTrainer.egg-info/top_level.txt\n",
      "Step #3 - \"Local Test E2E Pipeline\": writing manifest file 'tfx_user_code_ModelTrainer.egg-info/SOURCES.txt'\n",
      "Step #3 - \"Local Test E2E Pipeline\": reading manifest file 'tfx_user_code_ModelTrainer.egg-info/SOURCES.txt'\n",
      "Step #3 - \"Local Test E2E Pipeline\": writing manifest file 'tfx_user_code_ModelTrainer.egg-info/SOURCES.txt'\n",
      "Step #3 - \"Local Test E2E Pipeline\": Copying tfx_user_code_ModelTrainer.egg-info to /tmp/tmpe9d_eqio/tfx_user_code_ModelTrainer-0.0+5bd9ac13044cc46c88b21ccdcff2b74d85a1c15be76527e33008964fa7da7d12-py3.7.egg-info\n",
      "Step #3 - \"Local Test E2E Pipeline\": running install_scripts\n",
      "Step #3 - \"Local Test E2E Pipeline\": creating /tmp/tmpe9d_eqio/tfx_user_code_ModelTrainer-0.0+5bd9ac13044cc46c88b21ccdcff2b74d85a1c15be76527e33008964fa7da7d12.dist-info/WHEEL\n",
      "Step #3 - \"Local Test E2E Pipeline\": creating '/tmp/tmppa853kpl/tfx_user_code_ModelTrainer-0.0+5bd9ac13044cc46c88b21ccdcff2b74d85a1c15be76527e33008964fa7da7d12-py3-none-any.whl' and adding '/tmp/tmpe9d_eqio' to it\n",
      "Step #3 - \"Local Test E2E Pipeline\": adding 'data.py'\n",
      "Step #3 - \"Local Test E2E Pipeline\": adding 'defaults.py'\n",
      "Step #3 - \"Local Test E2E Pipeline\": adding 'exporter.py'\n",
      "Step #3 - \"Local Test E2E Pipeline\": adding 'model.py'\n",
      "Step #3 - \"Local Test E2E Pipeline\": adding 'runner.py'\n",
      "Step #3 - \"Local Test E2E Pipeline\": adding 'task.py'\n",
      "Step #3 - \"Local Test E2E Pipeline\": adding 'trainer.py'\n",
      "Step #3 - \"Local Test E2E Pipeline\": adding 'tfx_user_code_ModelTrainer-0.0+5bd9ac13044cc46c88b21ccdcff2b74d85a1c15be76527e33008964fa7da7d12.dist-info/METADATA'\n",
      "Step #3 - \"Local Test E2E Pipeline\": adding 'tfx_user_code_ModelTrainer-0.0+5bd9ac13044cc46c88b21ccdcff2b74d85a1c15be76527e33008964fa7da7d12.dist-info/WHEEL'\n",
      "Step #3 - \"Local Test E2E Pipeline\": adding 'tfx_user_code_ModelTrainer-0.0+5bd9ac13044cc46c88b21ccdcff2b74d85a1c15be76527e33008964fa7da7d12.dist-info/top_level.txt'\n",
      "Step #3 - \"Local Test E2E Pipeline\": adding 'tfx_user_code_ModelTrainer-0.0+5bd9ac13044cc46c88b21ccdcff2b74d85a1c15be76527e33008964fa7da7d12.dist-info/RECORD'\n",
      "Step #3 - \"Local Test E2E Pipeline\": removing /tmp/tmpe9d_eqio\n",
      "Step #3 - \"Local Test E2E Pipeline\": Successfully built user code wheel distribution at 'gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/_wheels/tfx_user_code_ModelTrainer-0.0+5bd9ac13044cc46c88b21ccdcff2b74d85a1c15be76527e33008964fa7da7d12-py3-none-any.whl'; target user module is 'runner'.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Full user module path is 'runner@gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/_wheels/tfx_user_code_ModelTrainer-0.0+5bd9ac13044cc46c88b21ccdcff2b74d85a1c15be76527e33008964fa7da7d12-py3-none-any.whl'\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running pipeline:\n",
      "Step #3 - \"Local Test E2E Pipeline\":  pipeline_info {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   id: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": nodes {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   pipeline_node {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     node_info {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         name: \"tfx.dsl.components.common.resolver.Resolver\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       id: \"BaselineModelResolver\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"2022-03-30T17:18:26.717519\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.BaselineModelResolver\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"model\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           channels {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                   string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             artifact_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"Model\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"model_blessing\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           channels {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                   string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             artifact_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"ModelBlessing\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       resolver_config {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         resolver_steps {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           class_path: \"tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy.LatestBlessedModelStrategy\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           config_json: \"{}\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           input_keys: \"model\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           input_keys: \"model_blessing\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     downstream_nodes: \"ModelEvaluator\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     execution_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       caching_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": nodes {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   pipeline_node {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     node_info {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         name: \"src.tfx_pipelines.components.hyperparameters_gen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       id: \"HyperparamsGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"2022-03-30T17:18:26.717519\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.HyperparamsGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"hyperparameters\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name: \"HyperParameters\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"batch_size\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             int_value: 512\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"hidden_units\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"128,128\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"learning_rate\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             double_value: 0.001\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"num_epochs\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             int_value: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     downstream_nodes: \"ModelTrainer\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     execution_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       caching_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": nodes {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   pipeline_node {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     node_info {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         name: \"tfx.dsl.components.common.importer.Importer\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       id: \"SchemaImporter\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"2022-03-30T17:18:26.717519\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.SchemaImporter\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"result\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name: \"Schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"artifact_uri\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"src/raw_schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"reimport\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             int_value: 0\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     downstream_nodes: \"DataTransformer\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     downstream_nodes: \"ExampleValidator\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     downstream_nodes: \"ModelEvaluator\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     downstream_nodes: \"ModelTrainer\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     execution_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       caching_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": nodes {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   pipeline_node {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     node_info {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         name: \"tfx.extensions.google_cloud_big_query.example_gen.component.BigQueryExampleGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       id: \"TestDataGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"2022-03-30T17:18:26.717519\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.TestDataGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name: \"Examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 key: \"version\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"input_config\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"\\\\n    SELECT \\\\n        IF(trip_month IS NULL, -1, trip_month) trip_month,\\\\n        IF(trip_day IS NULL, -1, trip_day) trip_day,\\\\n        IF(trip_day_of_week IS NULL, -1, trip_day_of_week) trip_day_of_week,\\\\n        IF(trip_hour IS NULL, -1, trip_hour) trip_hour,\\\\n        IF(trip_seconds IS NULL, -1, trip_seconds) trip_seconds,\\\\n        IF(trip_miles IS NULL, -1, trip_miles) trip_miles,\\\\n        IF(payment_type IS NULL, \\'NA\\', payment_type) payment_type,\\\\n        IF(pickup_grid IS NULL, \\'NA\\', pickup_grid) pickup_grid,\\\\n        IF(dropoff_grid IS NULL, \\'NA\\', dropoff_grid) dropoff_grid,\\\\n        IF(euclidean IS NULL, -1, euclidean) euclidean,\\\\n        IF(loc_cross IS NULL, \\'NA\\', loc_cross) loc_cross,\\\\n        tip_bin\\\\n    FROM partner_training.chicago_taxitrips_prep \\\\n    WHERE ML_use = \\'TEST\\'\\\\n    LIMIT 100\\\"\\n    }\\n  ]\\n}\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"output_config\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"test\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"output_data_format\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             int_value: 6\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"output_file_format\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             int_value: 5\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     downstream_nodes: \"ModelEvaluator\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     execution_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       caching_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": nodes {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   pipeline_node {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     node_info {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         name: \"tfx.extensions.google_cloud_big_query.example_gen.component.BigQueryExampleGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       id: \"TrainDataGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"2022-03-30T17:18:26.717519\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.TrainDataGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name: \"Examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 key: \"version\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"input_config\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"\\\\n    SELECT \\\\n        IF(trip_month IS NULL, -1, trip_month) trip_month,\\\\n        IF(trip_day IS NULL, -1, trip_day) trip_day,\\\\n        IF(trip_day_of_week IS NULL, -1, trip_day_of_week) trip_day_of_week,\\\\n        IF(trip_hour IS NULL, -1, trip_hour) trip_hour,\\\\n        IF(trip_seconds IS NULL, -1, trip_seconds) trip_seconds,\\\\n        IF(trip_miles IS NULL, -1, trip_miles) trip_miles,\\\\n        IF(payment_type IS NULL, \\'NA\\', payment_type) payment_type,\\\\n        IF(pickup_grid IS NULL, \\'NA\\', pickup_grid) pickup_grid,\\\\n        IF(dropoff_grid IS NULL, \\'NA\\', dropoff_grid) dropoff_grid,\\\\n        IF(euclidean IS NULL, -1, euclidean) euclidean,\\\\n        IF(loc_cross IS NULL, \\'NA\\', loc_cross) loc_cross,\\\\n        tip_bin\\\\n    FROM partner_training.chicago_taxitrips_prep \\\\n    WHERE ML_use = \\'UNASSIGNED\\'\\\\n    LIMIT 1000\\\"\\n    }\\n  ]\\n}\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"output_config\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 4,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"output_data_format\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             int_value: 6\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"output_file_format\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             int_value: 5\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     downstream_nodes: \"DataTransformer\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     downstream_nodes: \"StatisticsGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     execution_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       caching_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": nodes {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   pipeline_node {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     node_info {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         name: \"tfx.dsl.components.common.resolver.Resolver\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       id: \"WarmstartModelResolver\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"2022-03-30T17:18:26.717519\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.WarmstartModelResolver\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"latest_model\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           channels {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                   string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             artifact_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"Model\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       resolver_config {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         resolver_steps {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           class_path: \"tfx.dsl.input_resolution.strategies.latest_artifact_strategy.LatestArtifactStrategy\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           config_json: \"{}\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           input_keys: \"latest_model\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     downstream_nodes: \"ModelTrainer\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     execution_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       caching_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": nodes {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   pipeline_node {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     node_info {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         name: \"tfx.components.statistics_gen.component.StatisticsGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       id: \"StatisticsGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"2022-03-30T17:18:26.717519\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.StatisticsGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           channels {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             producer_node_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               id: \"TrainDataGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                   string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                   string_value: \"2022-03-30T17:18:26.717519\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                   string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.TrainDataGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             artifact_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"Examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             output_key: \"examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"statistics\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name: \"ExampleStatistics\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"exclude_splits\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"[]\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     upstream_nodes: \"TrainDataGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     downstream_nodes: \"ExampleValidator\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     execution_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       caching_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": nodes {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   pipeline_node {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     node_info {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         name: \"tfx.components.example_validator.component.ExampleValidator\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       id: \"ExampleValidator\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"2022-03-30T17:18:26.717519\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.ExampleValidator\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           channels {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             producer_node_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               id: \"SchemaImporter\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                   string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                   string_value: \"2022-03-30T17:18:26.717519\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                   string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.SchemaImporter\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             artifact_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"Schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             output_key: \"result\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"statistics\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           channels {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             producer_node_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               id: \"StatisticsGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                   string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                   string_value: \"2022-03-30T17:18:26.717519\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                   string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.StatisticsGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             artifact_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"ExampleStatistics\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             output_key: \"statistics\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"anomalies\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name: \"ExampleAnomalies\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"exclude_splits\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"[]\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     upstream_nodes: \"SchemaImporter\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     upstream_nodes: \"StatisticsGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     downstream_nodes: \"DataTransformer\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     execution_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       caching_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": nodes {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   pipeline_node {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     node_info {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         name: \"tfx.components.transform.component.Transform\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       id: \"DataTransformer\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"2022-03-30T17:18:26.717519\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.DataTransformer\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           channels {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             producer_node_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               id: \"TrainDataGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                   string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                   string_value: \"2022-03-30T17:18:26.717519\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                   string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.TrainDataGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             artifact_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"Examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             output_key: \"examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           channels {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             producer_node_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               id: \"SchemaImporter\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                   string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                   string_value: \"2022-03-30T17:18:26.717519\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                   string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.SchemaImporter\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             artifact_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"Schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             output_key: \"result\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"post_transform_anomalies\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name: \"ExampleAnomalies\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"post_transform_schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name: \"Schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"post_transform_stats\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name: \"ExampleStatistics\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"pre_transform_schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name: \"Schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"pre_transform_stats\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name: \"ExampleStatistics\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"transform_graph\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name: \"TransformGraph\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"transformed_examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name: \"Examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 key: \"version\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"updated_analyzer_cache\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name: \"TransformCache\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"custom_config\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"null\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"disable_statistics\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             int_value: 0\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"force_tf_compat_v1\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             int_value: 0\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"module_path\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"transformations@gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/_wheels/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"splits_config\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"{\\n  \\\"analyze\\\": [\\n    \\\"train\\\"\\n  ],\\n  \\\"transform\\\": [\\n    \\\"train\\\",\\n    \\\"eval\\\"\\n  ]\\n}\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     upstream_nodes: \"ExampleValidator\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     upstream_nodes: \"SchemaImporter\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     upstream_nodes: \"TrainDataGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     downstream_nodes: \"ModelTrainer\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     execution_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       caching_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": nodes {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   pipeline_node {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     node_info {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         name: \"tfx.components.trainer.component.Trainer\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       id: \"ModelTrainer\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"2022-03-30T17:18:26.717519\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.ModelTrainer\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"base_model\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           channels {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             producer_node_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               id: \"WarmstartModelResolver\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                   string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                   string_value: \"2022-03-30T17:18:26.717519\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                   string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.WarmstartModelResolver\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             artifact_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"Model\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             output_key: \"latest_model\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           channels {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             producer_node_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               id: \"DataTransformer\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                   string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                   string_value: \"2022-03-30T17:18:26.717519\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                   string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.DataTransformer\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             artifact_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"Examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             output_key: \"transformed_examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"hyperparameters\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           channels {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             producer_node_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               id: \"HyperparamsGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                   string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                   string_value: \"2022-03-30T17:18:26.717519\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                   string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.HyperparamsGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             artifact_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"HyperParameters\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             output_key: \"hyperparameters\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           channels {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             producer_node_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               id: \"SchemaImporter\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                   string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                   string_value: \"2022-03-30T17:18:26.717519\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                   string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.SchemaImporter\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             artifact_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"Schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             output_key: \"result\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"transform_graph\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           channels {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             producer_node_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               id: \"DataTransformer\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                   string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                   string_value: \"2022-03-30T17:18:26.717519\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                   string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.DataTransformer\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             artifact_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"TransformGraph\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             output_key: \"transform_graph\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"model\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name: \"Model\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"model_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name: \"ModelRun\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"custom_config\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"null\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"eval_args\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"{}\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"module_path\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"runner@gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/_wheels/tfx_user_code_ModelTrainer-0.0+5bd9ac13044cc46c88b21ccdcff2b74d85a1c15be76527e33008964fa7da7d12-py3-none-any.whl\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"train_args\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"{}\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     upstream_nodes: \"DataTransformer\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     upstream_nodes: \"HyperparamsGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     upstream_nodes: \"SchemaImporter\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     upstream_nodes: \"WarmstartModelResolver\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     downstream_nodes: \"ModelEvaluator\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     downstream_nodes: \"ModelPusher\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     execution_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       caching_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": nodes {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   pipeline_node {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     node_info {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         name: \"tfx.components.evaluator.component.Evaluator\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       id: \"ModelEvaluator\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"2022-03-30T17:18:26.717519\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.ModelEvaluator\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"baseline_model\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           channels {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             producer_node_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               id: \"BaselineModelResolver\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                   string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                   string_value: \"2022-03-30T17:18:26.717519\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                   string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.BaselineModelResolver\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             artifact_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"Model\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             output_key: \"model\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           channels {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             producer_node_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               id: \"TestDataGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                   string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                   string_value: \"2022-03-30T17:18:26.717519\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                   string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.TestDataGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             artifact_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"Examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             output_key: \"examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"model\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           channels {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             producer_node_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               id: \"ModelTrainer\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                   string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                   string_value: \"2022-03-30T17:18:26.717519\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                   string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.ModelTrainer\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             artifact_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"Model\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             output_key: \"model\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           channels {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             producer_node_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               id: \"SchemaImporter\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                   string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                   string_value: \"2022-03-30T17:18:26.717519\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                   string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.SchemaImporter\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             artifact_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"Schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             output_key: \"result\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"blessing\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name: \"ModelBlessing\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"evaluation\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name: \"ModelEvaluation\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"eval_config\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"{\\n  \\\"metrics_specs\\\": [\\n    {\\n      \\\"metrics\\\": [\\n        {\\n          \\\"class_name\\\": \\\"ExampleCount\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"BinaryAccuracy\\\",\\n          \\\"threshold\\\": {\\n            \\\"change_threshold\\\": {\\n              \\\"absolute\\\": -1e-10,\\n              \\\"direction\\\": \\\"HIGHER_IS_BETTER\\\"\\n            },\\n            \\\"value_threshold\\\": {\\n              \\\"lower_bound\\\": 0.1\\n            }\\n          }\\n        }\\n      ]\\n    }\\n  ],\\n  \\\"model_specs\\\": [\\n    {\\n      \\\"label_key\\\": \\\"tip_bin\\\",\\n      \\\"prediction_key\\\": \\\"probabilities\\\",\\n      \\\"signature_name\\\": \\\"serving_tf_example\\\"\\n    }\\n  ],\\n  \\\"slicing_specs\\\": [\\n    {}\\n  ]\\n}\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"example_splits\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"[\\\"test\\\"]\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"fairness_indicator_thresholds\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"null\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     upstream_nodes: \"BaselineModelResolver\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     upstream_nodes: \"ModelTrainer\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     upstream_nodes: \"SchemaImporter\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     upstream_nodes: \"TestDataGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     downstream_nodes: \"ModelPusher\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     execution_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       caching_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": nodes {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   pipeline_node {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     node_info {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         name: \"tfx.components.pusher.component.Pusher\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       id: \"ModelPusher\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"2022-03-30T17:18:26.717519\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.ModelPusher\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"model\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           channels {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             producer_node_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               id: \"ModelTrainer\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                   string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                   string_value: \"2022-03-30T17:18:26.717519\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                   string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.ModelTrainer\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             artifact_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"Model\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             output_key: \"model\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"model_blessing\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           channels {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             producer_node_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               id: \"ModelEvaluator\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                   string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                   string_value: \"2022-03-30T17:18:26.717519\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                   string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.ModelEvaluator\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 }\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             artifact_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":                 name: \"ModelBlessing\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":               }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":             output_key: \"blessing\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"pushed_model\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               name: \"PushedModel\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"custom_config\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"null\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         key: \"push_destination\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             string_value: \"{\\n  \\\"filesystem\\\": {\\n    \\\"base_directory\\\": \\\"gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/model_registry/chicago-taxi-tips-classifier-v01\\\"\\n  }\\n}\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     upstream_nodes: \"ModelEvaluator\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     upstream_nodes: \"ModelTrainer\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     execution_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       caching_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": runtime_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   pipeline_root {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       string_value: \"gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   pipeline_run_id {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       string_value: \"2022-03-30T17:18:26.717519\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": execution_mode: SYNC\n",
      "Step #3 - \"Local Test E2E Pipeline\": deployment_config {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   type_url: \"type.googleapis.com/tfx.orchestration.IntermediateDeploymentConfig\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: \"\\n\\213\\001\\n\\013ModelPusher\\022|\\nOtype.googleapis.com/tfx.orchestration.executable_spec.PythonClassExecutableSpec\\022)\\n\\'tfx.components.pusher.executor.Executor\\n\\222\\002\\n\\016ModelEvaluator\\022\\377\\001\\nHtype.googleapis.com/tfx.orchestration.executable_spec.BeamExecutableSpec\\022\\262\\001\\n,\\n*tfx.components.evaluator.executor.Executor\\022\\037--project=grandelli-demo-295810\\022a--temp_location=gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/temp\\n\\226\\002\\n\\rStatisticsGen\\022\\204\\002\\nHtype.googleapis.com/tfx.orchestration.executable_spec.BeamExecutableSpec\\022\\267\\001\\n1\\n/tfx.components.statistics_gen.executor.Executor\\022\\037--project=grandelli-demo-295810\\022a--temp_location=gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/temp\\n\\225\\001\\n\\014ModelTrainer\\022\\204\\001\\nOtype.googleapis.com/tfx.orchestration.executable_spec.PythonClassExecutableSpec\\0221\\n/tfx.components.trainer.executor.GenericExecutor\\n\\251\\002\\n\\014TrainDataGen\\022\\230\\002\\nHtype.googleapis.com/tfx.orchestration.executable_spec.BeamExecutableSpec\\022\\313\\001\\nE\\nCtfx.extensions.google_cloud_big_query.example_gen.executor.Executor\\022\\037--project=grandelli-demo-295810\\022a--temp_location=gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/temp\\n\\241\\001\\n\\016HyperparamsGen\\022\\216\\001\\nOtype.googleapis.com/tfx.orchestration.executable_spec.PythonClassExecutableSpec\\022;\\n9src.tfx_pipelines.components.hyperparameters_gen_Executor\\n\\250\\002\\n\\013TestDataGen\\022\\230\\002\\nHtype.googleapis.com/tfx.orchestration.executable_spec.BeamExecutableSpec\\022\\313\\001\\nE\\nCtfx.extensions.google_cloud_big_query.example_gen.executor.Executor\\022\\037--project=grandelli-demo-295810\\022a--temp_location=gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/temp\\n\\223\\002\\n\\017DataTransformer\\022\\377\\001\\nHtype.googleapis.com/tfx.orchestration.executable_spec.BeamExecutableSpec\\022\\262\\001\\n,\\n*tfx.components.transform.executor.Executor\\022\\037--project=grandelli-demo-295810\\022a--temp_location=gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/temp\\n\\234\\001\\n\\020ExampleValidator\\022\\207\\001\\nOtype.googleapis.com/tfx.orchestration.executable_spec.PythonClassExecutableSpec\\0224\\n2tfx.components.example_validator.executor.Executor\\022\\230\\001\\n\\014TrainDataGen\\022\\207\\001\\nOtype.googleapis.com/tfx.orchestration.executable_spec.PythonClassExecutableSpec\\0224\\n2tfx.components.example_gen.driver.QueryBasedDriver\\022\\227\\001\\n\\013TestDataGen\\022\\207\\001\\nOtype.googleapis.com/tfx.orchestration.executable_spec.PythonClassExecutableSpec\\0224\\n2tfx.components.example_gen.driver.QueryBasedDriver*F\\n0type.googleapis.com/ml_metadata.ConnectionConfig\\022\\022\\032\\020\\n\\014mlmd.sqllite\\020\\003\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": Using deployment config:\n",
      "Step #3 - \"Local Test E2E Pipeline\":  executor_specs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"DataTransformer\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     beam_executable_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       python_executor_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         class_path: \"tfx.components.transform.executor.Executor\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       beam_pipeline_args: \"--project=grandelli-demo-295810\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       beam_pipeline_args: \"--temp_location=gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/temp\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": executor_specs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"ExampleValidator\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     python_class_executable_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       class_path: \"tfx.components.example_validator.executor.Executor\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": executor_specs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"HyperparamsGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     python_class_executable_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       class_path: \"src.tfx_pipelines.components.hyperparameters_gen_Executor\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": executor_specs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"ModelEvaluator\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     beam_executable_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       python_executor_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         class_path: \"tfx.components.evaluator.executor.Executor\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       beam_pipeline_args: \"--project=grandelli-demo-295810\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       beam_pipeline_args: \"--temp_location=gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/temp\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": executor_specs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"ModelPusher\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     python_class_executable_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       class_path: \"tfx.components.pusher.executor.Executor\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": executor_specs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"ModelTrainer\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     python_class_executable_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       class_path: \"tfx.components.trainer.executor.GenericExecutor\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": executor_specs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"StatisticsGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     beam_executable_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       python_executor_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         class_path: \"tfx.components.statistics_gen.executor.Executor\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       beam_pipeline_args: \"--project=grandelli-demo-295810\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       beam_pipeline_args: \"--temp_location=gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/temp\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": executor_specs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"TestDataGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     beam_executable_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       python_executor_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         class_path: \"tfx.extensions.google_cloud_big_query.example_gen.executor.Executor\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       beam_pipeline_args: \"--project=grandelli-demo-295810\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       beam_pipeline_args: \"--temp_location=gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/temp\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": executor_specs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"TrainDataGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     beam_executable_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       python_executor_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         class_path: \"tfx.extensions.google_cloud_big_query.example_gen.executor.Executor\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       beam_pipeline_args: \"--project=grandelli-demo-295810\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       beam_pipeline_args: \"--temp_location=gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/temp\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_driver_specs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"TestDataGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     python_class_executable_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       class_path: \"tfx.components.example_gen.driver.QueryBasedDriver\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_driver_specs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"TrainDataGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     python_class_executable_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       class_path: \"tfx.components.example_gen.driver.QueryBasedDriver\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": metadata_connection_config {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   sqlite {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     filename_uri: \"mlmd.sqllite\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     connection_mode: READWRITE_OPENCREATE\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": Using connection config:\n",
      "Step #3 - \"Local Test E2E Pipeline\":  sqlite {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   filename_uri: \"mlmd.sqllite\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   connection_mode: READWRITE_OPENCREATE\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": Component BaselineModelResolver is running.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running launcher for node_info {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name: \"tfx.dsl.components.common.resolver.Resolver\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   id: \"BaselineModelResolver\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"2022-03-30T17:18:26.717519\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.BaselineModelResolver\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"model\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       channels {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         artifact_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"Model\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"model_blessing\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       channels {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         artifact_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"ModelBlessing\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   resolver_config {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     resolver_steps {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       class_path: \"tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy.LatestBlessedModelStrategy\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       config_json: \"{}\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       input_keys: \"model\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       input_keys: \"model_blessing\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": downstream_nodes: \"ModelEvaluator\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": execution_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   caching_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": Running as an resolver node.\n",
      "Step #3 - \"Local Test E2E Pipeline\": MetadataStore with DB connection initialized\n",
      "Step #3 - \"Local Test E2E Pipeline\": WARNING: Logging before InitGoogleLogging() is written to STDERR\n",
      "Step #3 - \"Local Test E2E Pipeline\": I0330 17:18:26.750528     1 rdbms_metadata_access_object.cc:686] No property is defined for the Type\n",
      "Step #3 - \"Local Test E2E Pipeline\": I0330 17:18:26.761181     1 rdbms_metadata_access_object.cc:686] No property is defined for the Type\n",
      "Step #3 - \"Local Test E2E Pipeline\": I0330 17:18:26.771981     1 rdbms_metadata_access_object.cc:686] No property is defined for the Type\n",
      "Step #3 - \"Local Test E2E Pipeline\": Artifact type Model is not found in MLMD.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Artifact type ModelBlessing is not found in MLMD.\n",
      "Step #3 - \"Local Test E2E Pipeline\": I0330 17:18:26.784731     1 rdbms_metadata_access_object.cc:686] No property is defined for the Type\n",
      "Step #3 - \"Local Test E2E Pipeline\": Component BaselineModelResolver is finished.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Component HyperparamsGen is running.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running launcher for node_info {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name: \"src.tfx_pipelines.components.hyperparameters_gen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   id: \"HyperparamsGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"2022-03-30T17:18:26.717519\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.HyperparamsGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"hyperparameters\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"HyperParameters\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"batch_size\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         int_value: 512\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"hidden_units\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"128,128\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"learning_rate\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         double_value: 0.001\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"num_epochs\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         int_value: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": downstream_nodes: \"ModelTrainer\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": execution_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   caching_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": MetadataStore with DB connection initialized\n",
      "Step #3 - \"Local Test E2E Pipeline\": I0330 17:18:26.812721     1 rdbms_metadata_access_object.cc:686] No property is defined for the Type\n",
      "Step #3 - \"Local Test E2E Pipeline\": MetadataStore with DB connection initialized\n",
      "Step #3 - \"Local Test E2E Pipeline\": I0330 17:18:26.829080     1 rdbms_metadata_access_object.cc:686] No property is defined for the Type\n",
      "Step #3 - \"Local Test E2E Pipeline\": Going to run a new execution 2\n",
      "Step #3 - \"Local Test E2E Pipeline\": Going to run a new execution: ExecutionInfo(execution_id=2, input_dict={}, output_dict=defaultdict(<class 'list'>, {'hyperparameters': [Artifact(artifact: uri: \"gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/HyperparamsGen/hyperparameters/2\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"name\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline:2022-03-30T17:18:26.717519:HyperparamsGen:hyperparameters:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": , artifact_type: name: \"HyperParameters\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": )]}), exec_properties={'batch_size': 512, 'learning_rate': 0.001, 'num_epochs': 1, 'hidden_units': '128,128'}, execution_output_uri='gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/HyperparamsGen/.system/executor_execution/2/executor_output.pb', stateful_working_dir='gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/HyperparamsGen/.system/stateful_working_dir/2022-03-30T17:18:26.717519', tmp_dir='gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/HyperparamsGen/.system/executor_execution/2/.temp/', pipeline_node=node_info {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name: \"src.tfx_pipelines.components.hyperparameters_gen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   id: \"HyperparamsGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"2022-03-30T17:18:26.717519\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.HyperparamsGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"hyperparameters\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"HyperParameters\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"batch_size\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         int_value: 512\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"hidden_units\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"128,128\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"learning_rate\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         double_value: 0.001\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"num_epochs\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         int_value: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": downstream_nodes: \"ModelTrainer\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": execution_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   caching_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": , pipeline_info=id: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": , pipeline_run_id='2022-03-30T17:18:26.717519')\n",
      "Step #3 - \"Local Test E2E Pipeline\": Hyperparameters: {'num_epochs': 1, 'batch_size': 512, 'learning_rate': 0.001, 'hidden_units': [128, 128]}\n",
      "Step #3 - \"Local Test E2E Pipeline\": Hyperparameters are written to: gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/HyperparamsGen/hyperparameters/2/hyperparameters.json\n",
      "Step #3 - \"Local Test E2E Pipeline\": Cleaning up stateless execution info.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Execution 2 succeeded.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Cleaning up stateful execution info.\n",
      "Step #3 - \"Local Test E2E Pipeline\": stateful_working_dir /workspace/mlops-with-vertex-ai/gs:/grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/HyperparamsGen/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Publishing output artifacts defaultdict(<class 'list'>, {'hyperparameters': [Artifact(artifact: uri: \"gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/HyperparamsGen/hyperparameters/2\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"name\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline:2022-03-30T17:18:26.717519:HyperparamsGen:hyperparameters:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"tfx_version\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"1.2.0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": , artifact_type: name: \"HyperParameters\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": )]}) for execution 2\n",
      "Step #3 - \"Local Test E2E Pipeline\": MetadataStore with DB connection initialized\n",
      "Step #3 - \"Local Test E2E Pipeline\": I0330 17:18:32.842676     1 rdbms_metadata_access_object.cc:686] No property is defined for the Type\n",
      "Step #3 - \"Local Test E2E Pipeline\": Component HyperparamsGen is finished.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Component SchemaImporter is running.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running launcher for node_info {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name: \"tfx.dsl.components.common.importer.Importer\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   id: \"SchemaImporter\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"2022-03-30T17:18:26.717519\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.SchemaImporter\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"result\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"Schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"artifact_uri\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"src/raw_schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"reimport\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         int_value: 0\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": downstream_nodes: \"DataTransformer\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": downstream_nodes: \"ExampleValidator\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": downstream_nodes: \"ModelEvaluator\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": downstream_nodes: \"ModelTrainer\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": execution_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   caching_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": Running as an importer node.\n",
      "Step #3 - \"Local Test E2E Pipeline\": MetadataStore with DB connection initialized\n",
      "Step #3 - \"Local Test E2E Pipeline\": I0330 17:18:32.867341     1 rdbms_metadata_access_object.cc:686] No property is defined for the Type\n",
      "Step #3 - \"Local Test E2E Pipeline\": Processing source uri: src/raw_schema, properties: {}, custom_properties: {}\n",
      "Step #3 - \"Local Test E2E Pipeline\": I0330 17:18:32.881383     1 rdbms_metadata_access_object.cc:686] No property is defined for the Type\n",
      "Step #3 - \"Local Test E2E Pipeline\": Component SchemaImporter is finished.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Component TestDataGen is running.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running launcher for node_info {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name: \"tfx.extensions.google_cloud_big_query.example_gen.component.BigQueryExampleGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   id: \"TestDataGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"2022-03-30T17:18:26.717519\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.TestDataGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"Examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"version\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"input_config\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"\\\\n    SELECT \\\\n        IF(trip_month IS NULL, -1, trip_month) trip_month,\\\\n        IF(trip_day IS NULL, -1, trip_day) trip_day,\\\\n        IF(trip_day_of_week IS NULL, -1, trip_day_of_week) trip_day_of_week,\\\\n        IF(trip_hour IS NULL, -1, trip_hour) trip_hour,\\\\n        IF(trip_seconds IS NULL, -1, trip_seconds) trip_seconds,\\\\n        IF(trip_miles IS NULL, -1, trip_miles) trip_miles,\\\\n        IF(payment_type IS NULL, \\'NA\\', payment_type) payment_type,\\\\n        IF(pickup_grid IS NULL, \\'NA\\', pickup_grid) pickup_grid,\\\\n        IF(dropoff_grid IS NULL, \\'NA\\', dropoff_grid) dropoff_grid,\\\\n        IF(euclidean IS NULL, -1, euclidean) euclidean,\\\\n        IF(loc_cross IS NULL, \\'NA\\', loc_cross) loc_cross,\\\\n        tip_bin\\\\n    FROM partner_training.chicago_taxitrips_prep \\\\n    WHERE ML_use = \\'TEST\\'\\\\n    LIMIT 100\\\"\\n    }\\n  ]\\n}\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"output_config\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"test\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"output_data_format\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         int_value: 6\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"output_file_format\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         int_value: 5\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": downstream_nodes: \"ModelEvaluator\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": execution_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   caching_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": MetadataStore with DB connection initialized\n",
      "Step #3 - \"Local Test E2E Pipeline\": I0330 17:18:32.905162     1 rdbms_metadata_access_object.cc:686] No property is defined for the Type\n",
      "Step #3 - \"Local Test E2E Pipeline\": MetadataStore with DB connection initialized\n",
      "Step #3 - \"Local Test E2E Pipeline\": Going to run a new execution 4\n",
      "Step #3 - \"Local Test E2E Pipeline\": Going to run a new execution: ExecutionInfo(execution_id=4, input_dict={}, output_dict=defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/TestDataGen/examples/4\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"name\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline:2022-03-30T17:18:26.717519:TestDataGen:examples:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     int_value: 0\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": , artifact_type: name: \"Examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"version\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": )]}), exec_properties={'output_config': '{\\n  \"split_config\": {\\n    \"splits\": [\\n      {\\n        \"hash_buckets\": 1,\\n        \"name\": \"test\"\\n      }\\n    ]\\n  }\\n}', 'input_config': '{\\n  \"splits\": [\\n    {\\n      \"name\": \"single_split\",\\n      \"pattern\": \"\\\\n    SELECT \\\\n        IF(trip_month IS NULL, -1, trip_month) trip_month,\\\\n        IF(trip_day IS NULL, -1, trip_day) trip_day,\\\\n        IF(trip_day_of_week IS NULL, -1, trip_day_of_week) trip_day_of_week,\\\\n        IF(trip_hour IS NULL, -1, trip_hour) trip_hour,\\\\n        IF(trip_seconds IS NULL, -1, trip_seconds) trip_seconds,\\\\n        IF(trip_miles IS NULL, -1, trip_miles) trip_miles,\\\\n        IF(payment_type IS NULL, \\'NA\\', payment_type) payment_type,\\\\n        IF(pickup_grid IS NULL, \\'NA\\', pickup_grid) pickup_grid,\\\\n        IF(dropoff_grid IS NULL, \\'NA\\', dropoff_grid) dropoff_grid,\\\\n        IF(euclidean IS NULL, -1, euclidean) euclidean,\\\\n        IF(loc_cross IS NULL, \\'NA\\', loc_cross) loc_cross,\\\\n        tip_bin\\\\n    FROM partner_training.chicago_taxitrips_prep \\\\n    WHERE ML_use = \\'TEST\\'\\\\n    LIMIT 100\"\\n    }\\n  ]\\n}', 'output_data_format': 6, 'output_file_format': 5, 'span': 0, 'version': None, 'input_fingerprint': None}, execution_output_uri='gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/TestDataGen/.system/executor_execution/4/executor_output.pb', stateful_working_dir='gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/TestDataGen/.system/stateful_working_dir/2022-03-30T17:18:26.717519', tmp_dir='gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/TestDataGen/.system/executor_execution/4/.temp/', pipeline_node=node_info {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name: \"tfx.extensions.google_cloud_big_query.example_gen.component.BigQueryExampleGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   id: \"TestDataGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"2022-03-30T17:18:26.717519\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.TestDataGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"Examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"version\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"input_config\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"\\\\n    SELECT \\\\n        IF(trip_month IS NULL, -1, trip_month) trip_month,\\\\n        IF(trip_day IS NULL, -1, trip_day) trip_day,\\\\n        IF(trip_day_of_week IS NULL, -1, trip_day_of_week) trip_day_of_week,\\\\n        IF(trip_hour IS NULL, -1, trip_hour) trip_hour,\\\\n        IF(trip_seconds IS NULL, -1, trip_seconds) trip_seconds,\\\\n        IF(trip_miles IS NULL, -1, trip_miles) trip_miles,\\\\n        IF(payment_type IS NULL, \\'NA\\', payment_type) payment_type,\\\\n        IF(pickup_grid IS NULL, \\'NA\\', pickup_grid) pickup_grid,\\\\n        IF(dropoff_grid IS NULL, \\'NA\\', dropoff_grid) dropoff_grid,\\\\n        IF(euclidean IS NULL, -1, euclidean) euclidean,\\\\n        IF(loc_cross IS NULL, \\'NA\\', loc_cross) loc_cross,\\\\n        tip_bin\\\\n    FROM partner_training.chicago_taxitrips_prep \\\\n    WHERE ML_use = \\'TEST\\'\\\\n    LIMIT 100\\\"\\n    }\\n  ]\\n}\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"output_config\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"test\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"output_data_format\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         int_value: 6\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"output_file_format\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         int_value: 5\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": downstream_nodes: \"ModelEvaluator\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": execution_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   caching_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": , pipeline_info=id: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": , pipeline_run_id='2022-03-30T17:18:26.717519')\n",
      "Step #3 - \"Local Test E2E Pipeline\": Attempting to infer TFX Python dependency for beam\n",
      "Step #3 - \"Local Test E2E Pipeline\": Copying all content from install dir /opt/conda/lib/python3.7/site-packages/tfx to temp dir /tmp/tmp2asskb3t/build/tfx\n",
      "Step #3 - \"Local Test E2E Pipeline\": Generating a temp setup file at /tmp/tmp2asskb3t/build/tfx/setup.py\n",
      "Step #3 - \"Local Test E2E Pipeline\": Creating temporary sdist package, logs available at /tmp/tmp2asskb3t/build/tfx/setup.log\n",
      "Step #3 - \"Local Test E2E Pipeline\": E0330 17:18:40.294737472       1 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "Step #3 - \"Local Test E2E Pipeline\": Added --extra_package=/tmp/tmp2asskb3t/build/tfx/dist/tfx_ephemeral-1.2.0.tar.gz to beam args\n",
      "Step #3 - \"Local Test E2E Pipeline\": Length of label `tfx-extensions-google_cloud_big_query-example_gen-executor-executor` exceeds maximum length(63), trimmed.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Generating examples.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Default Python SDK image for environment is apache/beam_python3.7_sdk:2.31.0\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function annotate_downstream_side_inputs at 0x7f5e1692f8c0> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function fix_side_input_pcoll_coders at 0x7f5e1692f9e0> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function pack_combiners at 0x7f5e1692fe60> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function lift_combiners at 0x7f5e1692fef0> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function expand_sdf at 0x7f5e169380e0> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function expand_gbk at 0x7f5e16938170> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function sink_flattens at 0x7f5e16938290> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function greedily_fuse at 0x7f5e16938320> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function read_to_impulse at 0x7f5e169383b0> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function impulse_to_input at 0x7f5e16938440> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function sort_stages at 0x7f5e16938680> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function setup_timer_mapping at 0x7f5e169385f0> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function populate_data_channel_coders at 0x7f5e16938710> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": Creating state cache with size 100\n",
      "Step #3 - \"Local Test E2E Pipeline\": Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7f5e13476990> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-Read-Impulse_12)+(ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-Read-Map-lambda-at-iobase-py-894-_13))+(InputToRecord/QueryTable/ReadFromBigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(InputToRecord/QueryTable/ReadFromBigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Setting socket default timeout to 60 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": socket default timeout is 60.0 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Attempting refresh to obtain initial access_token\n",
      "Step #3 - \"Local Test E2E Pipeline\": Stated BigQuery job: <JobReference\n",
      "Step #3 - \"Local Test E2E Pipeline\":  location: 'US'\n",
      "Step #3 - \"Local Test E2E Pipeline\":  projectId: 'grandelli-demo-295810'>\n",
      "Step #3 - \"Local Test E2E Pipeline\":  bq show -j --format=prettyjson --project_id=grandelli-demo-295810 None\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using location 'US' from table <TableReference\n",
      "Step #3 - \"Local Test E2E Pipeline\":  datasetId: 'partner_training'\n",
      "Step #3 - \"Local Test E2E Pipeline\":  projectId: 'grandelli-demo-295810'\n",
      "Step #3 - \"Local Test E2E Pipeline\":  tableId: 'chicago_taxitrips_prep'> referenced by query \n",
      "Step #3 - \"Local Test E2E Pipeline\":     SELECT \n",
      "Step #3 - \"Local Test E2E Pipeline\":         IF(trip_month IS NULL, -1, trip_month) trip_month,\n",
      "Step #3 - \"Local Test E2E Pipeline\":         IF(trip_day IS NULL, -1, trip_day) trip_day,\n",
      "Step #3 - \"Local Test E2E Pipeline\":         IF(trip_day_of_week IS NULL, -1, trip_day_of_week) trip_day_of_week,\n",
      "Step #3 - \"Local Test E2E Pipeline\":         IF(trip_hour IS NULL, -1, trip_hour) trip_hour,\n",
      "Step #3 - \"Local Test E2E Pipeline\":         IF(trip_seconds IS NULL, -1, trip_seconds) trip_seconds,\n",
      "Step #3 - \"Local Test E2E Pipeline\":         IF(trip_miles IS NULL, -1, trip_miles) trip_miles,\n",
      "Step #3 - \"Local Test E2E Pipeline\":         IF(payment_type IS NULL, 'NA', payment_type) payment_type,\n",
      "Step #3 - \"Local Test E2E Pipeline\":         IF(pickup_grid IS NULL, 'NA', pickup_grid) pickup_grid,\n",
      "Step #3 - \"Local Test E2E Pipeline\":         IF(dropoff_grid IS NULL, 'NA', dropoff_grid) dropoff_grid,\n",
      "Step #3 - \"Local Test E2E Pipeline\":         IF(euclidean IS NULL, -1, euclidean) euclidean,\n",
      "Step #3 - \"Local Test E2E Pipeline\":         IF(loc_cross IS NULL, 'NA', loc_cross) loc_cross,\n",
      "Step #3 - \"Local Test E2E Pipeline\":         tip_bin\n",
      "Step #3 - \"Local Test E2E Pipeline\":     FROM partner_training.chicago_taxitrips_prep \n",
      "Step #3 - \"Local Test E2E Pipeline\":     WHERE ML_use = 'TEST'\n",
      "Step #3 - \"Local Test E2E Pipeline\":     LIMIT 100\n",
      "Step #3 - \"Local Test E2E Pipeline\": Dataset grandelli-demo-295810:temp_dataset_385a548b68704cb398f99cafc298933c does not exist so we will create it as temporary with location=US\n",
      "Step #3 - \"Local Test E2E Pipeline\": Stated BigQuery job: <JobReference\n",
      "Step #3 - \"Local Test E2E Pipeline\":  jobId: 'beam_bq_job_QUERY_BQ_EXPORT_JOB_ca4edfe7-2_1648660724_72'\n",
      "Step #3 - \"Local Test E2E Pipeline\":  location: 'US'\n",
      "Step #3 - \"Local Test E2E Pipeline\":  projectId: 'grandelli-demo-295810'>\n",
      "Step #3 - \"Local Test E2E Pipeline\":  bq show -j --format=prettyjson --project_id=grandelli-demo-295810 beam_bq_job_QUERY_BQ_EXPORT_JOB_ca4edfe7-2_1648660724_72\n",
      "Step #3 - \"Local Test E2E Pipeline\": Job status: RUNNING\n",
      "Step #3 - \"Local Test E2E Pipeline\": Job status: DONE\n",
      "Step #3 - \"Local Test E2E Pipeline\": Stated BigQuery job: <JobReference\n",
      "Step #3 - \"Local Test E2E Pipeline\":  jobId: 'beam_bq_job_EXPORT_BQ_EXPORT_JOB_ca4edfe7-2_1648660730_85'\n",
      "Step #3 - \"Local Test E2E Pipeline\":  location: 'US'\n",
      "Step #3 - \"Local Test E2E Pipeline\":  projectId: 'grandelli-demo-295810'>\n",
      "Step #3 - \"Local Test E2E Pipeline\":  bq show -j --format=prettyjson --project_id=grandelli-demo-295810 beam_bq_job_EXPORT_BQ_EXPORT_JOB_ca4edfe7-2_1648660730_85\n",
      "Step #3 - \"Local Test E2E Pipeline\": Job status: RUNNING\n",
      "Step #3 - \"Local Test E2E Pipeline\": Job status: DONE\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.07674837112426758 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((((((ref_PCollection_PCollection_6_split/Read)+(InputToRecord/QueryTable/ReadFromBigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-_PassThroughThenCleanup-ParDo-PassTh_18))+(ref_AppliedPTransform_InputToRecord-ToTFExample_25))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_SplitData-ParDo-ApplyPartitionFnFn-ParDo-ApplyPartitionFnFn-_28))+(ref_AppliedPTransform_WriteSplit-test-MaybeSerialize_30))+(ref_AppliedPTransform_WriteSplit-test-Shuffle-AddRandomKeys_32))+(ref_AppliedPTransform_WriteSplit-test-Shuffle-ReshufflePerKey-Map-reify_timestamps-_34))+(WriteSplit[test]/Shuffle/ReshufflePerKey/GroupByKey/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_WriteSplit-test-Write-Write-WriteImpl-DoOnce-Impulse_42)+(ref_AppliedPTransform_WriteSplit-test-Write-Write-WriteImpl-DoOnce-FlatMap-lambda-at-core-py-2979-_43))+(ref_AppliedPTransform_WriteSplit-test-Write-Write-WriteImpl-DoOnce-Map-decode-_45))+(ref_AppliedPTransform_WriteSplit-test-Write-Write-WriteImpl-InitializeWrite_46))+(ref_PCollection_PCollection_25/Write))+(ref_PCollection_PCollection_26/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": E0330 17:18:57.344447434       1 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((WriteSplit[test]/Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_WriteSplit-test-Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_36))+(ref_AppliedPTransform_WriteSplit-test-Shuffle-RemoveRandomKeys_37))+(ref_AppliedPTransform_WriteSplit-test-Write-Write-WriteImpl-WindowInto-WindowIntoFn-_47))+(ref_AppliedPTransform_WriteSplit-test-Write-Write-WriteImpl-WriteBundles_48))+(ref_AppliedPTransform_WriteSplit-test-Write-Write-WriteImpl-Pair_49))+(WriteSplit[test]/Write/Write/WriteImpl/GroupByKey/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((WriteSplit[test]/Write/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteSplit-test-Write-Write-WriteImpl-Extract_51))+(ref_PCollection_PCollection_31/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_25/Read)+(ref_AppliedPTransform_WriteSplit-test-Write-Write-WriteImpl-PreFinalize_52))+(ref_PCollection_PCollection_32/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.06646394729614258 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_25/Read)+(ref_AppliedPTransform_WriteSplit-test-Write-Write-WriteImpl-FinalizeWrite_53)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.07119345664978027 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.07513761520385742 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.40 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-FilesToRemoveImpulse-Impulse_6)+(ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-FilesToRemoveImpulse-FlatMap-lambda-_7))+(ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-FilesToRemoveImpulse-Map-decode-_9))+(ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-MapFilesToRemove_10))+(ref_PCollection_PCollection_4/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-_PassThroughThenCleanup-Create-Impul_20)+(ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-_PassThroughThenCleanup-Create-FlatM_21))+(ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-_PassThroughThenCleanup-Create-Map-d_23))+(ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-_PassThroughThenCleanup-ParDo-Remove_24)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.07120513916015625 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Examples generated.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Cleaning up stateless execution info.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Execution 4 succeeded.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Cleaning up stateful execution info.\n",
      "Step #3 - \"Local Test E2E Pipeline\": stateful_working_dir /workspace/mlops-with-vertex-ai/gs:/grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/TestDataGen/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Publishing output artifacts defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/TestDataGen/examples/4\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"name\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline:2022-03-30T17:18:26.717519:TestDataGen:examples:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     int_value: 0\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"tfx_version\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"1.2.0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": , artifact_type: name: \"Examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"version\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": )]}) for execution 4\n",
      "Step #3 - \"Local Test E2E Pipeline\": MetadataStore with DB connection initialized\n",
      "Step #3 - \"Local Test E2E Pipeline\": Component TestDataGen is finished.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Component TrainDataGen is running.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running launcher for node_info {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name: \"tfx.extensions.google_cloud_big_query.example_gen.component.BigQueryExampleGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   id: \"TrainDataGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"2022-03-30T17:18:26.717519\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.TrainDataGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"Examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"version\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"input_config\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"\\\\n    SELECT \\\\n        IF(trip_month IS NULL, -1, trip_month) trip_month,\\\\n        IF(trip_day IS NULL, -1, trip_day) trip_day,\\\\n        IF(trip_day_of_week IS NULL, -1, trip_day_of_week) trip_day_of_week,\\\\n        IF(trip_hour IS NULL, -1, trip_hour) trip_hour,\\\\n        IF(trip_seconds IS NULL, -1, trip_seconds) trip_seconds,\\\\n        IF(trip_miles IS NULL, -1, trip_miles) trip_miles,\\\\n        IF(payment_type IS NULL, \\'NA\\', payment_type) payment_type,\\\\n        IF(pickup_grid IS NULL, \\'NA\\', pickup_grid) pickup_grid,\\\\n        IF(dropoff_grid IS NULL, \\'NA\\', dropoff_grid) dropoff_grid,\\\\n        IF(euclidean IS NULL, -1, euclidean) euclidean,\\\\n        IF(loc_cross IS NULL, \\'NA\\', loc_cross) loc_cross,\\\\n        tip_bin\\\\n    FROM partner_training.chicago_taxitrips_prep \\\\n    WHERE ML_use = \\'UNASSIGNED\\'\\\\n    LIMIT 1000\\\"\\n    }\\n  ]\\n}\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"output_config\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 4,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"output_data_format\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         int_value: 6\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"output_file_format\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         int_value: 5\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": downstream_nodes: \"DataTransformer\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": downstream_nodes: \"StatisticsGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": execution_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   caching_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": MetadataStore with DB connection initialized\n",
      "Step #3 - \"Local Test E2E Pipeline\": MetadataStore with DB connection initialized\n",
      "Step #3 - \"Local Test E2E Pipeline\": Going to run a new execution 5\n",
      "Step #3 - \"Local Test E2E Pipeline\": Going to run a new execution: ExecutionInfo(execution_id=5, input_dict={}, output_dict=defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/TrainDataGen/examples/5\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"name\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline:2022-03-30T17:18:26.717519:TrainDataGen:examples:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     int_value: 0\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": , artifact_type: name: \"Examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"version\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": )]}), exec_properties={'output_data_format': 6, 'output_config': '{\\n  \"split_config\": {\\n    \"splits\": [\\n      {\\n        \"hash_buckets\": 4,\\n        \"name\": \"train\"\\n      },\\n      {\\n        \"hash_buckets\": 1,\\n        \"name\": \"eval\"\\n      }\\n    ]\\n  }\\n}', 'output_file_format': 5, 'input_config': '{\\n  \"splits\": [\\n    {\\n      \"name\": \"single_split\",\\n      \"pattern\": \"\\\\n    SELECT \\\\n        IF(trip_month IS NULL, -1, trip_month) trip_month,\\\\n        IF(trip_day IS NULL, -1, trip_day) trip_day,\\\\n        IF(trip_day_of_week IS NULL, -1, trip_day_of_week) trip_day_of_week,\\\\n        IF(trip_hour IS NULL, -1, trip_hour) trip_hour,\\\\n        IF(trip_seconds IS NULL, -1, trip_seconds) trip_seconds,\\\\n        IF(trip_miles IS NULL, -1, trip_miles) trip_miles,\\\\n        IF(payment_type IS NULL, \\'NA\\', payment_type) payment_type,\\\\n        IF(pickup_grid IS NULL, \\'NA\\', pickup_grid) pickup_grid,\\\\n        IF(dropoff_grid IS NULL, \\'NA\\', dropoff_grid) dropoff_grid,\\\\n        IF(euclidean IS NULL, -1, euclidean) euclidean,\\\\n        IF(loc_cross IS NULL, \\'NA\\', loc_cross) loc_cross,\\\\n        tip_bin\\\\n    FROM partner_training.chicago_taxitrips_prep \\\\n    WHERE ML_use = \\'UNASSIGNED\\'\\\\n    LIMIT 1000\"\\n    }\\n  ]\\n}', 'span': 0, 'version': None, 'input_fingerprint': None}, execution_output_uri='gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/TrainDataGen/.system/executor_execution/5/executor_output.pb', stateful_working_dir='gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/TrainDataGen/.system/stateful_working_dir/2022-03-30T17:18:26.717519', tmp_dir='gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/TrainDataGen/.system/executor_execution/5/.temp/', pipeline_node=node_info {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name: \"tfx.extensions.google_cloud_big_query.example_gen.component.BigQueryExampleGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   id: \"TrainDataGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"2022-03-30T17:18:26.717519\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.TrainDataGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"Examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"version\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"input_config\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"\\\\n    SELECT \\\\n        IF(trip_month IS NULL, -1, trip_month) trip_month,\\\\n        IF(trip_day IS NULL, -1, trip_day) trip_day,\\\\n        IF(trip_day_of_week IS NULL, -1, trip_day_of_week) trip_day_of_week,\\\\n        IF(trip_hour IS NULL, -1, trip_hour) trip_hour,\\\\n        IF(trip_seconds IS NULL, -1, trip_seconds) trip_seconds,\\\\n        IF(trip_miles IS NULL, -1, trip_miles) trip_miles,\\\\n        IF(payment_type IS NULL, \\'NA\\', payment_type) payment_type,\\\\n        IF(pickup_grid IS NULL, \\'NA\\', pickup_grid) pickup_grid,\\\\n        IF(dropoff_grid IS NULL, \\'NA\\', dropoff_grid) dropoff_grid,\\\\n        IF(euclidean IS NULL, -1, euclidean) euclidean,\\\\n        IF(loc_cross IS NULL, \\'NA\\', loc_cross) loc_cross,\\\\n        tip_bin\\\\n    FROM partner_training.chicago_taxitrips_prep \\\\n    WHERE ML_use = \\'UNASSIGNED\\'\\\\n    LIMIT 1000\\\"\\n    }\\n  ]\\n}\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"output_config\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 4,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"output_data_format\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         int_value: 6\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"output_file_format\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         int_value: 5\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": downstream_nodes: \"DataTransformer\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": downstream_nodes: \"StatisticsGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": execution_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   caching_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": , pipeline_info=id: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": , pipeline_run_id='2022-03-30T17:18:26.717519')\n",
      "Step #3 - \"Local Test E2E Pipeline\": Attempting to infer TFX Python dependency for beam\n",
      "Step #3 - \"Local Test E2E Pipeline\": Copying all content from install dir /opt/conda/lib/python3.7/site-packages/tfx to temp dir /tmp/tmpol53lhu8/build/tfx\n",
      "Step #3 - \"Local Test E2E Pipeline\": Generating a temp setup file at /tmp/tmpol53lhu8/build/tfx/setup.py\n",
      "Step #3 - \"Local Test E2E Pipeline\": Creating temporary sdist package, logs available at /tmp/tmpol53lhu8/build/tfx/setup.log\n",
      "Step #3 - \"Local Test E2E Pipeline\": E0330 17:19:05.533542507       1 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "Step #3 - \"Local Test E2E Pipeline\": Added --extra_package=/tmp/tmpol53lhu8/build/tfx/dist/tfx_ephemeral-1.2.0.tar.gz to beam args\n",
      "Step #3 - \"Local Test E2E Pipeline\": Length of label `tfx-extensions-google_cloud_big_query-example_gen-executor-executor` exceeds maximum length(63), trimmed.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Generating examples.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Default Python SDK image for environment is apache/beam_python3.7_sdk:2.31.0\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function annotate_downstream_side_inputs at 0x7f5e1692f8c0> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function fix_side_input_pcoll_coders at 0x7f5e1692f9e0> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function pack_combiners at 0x7f5e1692fe60> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function lift_combiners at 0x7f5e1692fef0> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function expand_sdf at 0x7f5e169380e0> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function expand_gbk at 0x7f5e16938170> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function sink_flattens at 0x7f5e16938290> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function greedily_fuse at 0x7f5e16938320> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function read_to_impulse at 0x7f5e169383b0> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function impulse_to_input at 0x7f5e16938440> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function sort_stages at 0x7f5e16938680> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function setup_timer_mapping at 0x7f5e169385f0> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function populate_data_channel_coders at 0x7f5e16938710> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": Creating state cache with size 100\n",
      "Step #3 - \"Local Test E2E Pipeline\": Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7f5e132ddc90> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-FilesToRemoveImpulse-Impulse_6)+(ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-FilesToRemoveImpulse-FlatMap-lambda-_7))+(ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-FilesToRemoveImpulse-Map-decode-_9))+(ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-MapFilesToRemove_10))+(ref_PCollection_PCollection_4/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_WriteSplit-eval-Write-Write-WriteImpl-DoOnce-Impulse_67)+(ref_AppliedPTransform_WriteSplit-eval-Write-Write-WriteImpl-DoOnce-FlatMap-lambda-at-core-py-2979-_68))+(ref_AppliedPTransform_WriteSplit-eval-Write-Write-WriteImpl-DoOnce-Map-decode-_70))+(ref_AppliedPTransform_WriteSplit-eval-Write-Write-WriteImpl-InitializeWrite_71))+(ref_PCollection_PCollection_43/Write))+(ref_PCollection_PCollection_44/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-Read-Impulse_12)+(ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-Read-Map-lambda-at-iobase-py-894-_13))+(InputToRecord/QueryTable/ReadFromBigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(InputToRecord/QueryTable/ReadFromBigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Stated BigQuery job: <JobReference\n",
      "Step #3 - \"Local Test E2E Pipeline\":  location: 'US'\n",
      "Step #3 - \"Local Test E2E Pipeline\":  projectId: 'grandelli-demo-295810'>\n",
      "Step #3 - \"Local Test E2E Pipeline\":  bq show -j --format=prettyjson --project_id=grandelli-demo-295810 None\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using location 'US' from table <TableReference\n",
      "Step #3 - \"Local Test E2E Pipeline\":  datasetId: 'partner_training'\n",
      "Step #3 - \"Local Test E2E Pipeline\":  projectId: 'grandelli-demo-295810'\n",
      "Step #3 - \"Local Test E2E Pipeline\":  tableId: 'chicago_taxitrips_prep'> referenced by query \n",
      "Step #3 - \"Local Test E2E Pipeline\":     SELECT \n",
      "Step #3 - \"Local Test E2E Pipeline\":         IF(trip_month IS NULL, -1, trip_month) trip_month,\n",
      "Step #3 - \"Local Test E2E Pipeline\":         IF(trip_day IS NULL, -1, trip_day) trip_day,\n",
      "Step #3 - \"Local Test E2E Pipeline\":         IF(trip_day_of_week IS NULL, -1, trip_day_of_week) trip_day_of_week,\n",
      "Step #3 - \"Local Test E2E Pipeline\":         IF(trip_hour IS NULL, -1, trip_hour) trip_hour,\n",
      "Step #3 - \"Local Test E2E Pipeline\":         IF(trip_seconds IS NULL, -1, trip_seconds) trip_seconds,\n",
      "Step #3 - \"Local Test E2E Pipeline\":         IF(trip_miles IS NULL, -1, trip_miles) trip_miles,\n",
      "Step #3 - \"Local Test E2E Pipeline\":         IF(payment_type IS NULL, 'NA', payment_type) payment_type,\n",
      "Step #3 - \"Local Test E2E Pipeline\":         IF(pickup_grid IS NULL, 'NA', pickup_grid) pickup_grid,\n",
      "Step #3 - \"Local Test E2E Pipeline\":         IF(dropoff_grid IS NULL, 'NA', dropoff_grid) dropoff_grid,\n",
      "Step #3 - \"Local Test E2E Pipeline\":         IF(euclidean IS NULL, -1, euclidean) euclidean,\n",
      "Step #3 - \"Local Test E2E Pipeline\":         IF(loc_cross IS NULL, 'NA', loc_cross) loc_cross,\n",
      "Step #3 - \"Local Test E2E Pipeline\":         tip_bin\n",
      "Step #3 - \"Local Test E2E Pipeline\":     FROM partner_training.chicago_taxitrips_prep \n",
      "Step #3 - \"Local Test E2E Pipeline\":     WHERE ML_use = 'UNASSIGNED'\n",
      "Step #3 - \"Local Test E2E Pipeline\":     LIMIT 1000\n",
      "Step #3 - \"Local Test E2E Pipeline\": Dataset grandelli-demo-295810:temp_dataset_cd694daa96884a53a05a2dd995d20158 does not exist so we will create it as temporary with location=US\n",
      "Step #3 - \"Local Test E2E Pipeline\": Stated BigQuery job: <JobReference\n",
      "Step #3 - \"Local Test E2E Pipeline\":  jobId: 'beam_bq_job_QUERY_BQ_EXPORT_JOB_19e0e908-7_1648660749_791'\n",
      "Step #3 - \"Local Test E2E Pipeline\":  location: 'US'\n",
      "Step #3 - \"Local Test E2E Pipeline\":  projectId: 'grandelli-demo-295810'>\n",
      "Step #3 - \"Local Test E2E Pipeline\":  bq show -j --format=prettyjson --project_id=grandelli-demo-295810 beam_bq_job_QUERY_BQ_EXPORT_JOB_19e0e908-7_1648660749_791\n",
      "Step #3 - \"Local Test E2E Pipeline\": Job status: RUNNING\n",
      "Step #3 - \"Local Test E2E Pipeline\": Job status: DONE\n",
      "Step #3 - \"Local Test E2E Pipeline\": Stated BigQuery job: <JobReference\n",
      "Step #3 - \"Local Test E2E Pipeline\":  jobId: 'beam_bq_job_EXPORT_BQ_EXPORT_JOB_19e0e908-7_1648660755_592'\n",
      "Step #3 - \"Local Test E2E Pipeline\":  location: 'US'\n",
      "Step #3 - \"Local Test E2E Pipeline\":  projectId: 'grandelli-demo-295810'>\n",
      "Step #3 - \"Local Test E2E Pipeline\":  bq show -j --format=prettyjson --project_id=grandelli-demo-295810 beam_bq_job_EXPORT_BQ_EXPORT_JOB_19e0e908-7_1648660755_592\n",
      "Step #3 - \"Local Test E2E Pipeline\": Job status: RUNNING\n",
      "Step #3 - \"Local Test E2E Pipeline\": Job status: DONE\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.09584712982177734 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((((((((((ref_PCollection_PCollection_6_split/Read)+(InputToRecord/QueryTable/ReadFromBigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-_PassThroughThenCleanup-ParDo-PassTh_18))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_InputToRecord-ToTFExample_25))+(ref_AppliedPTransform_SplitData-ParDo-ApplyPartitionFnFn-ParDo-ApplyPartitionFnFn-_28))+(ref_AppliedPTransform_WriteSplit-eval-MaybeSerialize_55))+(ref_AppliedPTransform_WriteSplit-train-MaybeSerialize_30))+(ref_AppliedPTransform_WriteSplit-train-Shuffle-AddRandomKeys_32))+(ref_AppliedPTransform_WriteSplit-train-Shuffle-ReshufflePerKey-Map-reify_timestamps-_34))+(WriteSplit[train]/Shuffle/ReshufflePerKey/GroupByKey/Write))+(ref_AppliedPTransform_WriteSplit-eval-Shuffle-AddRandomKeys_57))+(ref_AppliedPTransform_WriteSplit-eval-Shuffle-ReshufflePerKey-Map-reify_timestamps-_59))+(WriteSplit[eval]/Shuffle/ReshufflePerKey/GroupByKey/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-_PassThroughThenCleanup-Create-Impul_20)+(ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-_PassThroughThenCleanup-Create-FlatM_21))+(ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-_PassThroughThenCleanup-Create-Map-d_23))+(ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-_PassThroughThenCleanup-ParDo-Remove_24)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.13262486457824707 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((WriteSplit[eval]/Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_WriteSplit-eval-Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_61))+(ref_AppliedPTransform_WriteSplit-eval-Shuffle-RemoveRandomKeys_62))+(ref_AppliedPTransform_WriteSplit-eval-Write-Write-WriteImpl-WindowInto-WindowIntoFn-_72))+(ref_AppliedPTransform_WriteSplit-eval-Write-Write-WriteImpl-WriteBundles_73))+(ref_AppliedPTransform_WriteSplit-eval-Write-Write-WriteImpl-Pair_74))+(WriteSplit[eval]/Write/Write/WriteImpl/GroupByKey/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((WriteSplit[eval]/Write/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteSplit-eval-Write-Write-WriteImpl-Extract_76))+(ref_PCollection_PCollection_49/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_43/Read)+(ref_AppliedPTransform_WriteSplit-eval-Write-Write-WriteImpl-PreFinalize_77))+(ref_PCollection_PCollection_50/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.08266758918762207 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_WriteSplit-train-Write-Write-WriteImpl-DoOnce-Impulse_42)+(ref_AppliedPTransform_WriteSplit-train-Write-Write-WriteImpl-DoOnce-FlatMap-lambda-at-core-py-2979-_43))+(ref_AppliedPTransform_WriteSplit-train-Write-Write-WriteImpl-DoOnce-Map-decode-_45))+(ref_AppliedPTransform_WriteSplit-train-Write-Write-WriteImpl-InitializeWrite_46))+(ref_PCollection_PCollection_26/Write))+(ref_PCollection_PCollection_27/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((WriteSplit[train]/Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_WriteSplit-train-Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_36))+(ref_AppliedPTransform_WriteSplit-train-Shuffle-RemoveRandomKeys_37))+(ref_AppliedPTransform_WriteSplit-train-Write-Write-WriteImpl-WindowInto-WindowIntoFn-_47))+(ref_AppliedPTransform_WriteSplit-train-Write-Write-WriteImpl-WriteBundles_48))+(ref_AppliedPTransform_WriteSplit-train-Write-Write-WriteImpl-Pair_49))+(WriteSplit[train]/Write/Write/WriteImpl/GroupByKey/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((WriteSplit[train]/Write/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteSplit-train-Write-Write-WriteImpl-Extract_51))+(ref_PCollection_PCollection_32/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_26/Read)+(ref_AppliedPTransform_WriteSplit-train-Write-Write-WriteImpl-PreFinalize_52))+(ref_PCollection_PCollection_33/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.07532858848571777 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_43/Read)+(ref_AppliedPTransform_WriteSplit-eval-Write-Write-WriteImpl-FinalizeWrite_78)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.07910394668579102 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.07355213165283203 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.50 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_26/Read)+(ref_AppliedPTransform_WriteSplit-train-Write-Write-WriteImpl-FinalizeWrite_53)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.07288861274719238 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.07456564903259277 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.50 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Examples generated.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Cleaning up stateless execution info.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Execution 5 succeeded.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Cleaning up stateful execution info.\n",
      "Step #3 - \"Local Test E2E Pipeline\": stateful_working_dir /workspace/mlops-with-vertex-ai/gs:/grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/TrainDataGen/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Publishing output artifacts defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/TrainDataGen/examples/5\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"name\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline:2022-03-30T17:18:26.717519:TrainDataGen:examples:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     int_value: 0\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"tfx_version\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"1.2.0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": , artifact_type: name: \"Examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"version\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": )]}) for execution 5\n",
      "Step #3 - \"Local Test E2E Pipeline\": MetadataStore with DB connection initialized\n",
      "Step #3 - \"Local Test E2E Pipeline\": Component TrainDataGen is finished.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Component WarmstartModelResolver is running.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running launcher for node_info {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name: \"tfx.dsl.components.common.resolver.Resolver\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   id: \"WarmstartModelResolver\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"2022-03-30T17:18:26.717519\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.WarmstartModelResolver\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"latest_model\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       channels {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         artifact_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"Model\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   resolver_config {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     resolver_steps {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       class_path: \"tfx.dsl.input_resolution.strategies.latest_artifact_strategy.LatestArtifactStrategy\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       config_json: \"{}\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       input_keys: \"latest_model\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": downstream_nodes: \"ModelTrainer\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": execution_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   caching_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": Running as an resolver node.\n",
      "Step #3 - \"Local Test E2E Pipeline\": MetadataStore with DB connection initialized\n",
      "Step #3 - \"Local Test E2E Pipeline\": Artifact type Model is not found in MLMD.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Component WarmstartModelResolver is finished.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Component StatisticsGen is running.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running launcher for node_info {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name: \"tfx.components.statistics_gen.component.StatisticsGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   id: \"StatisticsGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"2022-03-30T17:18:26.717519\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.StatisticsGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       channels {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         producer_node_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           id: \"TrainDataGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"2022-03-30T17:18:26.717519\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.TrainDataGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         artifact_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"Examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         output_key: \"examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"statistics\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"ExampleStatistics\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"exclude_splits\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"[]\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": upstream_nodes: \"TrainDataGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": downstream_nodes: \"ExampleValidator\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": execution_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   caching_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": MetadataStore with DB connection initialized\n",
      "Step #3 - \"Local Test E2E Pipeline\": I0330 17:19:25.576103     1 rdbms_metadata_access_object.cc:686] No property is defined for the Type\n",
      "Step #3 - \"Local Test E2E Pipeline\": MetadataStore with DB connection initialized\n",
      "Step #3 - \"Local Test E2E Pipeline\": Going to run a new execution 7\n",
      "Step #3 - \"Local Test E2E Pipeline\": Going to run a new execution: ExecutionInfo(execution_id=7, input_dict={'examples': [Artifact(artifact: id: 4\n",
      "Step #3 - \"Local Test E2E Pipeline\": type_id: 20\n",
      "Step #3 - \"Local Test E2E Pipeline\": uri: \"gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/TrainDataGen/examples/5\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"file_format\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"tfrecords_gzip\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"name\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline:2022-03-30T17:18:26.717519:TrainDataGen:examples:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"payload_format\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"FORMAT_TF_EXAMPLE\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     int_value: 0\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"tfx_version\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"1.2.0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": state: LIVE\n",
      "Step #3 - \"Local Test E2E Pipeline\": create_time_since_epoch: 1648660765535\n",
      "Step #3 - \"Local Test E2E Pipeline\": last_update_time_since_epoch: 1648660765535\n",
      "Step #3 - \"Local Test E2E Pipeline\": , artifact_type: id: 20\n",
      "Step #3 - \"Local Test E2E Pipeline\": name: \"Examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"version\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": )]}, output_dict=defaultdict(<class 'list'>, {'statistics': [Artifact(artifact: uri: \"gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/StatisticsGen/statistics/7\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"name\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline:2022-03-30T17:18:26.717519:StatisticsGen:statistics:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": , artifact_type: name: \"ExampleStatistics\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": )]}), exec_properties={'exclude_splits': '[]'}, execution_output_uri='gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/StatisticsGen/.system/executor_execution/7/executor_output.pb', stateful_working_dir='gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/StatisticsGen/.system/stateful_working_dir/2022-03-30T17:18:26.717519', tmp_dir='gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/StatisticsGen/.system/executor_execution/7/.temp/', pipeline_node=node_info {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name: \"tfx.components.statistics_gen.component.StatisticsGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   id: \"StatisticsGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"2022-03-30T17:18:26.717519\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.StatisticsGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       channels {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         producer_node_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           id: \"TrainDataGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"2022-03-30T17:18:26.717519\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.TrainDataGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         artifact_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"Examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         output_key: \"examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"statistics\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"ExampleStatistics\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"exclude_splits\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"[]\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": upstream_nodes: \"TrainDataGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": downstream_nodes: \"ExampleValidator\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": execution_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   caching_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": , pipeline_info=id: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": , pipeline_run_id='2022-03-30T17:18:26.717519')\n",
      "Step #3 - \"Local Test E2E Pipeline\": Attempting to infer TFX Python dependency for beam\n",
      "Step #3 - \"Local Test E2E Pipeline\": Copying all content from install dir /opt/conda/lib/python3.7/site-packages/tfx to temp dir /tmp/tmpb988rix6/build/tfx\n",
      "Step #3 - \"Local Test E2E Pipeline\": Generating a temp setup file at /tmp/tmpb988rix6/build/tfx/setup.py\n",
      "Step #3 - \"Local Test E2E Pipeline\": Creating temporary sdist package, logs available at /tmp/tmpb988rix6/build/tfx/setup.log\n",
      "Step #3 - \"Local Test E2E Pipeline\": E0330 17:19:30.674004826       1 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "Step #3 - \"Local Test E2E Pipeline\": Added --extra_package=/tmp/tmpb988rix6/build/tfx/dist/tfx_ephemeral-1.2.0.tar.gz to beam args\n",
      "Step #3 - \"Local Test E2E Pipeline\": Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Generating statistics for split train.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.07522797584533691 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Statistics for split train written to gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/StatisticsGen/statistics/7/Split-train.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Generating statistics for split eval.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.07393956184387207 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Statistics for split eval written to gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/StatisticsGen/statistics/7/Split-eval.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Default Python SDK image for environment is apache/beam_python3.7_sdk:2.31.0\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function annotate_downstream_side_inputs at 0x7f5e1692f8c0> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function fix_side_input_pcoll_coders at 0x7f5e1692f9e0> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function pack_combiners at 0x7f5e1692fe60> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function lift_combiners at 0x7f5e1692fef0> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function expand_sdf at 0x7f5e169380e0> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function expand_gbk at 0x7f5e16938170> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function sink_flattens at 0x7f5e16938290> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function greedily_fuse at 0x7f5e16938320> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function read_to_impulse at 0x7f5e169383b0> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function impulse_to_input at 0x7f5e16938440> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function sort_stages at 0x7f5e16938680> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function setup_timer_mapping at 0x7f5e169385f0> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function populate_data_channel_coders at 0x7f5e16938710> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": Creating state cache with size 100\n",
      "Step #3 - \"Local Test E2E Pipeline\": Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7f5e11fb70d0> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((ref_AppliedPTransform_TFXIORead-eval-RawRecordBeamSource-ReadRawRecords-ReadFromTFRecord-0-Read-Impu_106)+(ref_AppliedPTransform_TFXIORead-eval-RawRecordBeamSource-ReadRawRecords-ReadFromTFRecord-0-Read-Map-_107))+(TFXIORead[eval]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(TFXIORead[eval]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_58_split/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.07213020324707031 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.07716703414916992 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((((((((((((ref_PCollection_PCollection_58_split/Read)+(TFXIORead[eval]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_TFXIORead-eval-RawRecordBeamSource-ReadRawRecords-FlattenPCollsFromPatterns_110))+(ref_AppliedPTransform_TFXIORead-eval-RawRecordBeamSource-CollectRawRecordTelemetry-ProfileRawRecords_112))+(ref_AppliedPTransform_TFXIORead-eval-RawRecordToRecordBatch-RawRecordToRecordBatch-Batch-ParDo-_Glob_116))+(ref_AppliedPTransform_TFXIORead-eval-RawRecordToRecordBatch-RawRecordToRecordBatch-Decode_117))+(ref_AppliedPTransform_TFXIORead-eval-RawRecordToRecordBatch-CollectRecordBatchTelemetry-ProfileRecor_119))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-KeyWithVoid_122))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-TopKUn_125))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-RunCom_150))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/Precombine))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/Group/Write))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-RunCom_151))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/Flatten/Transcode/0))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PreCombineFn)/Precombine))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PreCombineFn)/Group/Write))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/Flatten/Write/0)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PreCombineFn)/Group/Read)+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PreCombineFn)/Merge))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PreCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-RunCom_156))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-RunCom_157))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/Flatten/Transcode/1))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/Flatten/Write/1)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/Flatten/Read)+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PostCombineFn)/Precombine))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PostCombineFn)/Group/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PostCombineFn)/Group/Read)+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PostCombineFn)/Merge))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PostCombineFn)/ExtractOutputs))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Transcode/1))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Write/1)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((((((GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/Group/Read)+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/Merge))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-TopKUn_130))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/Precombine))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-TopKUn_139))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/Group/Write))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-TopKUn_141))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Precombine))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Group/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Group/Read)+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Merge))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-TopKUn_146))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesFeatureStatsProtos/Write/1)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/Group/Read)+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/Merge))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-TopKUn_136))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-TopKUn_137))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesFeatureStatsProtos/Write/0)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesFeatureStatsProtos/Read)+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Transcode/0))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Write/0)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Read)+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Precombine))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Group/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((ref_AppliedPTransform_TFXIORead-train-RawRecordBeamSource-ReadRawRecords-ReadFromTFRecord-0-Read-Imp_7)+(ref_AppliedPTransform_TFXIORead-train-RawRecordBeamSource-ReadRawRecords-ReadFromTFRecord-0-Read-Map_8))+(TFXIORead[train]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(TFXIORead[train]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_2_split/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.07573127746582031 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.07466292381286621 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((((((((((((ref_PCollection_PCollection_2_split/Read)+(TFXIORead[train]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_TFXIORead-train-RawRecordBeamSource-ReadRawRecords-FlattenPCollsFromPatterns_11))+(ref_AppliedPTransform_TFXIORead-train-RawRecordBeamSource-CollectRawRecordTelemetry-ProfileRawRecord_13))+(ref_AppliedPTransform_TFXIORead-train-RawRecordToRecordBatch-RawRecordToRecordBatch-Batch-ParDo-_Glo_17))+(ref_AppliedPTransform_TFXIORead-train-RawRecordToRecordBatch-RawRecordToRecordBatch-Decode_18))+(ref_AppliedPTransform_TFXIORead-train-RawRecordToRecordBatch-CollectRecordBatchTelemetry-ProfileReco_20))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-KeyWithVoid_23))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-TopKU_26))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-RunCo_51))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/Precombine))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/Group/Write))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-RunCo_52))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/Flatten/Transcode/0))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PreCombineFn)/Precombine))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PreCombineFn)/Group/Write))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/Flatten/Write/0)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PreCombineFn)/Group/Read)+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PreCombineFn)/Merge))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PreCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-RunCo_57))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-RunCo_58))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/Flatten/Transcode/1))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/Flatten/Write/1)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/Flatten/Read)+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PostCombineFn)/Precombine))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PostCombineFn)/Group/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PostCombineFn)/Group/Read)+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PostCombineFn)/Merge))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PostCombineFn)/ExtractOutputs))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Transcode/1))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Write/1)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((((((GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/Group/Read)+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/Merge))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-TopKU_31))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/Precombine))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-TopKU_40))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/Group/Write))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-TopKU_42))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Precombine))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Group/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/Group/Read)+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/Merge))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-TopKU_37))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-TopKU_38))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesFeatureStatsProtos/Write/0)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Group/Read)+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Merge))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-TopKU_47))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesFeatureStatsProtos/Write/1)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesFeatureStatsProtos/Read)+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Transcode/0))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Write/0)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Read)+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Precombine))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Group/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Group/Read)+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Merge))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-AddSl_69))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-ToLis_72))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Precombine))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Read)+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Merge))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-ToLis_77))+(ref_PCollection_PCollection_41/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((((ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-ToLis_79)+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-ToLis_80))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-ToLis_82))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-ToLis_83))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-MakeD_84))+(ref_AppliedPTransform_WriteStatsOutput-train-WriteStats-Write-WriteImpl-Map-lambda-at-iobase-py-1126_95))+(ref_AppliedPTransform_WriteStatsOutput-train-WriteStats-Write-WriteImpl-WindowInto-WindowIntoFn-_96))+(WriteStatsOutput[train]/WriteStats/Write/WriteImpl/GroupByKey/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_WriteStatsOutput-train-WriteStats-Write-WriteImpl-DoOnce-Impulse_90)+(ref_AppliedPTransform_WriteStatsOutput-train-WriteStats-Write-WriteImpl-DoOnce-FlatMap-lambda-at-cor_91))+(ref_AppliedPTransform_WriteStatsOutput-train-WriteStats-Write-WriteImpl-DoOnce-Map-decode-_93))+(ref_AppliedPTransform_WriteStatsOutput-train-WriteStats-Write-WriteImpl-InitializeWrite_94))+(ref_PCollection_PCollection_49/Write))+(ref_PCollection_PCollection_50/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((WriteStatsOutput[train]/WriteStats/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteStatsOutput-train-WriteStats-Write-WriteImpl-WriteBundles_98))+(ref_PCollection_PCollection_54/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_49/Read)+(ref_AppliedPTransform_WriteStatsOutput-train-WriteStats-Write-WriteImpl-PreFinalize_99))+(ref_PCollection_PCollection_55/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Group/Read)+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Merge))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-AddSli_168))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-ToList_171))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Precombine))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Read)+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Merge))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-ToList_176))+(ref_PCollection_PCollection_97/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_WriteStatsOutput-eval-WriteStats-Write-WriteImpl-DoOnce-Impulse_189)+(ref_AppliedPTransform_WriteStatsOutput-eval-WriteStats-Write-WriteImpl-DoOnce-FlatMap-lambda-at-core_190))+(ref_AppliedPTransform_WriteStatsOutput-eval-WriteStats-Write-WriteImpl-DoOnce-Map-decode-_192))+(ref_AppliedPTransform_WriteStatsOutput-eval-WriteStats-Write-WriteImpl-InitializeWrite_193))+(ref_PCollection_PCollection_105/Write))+(ref_PCollection_PCollection_106/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((((ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-ToList_178)+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-ToList_179))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-ToList_181))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-ToList_182))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-MakeDa_183))+(ref_AppliedPTransform_WriteStatsOutput-eval-WriteStats-Write-WriteImpl-Map-lambda-at-iobase-py-1126-_194))+(ref_AppliedPTransform_WriteStatsOutput-eval-WriteStats-Write-WriteImpl-WindowInto-WindowIntoFn-_195))+(WriteStatsOutput[eval]/WriteStats/Write/WriteImpl/GroupByKey/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((WriteStatsOutput[eval]/WriteStats/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteStatsOutput-eval-WriteStats-Write-WriteImpl-WriteBundles_197))+(ref_PCollection_PCollection_110/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_105/Read)+(ref_AppliedPTransform_WriteStatsOutput-eval-WriteStats-Write-WriteImpl-PreFinalize_198))+(ref_PCollection_PCollection_111/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_105/Read)+(ref_AppliedPTransform_WriteStatsOutput-eval-WriteStats-Write-WriteImpl-FinalizeWrite_199)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.06975173950195312 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.50 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_49/Read)+(ref_AppliedPTransform_WriteStatsOutput-train-WriteStats-Write-WriteImpl-FinalizeWrite_100)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.07805824279785156 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.40 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Cleaning up stateless execution info.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Execution 7 succeeded.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Cleaning up stateful execution info.\n",
      "Step #3 - \"Local Test E2E Pipeline\": stateful_working_dir /workspace/mlops-with-vertex-ai/gs:/grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/StatisticsGen/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Publishing output artifacts defaultdict(<class 'list'>, {'statistics': [Artifact(artifact: uri: \"gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/StatisticsGen/statistics/7\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"name\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline:2022-03-30T17:18:26.717519:StatisticsGen:statistics:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"tfx_version\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"1.2.0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": , artifact_type: name: \"ExampleStatistics\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": )]}) for execution 7\n",
      "Step #3 - \"Local Test E2E Pipeline\": MetadataStore with DB connection initialized\n",
      "Step #3 - \"Local Test E2E Pipeline\": Component StatisticsGen is finished.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Component ExampleValidator is running.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running launcher for node_info {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name: \"tfx.components.example_validator.component.ExampleValidator\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   id: \"ExampleValidator\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"2022-03-30T17:18:26.717519\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.ExampleValidator\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       channels {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         producer_node_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           id: \"SchemaImporter\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"2022-03-30T17:18:26.717519\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.SchemaImporter\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         artifact_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"Schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         output_key: \"result\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"statistics\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       channels {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         producer_node_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           id: \"StatisticsGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"2022-03-30T17:18:26.717519\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.StatisticsGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         artifact_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"ExampleStatistics\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         output_key: \"statistics\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"anomalies\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"ExampleAnomalies\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"exclude_splits\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"[]\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": upstream_nodes: \"SchemaImporter\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": upstream_nodes: \"StatisticsGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": downstream_nodes: \"DataTransformer\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": execution_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   caching_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": MetadataStore with DB connection initialized\n",
      "Step #3 - \"Local Test E2E Pipeline\": I0330 17:19:37.605775     1 rdbms_metadata_access_object.cc:686] No property is defined for the Type\n",
      "Step #3 - \"Local Test E2E Pipeline\": MetadataStore with DB connection initialized\n",
      "Step #3 - \"Local Test E2E Pipeline\": Going to run a new execution 8\n",
      "Step #3 - \"Local Test E2E Pipeline\": Going to run a new execution: ExecutionInfo(execution_id=8, input_dict={'statistics': [Artifact(artifact: id: 5\n",
      "Step #3 - \"Local Test E2E Pipeline\": type_id: 22\n",
      "Step #3 - \"Local Test E2E Pipeline\": uri: \"gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/StatisticsGen/statistics/7\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"name\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline:2022-03-30T17:18:26.717519:StatisticsGen:statistics:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"tfx_version\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"1.2.0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": state: LIVE\n",
      "Step #3 - \"Local Test E2E Pipeline\": create_time_since_epoch: 1648660777584\n",
      "Step #3 - \"Local Test E2E Pipeline\": last_update_time_since_epoch: 1648660777584\n",
      "Step #3 - \"Local Test E2E Pipeline\": , artifact_type: id: 22\n",
      "Step #3 - \"Local Test E2E Pipeline\": name: \"ExampleStatistics\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": )], 'schema': [Artifact(artifact: id: 2\n",
      "Step #3 - \"Local Test E2E Pipeline\": type_id: 18\n",
      "Step #3 - \"Local Test E2E Pipeline\": uri: \"src/raw_schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"tfx_version\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"1.2.0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": state: LIVE\n",
      "Step #3 - \"Local Test E2E Pipeline\": create_time_since_epoch: 1648660712887\n",
      "Step #3 - \"Local Test E2E Pipeline\": last_update_time_since_epoch: 1648660712887\n",
      "Step #3 - \"Local Test E2E Pipeline\": , artifact_type: id: 18\n",
      "Step #3 - \"Local Test E2E Pipeline\": name: \"Schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": )]}, output_dict=defaultdict(<class 'list'>, {'anomalies': [Artifact(artifact: uri: \"gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/ExampleValidator/anomalies/8\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"name\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline:2022-03-30T17:18:26.717519:ExampleValidator:anomalies:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": , artifact_type: name: \"ExampleAnomalies\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": )]}), exec_properties={'exclude_splits': '[]'}, execution_output_uri='gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/ExampleValidator/.system/executor_execution/8/executor_output.pb', stateful_working_dir='gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/ExampleValidator/.system/stateful_working_dir/2022-03-30T17:18:26.717519', tmp_dir='gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/ExampleValidator/.system/executor_execution/8/.temp/', pipeline_node=node_info {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name: \"tfx.components.example_validator.component.ExampleValidator\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   id: \"ExampleValidator\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"2022-03-30T17:18:26.717519\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.ExampleValidator\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       channels {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         producer_node_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           id: \"SchemaImporter\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"2022-03-30T17:18:26.717519\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.SchemaImporter\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         artifact_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"Schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         output_key: \"result\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"statistics\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       channels {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         producer_node_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           id: \"StatisticsGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"2022-03-30T17:18:26.717519\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.StatisticsGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         artifact_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"ExampleStatistics\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         output_key: \"statistics\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"anomalies\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"ExampleAnomalies\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"exclude_splits\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"[]\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": upstream_nodes: \"SchemaImporter\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": upstream_nodes: \"StatisticsGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": downstream_nodes: \"DataTransformer\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": execution_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   caching_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": , pipeline_info=id: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": , pipeline_run_id='2022-03-30T17:18:26.717519')\n",
      "Step #3 - \"Local Test E2E Pipeline\": Validating schema against the computed statistics for split train.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Validation complete for split train. Anomalies written to gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/ExampleValidator/anomalies/8/Split-train.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Validating schema against the computed statistics for split eval.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Validation complete for split eval. Anomalies written to gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/ExampleValidator/anomalies/8/Split-eval.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Cleaning up stateless execution info.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Execution 8 succeeded.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Cleaning up stateful execution info.\n",
      "Step #3 - \"Local Test E2E Pipeline\": stateful_working_dir /workspace/mlops-with-vertex-ai/gs:/grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/ExampleValidator/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Publishing output artifacts defaultdict(<class 'list'>, {'anomalies': [Artifact(artifact: uri: \"gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/ExampleValidator/anomalies/8\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"name\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline:2022-03-30T17:18:26.717519:ExampleValidator:anomalies:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"tfx_version\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"1.2.0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": , artifact_type: name: \"ExampleAnomalies\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": )]}) for execution 8\n",
      "Step #3 - \"Local Test E2E Pipeline\": MetadataStore with DB connection initialized\n",
      "Step #3 - \"Local Test E2E Pipeline\": Component ExampleValidator is finished.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Component DataTransformer is running.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running launcher for node_info {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name: \"tfx.components.transform.component.Transform\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   id: \"DataTransformer\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"2022-03-30T17:18:26.717519\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.DataTransformer\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       channels {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         producer_node_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           id: \"TrainDataGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"2022-03-30T17:18:26.717519\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.TrainDataGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         artifact_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"Examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         output_key: \"examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       channels {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         producer_node_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           id: \"SchemaImporter\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"2022-03-30T17:18:26.717519\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.SchemaImporter\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         artifact_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"Schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         output_key: \"result\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"post_transform_anomalies\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"ExampleAnomalies\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"post_transform_schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"Schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"post_transform_stats\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"ExampleStatistics\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"pre_transform_schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"Schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"pre_transform_stats\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"ExampleStatistics\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"transform_graph\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"TransformGraph\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"transformed_examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"Examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"version\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"updated_analyzer_cache\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"TransformCache\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"custom_config\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"null\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"disable_statistics\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         int_value: 0\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"force_tf_compat_v1\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         int_value: 0\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"module_path\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"transformations@gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/_wheels/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"splits_config\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"{\\n  \\\"analyze\\\": [\\n    \\\"train\\\"\\n  ],\\n  \\\"transform\\\": [\\n    \\\"train\\\",\\n    \\\"eval\\\"\\n  ]\\n}\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": upstream_nodes: \"ExampleValidator\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": upstream_nodes: \"SchemaImporter\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": upstream_nodes: \"TrainDataGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": downstream_nodes: \"ModelTrainer\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": execution_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   caching_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": MetadataStore with DB connection initialized\n",
      "Step #3 - \"Local Test E2E Pipeline\": I0330 17:19:45.628293     1 rdbms_metadata_access_object.cc:686] No property is defined for the Type\n",
      "Step #3 - \"Local Test E2E Pipeline\": MetadataStore with DB connection initialized\n",
      "Step #3 - \"Local Test E2E Pipeline\": Going to run a new execution 9\n",
      "Step #3 - \"Local Test E2E Pipeline\": Going to run a new execution: ExecutionInfo(execution_id=9, input_dict={'schema': [Artifact(artifact: id: 2\n",
      "Step #3 - \"Local Test E2E Pipeline\": type_id: 18\n",
      "Step #3 - \"Local Test E2E Pipeline\": uri: \"src/raw_schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"tfx_version\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"1.2.0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": state: LIVE\n",
      "Step #3 - \"Local Test E2E Pipeline\": create_time_since_epoch: 1648660712887\n",
      "Step #3 - \"Local Test E2E Pipeline\": last_update_time_since_epoch: 1648660712887\n",
      "Step #3 - \"Local Test E2E Pipeline\": , artifact_type: id: 18\n",
      "Step #3 - \"Local Test E2E Pipeline\": name: \"Schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": )], 'examples': [Artifact(artifact: id: 4\n",
      "Step #3 - \"Local Test E2E Pipeline\": type_id: 20\n",
      "Step #3 - \"Local Test E2E Pipeline\": uri: \"gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/TrainDataGen/examples/5\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"file_format\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"tfrecords_gzip\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"name\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline:2022-03-30T17:18:26.717519:TrainDataGen:examples:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"payload_format\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"FORMAT_TF_EXAMPLE\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     int_value: 0\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"tfx_version\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"1.2.0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": state: LIVE\n",
      "Step #3 - \"Local Test E2E Pipeline\": create_time_since_epoch: 1648660765535\n",
      "Step #3 - \"Local Test E2E Pipeline\": last_update_time_since_epoch: 1648660765535\n",
      "Step #3 - \"Local Test E2E Pipeline\": , artifact_type: id: 20\n",
      "Step #3 - \"Local Test E2E Pipeline\": name: \"Examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"version\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": )]}, output_dict=defaultdict(<class 'list'>, {'pre_transform_stats': [Artifact(artifact: uri: \"gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/DataTransformer/pre_transform_stats/9\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"name\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline:2022-03-30T17:18:26.717519:DataTransformer:pre_transform_stats:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": , artifact_type: name: \"ExampleStatistics\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": )], 'post_transform_anomalies': [Artifact(artifact: uri: \"gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/DataTransformer/post_transform_anomalies/9\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"name\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline:2022-03-30T17:18:26.717519:DataTransformer:post_transform_anomalies:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": , artifact_type: name: \"ExampleAnomalies\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": )], 'post_transform_stats': [Artifact(artifact: uri: \"gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/DataTransformer/post_transform_stats/9\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"name\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline:2022-03-30T17:18:26.717519:DataTransformer:post_transform_stats:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": , artifact_type: name: \"ExampleStatistics\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": )], 'pre_transform_schema': [Artifact(artifact: uri: \"gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/DataTransformer/pre_transform_schema/9\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"name\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline:2022-03-30T17:18:26.717519:DataTransformer:pre_transform_schema:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": , artifact_type: name: \"Schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": )], 'transform_graph': [Artifact(artifact: uri: \"gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/DataTransformer/transform_graph/9\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"name\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline:2022-03-30T17:18:26.717519:DataTransformer:transform_graph:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": , artifact_type: name: \"TransformGraph\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": )], 'transformed_examples': [Artifact(artifact: uri: \"gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/DataTransformer/transformed_examples/9\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"name\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline:2022-03-30T17:18:26.717519:DataTransformer:transformed_examples:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": , artifact_type: name: \"Examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"version\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": )], 'post_transform_schema': [Artifact(artifact: uri: \"gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/DataTransformer/post_transform_schema/9\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"name\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline:2022-03-30T17:18:26.717519:DataTransformer:post_transform_schema:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": , artifact_type: name: \"Schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": )], 'updated_analyzer_cache': [Artifact(artifact: uri: \"gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/DataTransformer/updated_analyzer_cache/9\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"name\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline:2022-03-30T17:18:26.717519:DataTransformer:updated_analyzer_cache:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": , artifact_type: name: \"TransformCache\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": )]}), exec_properties={'module_path': 'transformations@gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/_wheels/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl', 'disable_statistics': 0, 'custom_config': 'null', 'splits_config': '{\\n  \"analyze\": [\\n    \"train\"\\n  ],\\n  \"transform\": [\\n    \"train\",\\n    \"eval\"\\n  ]\\n}', 'force_tf_compat_v1': 0}, execution_output_uri='gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/DataTransformer/.system/executor_execution/9/executor_output.pb', stateful_working_dir='gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/DataTransformer/.system/stateful_working_dir/2022-03-30T17:18:26.717519', tmp_dir='gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/DataTransformer/.system/executor_execution/9/.temp/', pipeline_node=node_info {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name: \"tfx.components.transform.component.Transform\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   id: \"DataTransformer\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"2022-03-30T17:18:26.717519\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.DataTransformer\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       channels {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         producer_node_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           id: \"TrainDataGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"2022-03-30T17:18:26.717519\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.TrainDataGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         artifact_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"Examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         output_key: \"examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       channels {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         producer_node_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           id: \"SchemaImporter\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"2022-03-30T17:18:26.717519\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"chicago-taxi-tips-classifier-v01-train-pipeline.SchemaImporter\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         artifact_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"Schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         output_key: \"result\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"post_transform_anomalies\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"ExampleAnomalies\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"post_transform_schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"Schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"post_transform_stats\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"ExampleStatistics\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"pre_transform_schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"Schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"pre_transform_stats\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"ExampleStatistics\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"transform_graph\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"TransformGraph\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"transformed_examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"Examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"version\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"updated_analyzer_cache\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"TransformCache\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"custom_config\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"null\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"disable_statistics\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         int_value: 0\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"force_tf_compat_v1\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         int_value: 0\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"module_path\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"transformations@gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/_wheels/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"splits_config\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"{\\n  \\\"analyze\\\": [\\n    \\\"train\\\"\\n  ],\\n  \\\"transform\\\": [\\n    \\\"train\\\",\\n    \\\"eval\\\"\\n  ]\\n}\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": upstream_nodes: \"ExampleValidator\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": upstream_nodes: \"SchemaImporter\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": upstream_nodes: \"TrainDataGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": downstream_nodes: \"ModelTrainer\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": execution_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   caching_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": , pipeline_info=id: \"chicago-taxi-tips-classifier-v01-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": , pipeline_run_id='2022-03-30T17:18:26.717519')\n",
      "Step #3 - \"Local Test E2E Pipeline\": Attempting to infer TFX Python dependency for beam\n",
      "Step #3 - \"Local Test E2E Pipeline\": Copying all content from install dir /opt/conda/lib/python3.7/site-packages/tfx to temp dir /tmp/tmp_gcy8485/build/tfx\n",
      "Step #3 - \"Local Test E2E Pipeline\": Generating a temp setup file at /tmp/tmp_gcy8485/build/tfx/setup.py\n",
      "Step #3 - \"Local Test E2E Pipeline\": Creating temporary sdist package, logs available at /tmp/tmp_gcy8485/build/tfx/setup.log\n",
      "Step #3 - \"Local Test E2E Pipeline\": E0330 17:19:58.139232143       1 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "Step #3 - \"Local Test E2E Pipeline\": Added --extra_package=/tmp/tmp_gcy8485/build/tfx/dist/tfx_ephemeral-1.2.0.tar.gz to beam args\n",
      "Step #3 - \"Local Test E2E Pipeline\": udf_utils.get_fn {'module_file': None, 'module_path': 'transformations@gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/_wheels/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl', 'preprocessing_fn': None} 'preprocessing_fn'\n",
      "Step #3 - \"Local Test E2E Pipeline\": Installing '/tmp/tmpn4goj2zu/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl' to a temporary directory.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Executing: ['/opt/conda/bin/python', '-m', 'pip', 'install', '--target', '/tmp/tmpdz1u3vjg', '/tmp/tmpn4goj2zu/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl']\n",
      "Step #3 - \"Local Test E2E Pipeline\": E0330 17:19:59.649218887       1 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "Step #3 - \"Local Test E2E Pipeline\": Processing /tmp/tmpn4goj2zu/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl\n",
      "Step #3 - \"Local Test E2E Pipeline\": Installing collected packages: tfx-user-code-DataTransformer\n",
      "Step #3 - \"Local Test E2E Pipeline\": WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "Step #3 - \"Local Test E2E Pipeline\": Successfully installed tfx-user-code-DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d\n",
      "Step #3 - \"Local Test E2E Pipeline\": Successfully installed '/tmp/tmpn4goj2zu/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl'.\n",
      "Step #3 - \"Local Test E2E Pipeline\": udf_utils.get_fn {'module_file': None, 'module_path': 'transformations@gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/_wheels/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl', 'stats_options_updater_fn': None} 'stats_options_updater_fn'\n",
      "Step #3 - \"Local Test E2E Pipeline\": Installing '/tmp/tmp9pzla0td/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl' to a temporary directory.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Executing: ['/opt/conda/bin/python', '-m', 'pip', 'install', '--target', '/tmp/tmptvty_x29', '/tmp/tmp9pzla0td/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl']\n",
      "Step #3 - \"Local Test E2E Pipeline\": E0330 17:20:02.700835829       1 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "Step #3 - \"Local Test E2E Pipeline\": Processing /tmp/tmp9pzla0td/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl\n",
      "Step #3 - \"Local Test E2E Pipeline\": Installing collected packages: tfx-user-code-DataTransformer\n",
      "Step #3 - \"Local Test E2E Pipeline\": Successfully installed tfx-user-code-DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d\n",
      "Step #3 - \"Local Test E2E Pipeline\": WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "Step #3 - \"Local Test E2E Pipeline\": Successfully installed '/tmp/tmp9pzla0td/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl'.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature trip_month has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature trip_day has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature trip_day_of_week has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature trip_hour has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature trip_seconds has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature trip_miles has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature payment_type has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature pickup_grid has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature dropoff_grid has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature euclidean has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature loc_cross has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature tip_bin has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": 2022-03-30 17:20:05.498440: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "Step #3 - \"Local Test E2E Pipeline\": 2022-03-30 17:20:05.498505: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "Step #3 - \"Local Test E2E Pipeline\": 2022-03-30 17:20:05.498538: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (a8cf452f19d9): /proc/driver/nvidia/version does not exist\n",
      "Step #3 - \"Local Test E2E Pipeline\": 2022-03-30 17:20:05.498851: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "Step #3 - \"Local Test E2E Pipeline\": To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Step #3 - \"Local Test E2E Pipeline\": From /opt/conda/lib/python3.7/site-packages/tensorflow_transform/tf_utils.py:261: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Instructions for updating:\n",
      "Step #3 - \"Local Test E2E Pipeline\": Use ref() instead.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature trip_month has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature trip_day has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature trip_day_of_week has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature trip_hour has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature trip_seconds has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature trip_miles has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature payment_type has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature pickup_grid has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature dropoff_grid has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature euclidean has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature loc_cross has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature tip_bin has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature trip_month has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature trip_day has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature trip_day_of_week has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature trip_hour has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature trip_seconds has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature trip_miles has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature payment_type has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature pickup_grid has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature dropoff_grid has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature euclidean has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature loc_cross has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature tip_bin has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature trip_month has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature trip_day has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature trip_day_of_week has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature trip_hour has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature trip_seconds has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature trip_miles has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature payment_type has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature pickup_grid has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature dropoff_grid has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature euclidean has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature loc_cross has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature tip_bin has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature trip_month has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature trip_day has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature trip_day_of_week has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature trip_hour has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature trip_seconds has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature trip_miles has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature payment_type has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature pickup_grid has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature dropoff_grid has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature euclidean has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature loc_cross has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature tip_bin has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature trip_month has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature trip_day has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature trip_day_of_week has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature trip_hour has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature trip_seconds has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature trip_miles has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature payment_type has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature pickup_grid has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature dropoff_grid has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature euclidean has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature loc_cross has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature tip_bin has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Installing '/tmp/tmpju2jwj2q/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl' to a temporary directory.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Executing: ['/opt/conda/bin/python', '-m', 'pip', 'install', '--target', '/tmp/tmp83tv_dxt', '/tmp/tmpju2jwj2q/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl']\n",
      "Step #3 - \"Local Test E2E Pipeline\": E0330 17:20:05.988537038       1 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "Step #3 - \"Local Test E2E Pipeline\": Processing /tmp/tmpju2jwj2q/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl\n",
      "Step #3 - \"Local Test E2E Pipeline\": Installing collected packages: tfx-user-code-DataTransformer\n",
      "Step #3 - \"Local Test E2E Pipeline\": Successfully installed tfx-user-code-DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d\n",
      "Step #3 - \"Local Test E2E Pipeline\": WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "Step #3 - \"Local Test E2E Pipeline\": Successfully installed '/tmp/tmpju2jwj2q/tfx_user_code_DataTransformer-0.0+de07c8431e7a29dced215501daf4f187c64541d3189d2529c8a52c51eb6c9d4d-py3-none-any.whl'.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "Step #3 - \"Local Test E2E Pipeline\": This output type hint will be ignored and not used for type-checking purposes. Typically, output type hints for a PTransform are single (or nested) types wrapped by a PCollection, PDone, or None. Got: Tuple[Dict[str, Union[NoneType, _Dataset]], Union[Dict[str, Dict[str, PCollection]], NoneType]] instead.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "Step #3 - \"Local Test E2E Pipeline\": Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_1/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "Step #3 - \"Local Test E2E Pipeline\": Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_2/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "Step #3 - \"Local Test E2E Pipeline\": Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_3/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "Step #3 - \"Local Test E2E Pipeline\": Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_4/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "Step #3 - \"Local Test E2E Pipeline\": Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_5/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "Step #3 - \"Local Test E2E Pipeline\": Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_6/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "Step #3 - \"Local Test E2E Pipeline\": Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_7/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "Step #3 - \"Local Test E2E Pipeline\": Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "Step #3 - \"Local Test E2E Pipeline\": Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_1/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "Step #3 - \"Local Test E2E Pipeline\": Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_2/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "Step #3 - \"Local Test E2E Pipeline\": Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_3/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "Step #3 - \"Local Test E2E Pipeline\": Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_4/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "Step #3 - \"Local Test E2E Pipeline\": Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_5/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "Step #3 - \"Local Test E2E Pipeline\": Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_6/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "Step #3 - \"Local Test E2E Pipeline\": Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_7/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "Step #3 - \"Local Test E2E Pipeline\": This output type hint will be ignored and not used for type-checking purposes. Typically, output type hints for a PTransform are single (or nested) types wrapped by a PCollection, PDone, or None. Got: Tuple[Dict[str, Union[NoneType, _Dataset]], Union[Dict[str, Dict[str, PCollection]], NoneType]] instead.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.08455109596252441 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": 2022-03-30 17:20:31.221765: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Step #3 - \"Local Test E2E Pipeline\": 2022-03-30 17:20:31.222701: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2199995000 Hz\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.09626245498657227 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature trip_month has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature trip_day has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature trip_day_of_week has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature trip_hour has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature trip_seconds has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature trip_miles has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature payment_type has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature pickup_grid has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature dropoff_grid has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature euclidean has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature loc_cross has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature tip_bin has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.07533884048461914 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature trip_month has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature trip_day has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature trip_day_of_week has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature trip_hour has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature trip_seconds has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature trip_miles has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature payment_type has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature pickup_grid has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature dropoff_grid has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature euclidean has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature loc_cross has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature tip_bin has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Ignoring send_type hint: <class 'NoneType'>\n",
      "Step #3 - \"Local Test E2E Pipeline\": Ignoring return_type hint: <class 'NoneType'>\n",
      "Step #3 - \"Local Test E2E Pipeline\": Ignoring send_type hint: <class 'NoneType'>\n",
      "Step #3 - \"Local Test E2E Pipeline\": Ignoring return_type hint: <class 'NoneType'>\n",
      "Step #3 - \"Local Test E2E Pipeline\": Ignoring send_type hint: <class 'NoneType'>\n",
      "Step #3 - \"Local Test E2E Pipeline\": Ignoring return_type hint: <class 'NoneType'>\n",
      "Step #3 - \"Local Test E2E Pipeline\": Ignoring send_type hint: <class 'NoneType'>\n",
      "Step #3 - \"Local Test E2E Pipeline\": Ignoring return_type hint: <class 'NoneType'>\n",
      "Step #3 - \"Local Test E2E Pipeline\": Ignoring send_type hint: <class 'NoneType'>\n",
      "Step #3 - \"Local Test E2E Pipeline\": Ignoring return_type hint: <class 'NoneType'>\n",
      "Step #3 - \"Local Test E2E Pipeline\": Ignoring send_type hint: <class 'NoneType'>\n",
      "Step #3 - \"Local Test E2E Pipeline\": Ignoring return_type hint: <class 'NoneType'>\n",
      "Step #3 - \"Local Test E2E Pipeline\": Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Default Python SDK image for environment is apache/beam_python3.7_sdk:2.31.0\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function annotate_downstream_side_inputs at 0x7f5e1692f8c0> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function fix_side_input_pcoll_coders at 0x7f5e1692f9e0> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function pack_combiners at 0x7f5e1692fe60> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function lift_combiners at 0x7f5e1692fef0> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function expand_sdf at 0x7f5e169380e0> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function expand_gbk at 0x7f5e16938170> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function sink_flattens at 0x7f5e16938290> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function greedily_fuse at 0x7f5e16938320> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function read_to_impulse at 0x7f5e169383b0> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function impulse_to_input at 0x7f5e16938440> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function sort_stages at 0x7f5e16938680> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function setup_timer_mapping at 0x7f5e169385f0> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function populate_data_channel_coders at 0x7f5e16938710> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": Creating state cache with size 100\n",
      "Step #3 - \"Local Test E2E Pipeline\": Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7f5e11eb57d0> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteSchema-Write-WriteI_1335)+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteSchema-Write-WriteI_1336))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteSchema-Write-WriteI_1338))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteSchema-Write-WriteI_1339))+(ref_PCollection_PCollection_779/Write))+(ref_PCollection_PCollection_780/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteStats-WriteStats-Wr_1315)+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteStats-WriteStats-Wr_1316))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteStats-WriteStats-Wr_1318))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteStats-WriteStats-Wr_1319))+(ref_PCollection_PCollection_766/Write))+(ref_PCollection_PCollection_767/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((ref_AppliedPTransform_Analyze-CreateSavedModelForAnalyzerInputs-Phase0-tf_v2_only-CreateSole-Impulse_44)+(ref_AppliedPTransform_Analyze-CreateSavedModelForAnalyzerInputs-Phase0-tf_v2_only-CreateSole-FlatMap_45))+(ref_AppliedPTransform_Analyze-CreateSavedModelForAnalyzerInputs-Phase0-tf_v2_only-CreateSole-Map-dec_47))+(ref_AppliedPTransform_Analyze-CreateSavedModelForAnalyzerInputs-Phase0-tf_v2_only-CreateSavedModel_48))+(ref_PCollection_PCollection_23/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": 2022-03-30 17:20:48.563744: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Assets written to: gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/DataTransformer/transform_graph/9/.temp_path/tftransform_tmp/1933cfd400474776ad715c518ad97a2e/assets\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((ref_AppliedPTransform_TFXIOReadAndDecode-AnalysisIndex0-RawRecordBeamSource-ReadRawRecords-ReadFromT_20)+(ref_AppliedPTransform_TFXIOReadAndDecode-AnalysisIndex0-RawRecordBeamSource-ReadRawRecords-ReadFromT_21))+(TFXIOReadAndDecode[AnalysisIndex0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(TFXIOReadAndDecode[AnalysisIndex0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_9_split/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.07221651077270508 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.07563900947570801 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((ref_PCollection_PCollection_9_split/Read)+(TFXIOReadAndDecode[AnalysisIndex0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_TFXIOReadAndDecode-AnalysisIndex0-RawRecordBeamSource-ReadRawRecords-FlattenPC_24))+(ref_AppliedPTransform_TFXIOReadAndDecode-AnalysisIndex0-RawRecordBeamSource-CollectRawRecordTelemetr_26))+(ref_AppliedPTransform_TFXIOReadAndDecode-AnalysisIndex0-RawRecordToRecordBatch-RawRecordToRecordBatc_30))+(ref_AppliedPTransform_TFXIOReadAndDecode-AnalysisIndex0-RawRecordToRecordBatch-RawRecordToRecordBatc_31))+(ref_AppliedPTransform_TFXIOReadAndDecode-AnalysisIndex0-RawRecordToRecordBatch-CollectRecordBatchTel_33))+(ref_AppliedPTransform_Analyze-ExtractInputForSavedModel-AnalysisIndex0-Identity_57))+(ref_AppliedPTransform_FlattenAnalysisDatasets_1077))+(ref_AppliedPTransform_Analyze-ApplySavedModel-Phase0-AnalysisIndex0-ApplySavedModel_59))+(ref_AppliedPTransform_Analyze-ApplySavedModel-Phase0-AnalysisIndex0-ConvertToNumpy_60))+(ref_AppliedPTransform_Analyze-PackedCombineAccumulate-ApplySavedModel-Phase0-AnalysisIndex0-InitialP_63))+(ref_AppliedPTransform_Analyze-TensorSource-compute_and_apply_vocabulary-vocabulary-AnalysisIndex0-Ex_144))+(ref_AppliedPTransform_Analyze-TensorSource-compute_and_apply_vocabulary_1-vocabulary-AnalysisIndex0-_219))+(ref_AppliedPTransform_Analyze-TensorSource-compute_and_apply_vocabulary_2-vocabulary-AnalysisIndex0-_294))+(ref_AppliedPTransform_Analyze-TensorSource-compute_and_apply_vocabulary_3-vocabulary-AnalysisIndex0-_369))+(ref_AppliedPTransform_Analyze-TensorSource-compute_and_apply_vocabulary_4-vocabulary-AnalysisIndex0-_444))+(ref_AppliedPTransform_Analyze-TensorSource-compute_and_apply_vocabulary_5-vocabulary-AnalysisIndex0-_519))+(ref_AppliedPTransform_Analyze-TensorSource-compute_and_apply_vocabulary_6-vocabulary-AnalysisIndex0-_594))+(ref_AppliedPTransform_Analyze-TensorSource-compute_and_apply_vocabulary_7-vocabulary-AnalysisIndex0-_669))+(ref_AppliedPTransform_Analyze-PackedCombineAccumulate-ApplySavedModel-Phase0-AnalysisIndex0-InitialP_66))+(Analyze/PackedCombineAccumulate[ApplySavedModel[Phase0][AnalysisIndex0]]/InitialPackedCombineGlobally/CombinePerKey/Flatten/Transcode/0))+(ref_AppliedPTransform_Analyze-PackedCombineAccumulate-ApplySavedModel-Phase0-AnalysisIndex0-InitialP_67))+(Analyze/PackedCombineAccumulate[ApplySavedModel[Phase0][AnalysisIndex0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PreCombineFn)/Precombine))+(Analyze/PackedCombineAccumulate[ApplySavedModel[Phase0][AnalysisIndex0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PreCombineFn)/Group/Write))+(Analyze/PackedCombineAccumulate[ApplySavedModel[Phase0][AnalysisIndex0]]/InitialPackedCombineGlobally/CombinePerKey/Flatten/Write/0))+(ref_AppliedPTransform_Analyze-VocabularyAccumulate-compute_and_apply_vocabulary-vocabulary-AnalysisI_146))+(ref_AppliedPTransform_Analyze-VocabularyAccumulate-compute_and_apply_vocabulary-vocabulary-AnalysisI_148))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Precombine))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Group/Write))+(ref_AppliedPTransform_Analyze-VocabularyAccumulate-compute_and_apply_vocabulary_1-vocabulary-Analysi_221))+(ref_AppliedPTransform_Analyze-VocabularyAccumulate-compute_and_apply_vocabulary_1-vocabulary-Analysi_223))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_1#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Precombine))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_1#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Group/Write))+(ref_AppliedPTransform_Analyze-VocabularyAccumulate-compute_and_apply_vocabulary_2-vocabulary-Analysi_296))+(ref_AppliedPTransform_Analyze-VocabularyAccumulate-compute_and_apply_vocabulary_2-vocabulary-Analysi_298))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_2#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Precombine))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_2#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Group/Write))+(ref_AppliedPTransform_Analyze-VocabularyAccumulate-compute_and_apply_vocabulary_3-vocabulary-Analysi_371))+(ref_AppliedPTransform_Analyze-VocabularyAccumulate-compute_and_apply_vocabulary_3-vocabulary-Analysi_373))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_3#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Precombine))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_3#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Group/Write))+(ref_AppliedPTransform_Analyze-VocabularyAccumulate-compute_and_apply_vocabulary_4-vocabulary-Analysi_446))+(ref_AppliedPTransform_Analyze-VocabularyAccumulate-compute_and_apply_vocabulary_4-vocabulary-Analysi_448))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_4#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Precombine))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_4#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Group/Write))+(ref_AppliedPTransform_Analyze-VocabularyAccumulate-compute_and_apply_vocabulary_5-vocabulary-Analysi_521))+(ref_AppliedPTransform_Analyze-VocabularyAccumulate-compute_and_apply_vocabulary_5-vocabulary-Analysi_523))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_5#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Precombine))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_5#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Group/Write))+(ref_AppliedPTransform_Analyze-VocabularyAccumulate-compute_and_apply_vocabulary_6-vocabulary-Analysi_596))+(ref_AppliedPTransform_Analyze-VocabularyAccumulate-compute_and_apply_vocabulary_6-vocabulary-Analysi_598))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_6#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Precombine))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_6#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Group/Write))+(ref_AppliedPTransform_Analyze-VocabularyAccumulate-compute_and_apply_vocabulary_7-vocabulary-Analysi_671))+(ref_AppliedPTransform_Analyze-VocabularyAccumulate-compute_and_apply_vocabulary_7-vocabulary-Analysi_673))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_7#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Precombine))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_7#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Group/Write))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-FilterInternalColumn_1079))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-K_1082))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-G_1085))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-G_1110))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/Precombine))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/Group/Write))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/Flatten/Transcode/0))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-G_1111))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PreCombineFn)/Precombine))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PreCombineFn)/Group/Write))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/Flatten/Write/0)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex10-Write-WriteImpl-DoOnce-Impulse_1065)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex10-Write-WriteImpl-DoOnce-FlatMap_1066))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex10-Write-WriteImpl-DoOnce-Map-dec_1068))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex10-Write-WriteImpl-InitializeWrit_1069))+(ref_PCollection_PCollection_619/Write))+(ref_PCollection_PCollection_620/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((((((((Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_7#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Group/Read)+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_7#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Merge))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_7#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/ExtractOutputs))+(Analyze/FlattenCache[VocabularyMerge[compute_and_apply_vocabulary_7#vocabulary]]/Flatten/Transcode/0))+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_7-vocabu_867))+(ref_AppliedPTransform_Analyze-FlattenCache-VocabularyMerge-compute_and_apply_vocabulary_7-vocabulary_679))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_7#vocabulary]/MergeCountPerToken/Precombine))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_7#vocabulary]/MergeCountPerToken/Group/Write))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex10-Write-WriteImpl-WindowInto-Win_1070))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex10-Write-WriteImpl-WriteBundles_1071))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex10-Write-WriteImpl-Pair_1072))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex10]/Write/WriteImpl/GroupByKey/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((((((((Analyze/VocabularyMerge[compute_and_apply_vocabulary_7#vocabulary]/MergeCountPerToken/Group/Read)+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_7#vocabulary]/MergeCountPerToken/Merge))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_7#vocabulary]/MergeCountPerToken/ExtractOutputs))+(ref_AppliedPTransform_Analyze-VocabularyMerge-compute_and_apply_vocabulary_7-vocabulary-SwapTokensAn_686))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_7-vocabulary-TotalVocabSi_690))+(ref_AppliedPTransform_Analyze-VocabularyPrune-compute_and_apply_vocabulary_7-vocabulary-KeepOnlyVali_706))+(Analyze/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Precombine))+(Analyze/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Write))+(ref_AppliedPTransform_Analyze-VocabularyPrune-compute_and_apply_vocabulary_7-vocabulary-ApplyThresho_708))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_7-vocabulary-Batc_712))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_7-vocabulary-Batc_713))+(ref_PCollection_PCollection_395/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((Analyze/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Read)+(Analyze/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Merge))+(Analyze/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_7-vocabulary-TotalVocabSi_695))+(ref_PCollection_PCollection_385/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_7-vocabulary-TotalVocabSi_697)+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_7-vocabulary-TotalVocabSi_698))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_7-vocabulary-TotalVocabSi_700))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_7-vocabulary-TotalVocabSi_701))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_7-vocabulary-ToInt64_702))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-compute_and_apply_vocabulary_7-vocabulary-temporar_704))+(ref_PCollection_PCollection_391/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex8-Write-WriteImpl-DoOnce-Impulse_1033)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex8-Write-WriteImpl-DoOnce-FlatMap-_1034))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex8-Write-WriteImpl-DoOnce-Map-deco_1036))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex8-Write-WriteImpl-InitializeWrite_1037))+(ref_PCollection_PCollection_597/Write))+(ref_PCollection_PCollection_598/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((((((((Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_6#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Group/Read)+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_6#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Merge))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_6#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/ExtractOutputs))+(Analyze/FlattenCache[VocabularyMerge[compute_and_apply_vocabulary_6#vocabulary]]/Flatten/Transcode/0))+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_6-vocabu_849))+(ref_AppliedPTransform_Analyze-FlattenCache-VocabularyMerge-compute_and_apply_vocabulary_6-vocabulary_604))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_6#vocabulary]/MergeCountPerToken/Precombine))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_6#vocabulary]/MergeCountPerToken/Group/Write))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex8-Write-WriteImpl-WindowInto-Wind_1038))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex8-Write-WriteImpl-WriteBundles_1039))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex8-Write-WriteImpl-Pair_1040))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex8]/Write/WriteImpl/GroupByKey/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((((((((Analyze/VocabularyMerge[compute_and_apply_vocabulary_6#vocabulary]/MergeCountPerToken/Group/Read)+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_6#vocabulary]/MergeCountPerToken/Merge))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_6#vocabulary]/MergeCountPerToken/ExtractOutputs))+(ref_AppliedPTransform_Analyze-VocabularyMerge-compute_and_apply_vocabulary_6-vocabulary-SwapTokensAn_611))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_6-vocabulary-TotalVocabSi_615))+(ref_AppliedPTransform_Analyze-VocabularyPrune-compute_and_apply_vocabulary_6-vocabulary-KeepOnlyVali_631))+(Analyze/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Precombine))+(Analyze/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Write))+(ref_AppliedPTransform_Analyze-VocabularyPrune-compute_and_apply_vocabulary_6-vocabulary-ApplyThresho_633))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_6-vocabulary-Batc_637))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_6-vocabulary-Batc_638))+(ref_PCollection_PCollection_353/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((Analyze/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Read)+(Analyze/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Merge))+(Analyze/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_6-vocabulary-TotalVocabSi_620))+(ref_PCollection_PCollection_343/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_6-vocabulary-TotalVocabSi_622)+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_6-vocabulary-TotalVocabSi_623))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_6-vocabulary-TotalVocabSi_625))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_6-vocabulary-TotalVocabSi_626))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_6-vocabulary-ToInt64_627))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-compute_and_apply_vocabulary_6-vocabulary-temporar_629))+(ref_PCollection_PCollection_349/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_7-vocabulary-Writ_724)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_7-vocabulary-Writ_725))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_7-vocabulary-Writ_727))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_7-vocabulary-Writ_728))+(ref_PCollection_PCollection_402/Write))+(ref_PCollection_PCollection_403/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_7-vocabulary-Prep_715)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_7-vocabulary-Prep_716))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_7-vocabulary-Prep_718))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_7-vocabulary-Orde_719))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_7-vocabulary-Writ_729))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_7-vocabulary-Writ_730))+(Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_7-vocabulary-Writ_732))+(ref_PCollection_PCollection_407/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_402/Read)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_7-vocabulary-Writ_733))+(ref_PCollection_PCollection_408/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_402/Read)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_7-vocabulary-Writ_734))+(ref_PCollection_PCollection_409/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.06710577011108398 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.50 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_7-vocabulary-Crea_736)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_7-vocabulary-Crea_737))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_7-vocabulary-Crea_739))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_7-vocabulary-Wait_740))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-compute_and_apply_vocabulary_7-vocabulary-temporar_742))+(ref_PCollection_PCollection_414/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary-vocabulary-WriteT_199)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary-vocabulary-WriteT_200))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary-vocabulary-WriteT_202))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary-vocabulary-WriteT_203))+(ref_PCollection_PCollection_108/Write))+(ref_PCollection_PCollection_109/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex0-Write-WriteImpl-DoOnce-Impulse_905)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex0-Write-WriteImpl-DoOnce-FlatMap-_906))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex0-Write-WriteImpl-DoOnce-Map-deco_908))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex0-Write-WriteImpl-InitializeWrite_909))+(ref_PCollection_PCollection_509/Write))+(ref_PCollection_PCollection_510/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((((((((Analyze/VocabularyAccumulate[compute_and_apply_vocabulary#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Group/Read)+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Merge))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/ExtractOutputs))+(Analyze/FlattenCache[VocabularyMerge[compute_and_apply_vocabulary#vocabulary]]/Flatten/Transcode/0))+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary-vocabula_777))+(ref_AppliedPTransform_Analyze-FlattenCache-VocabularyMerge-compute_and_apply_vocabulary-vocabulary-F_154))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary#vocabulary]/MergeCountPerToken/Precombine))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary#vocabulary]/MergeCountPerToken/Group/Write))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex0-Write-WriteImpl-WindowInto-Wind_910))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex0-Write-WriteImpl-WriteBundles_911))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex0-Write-WriteImpl-Pair_912))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex0]/Write/WriteImpl/GroupByKey/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((((((((Analyze/VocabularyMerge[compute_and_apply_vocabulary#vocabulary]/MergeCountPerToken/Group/Read)+(Analyze/VocabularyMerge[compute_and_apply_vocabulary#vocabulary]/MergeCountPerToken/Merge))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary#vocabulary]/MergeCountPerToken/ExtractOutputs))+(ref_AppliedPTransform_Analyze-VocabularyMerge-compute_and_apply_vocabulary-vocabulary-SwapTokensAndC_161))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary-vocabulary-TotalVocabSize_165))+(ref_AppliedPTransform_Analyze-VocabularyPrune-compute_and_apply_vocabulary-vocabulary-ApplyThreshold_182))+(Analyze/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Precombine))+(Analyze/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Write))+(ref_AppliedPTransform_Analyze-VocabularyPrune-compute_and_apply_vocabulary-vocabulary-ApplyThreshold_183))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary-vocabulary-BatchA_187))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary-vocabulary-BatchA_188))+(ref_PCollection_PCollection_101/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary-vocabulary-Prepar_190)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary-vocabulary-Prepar_191))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary-vocabulary-Prepar_193))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary-vocabulary-OrderE_194))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary-vocabulary-WriteT_204))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary-vocabulary-WriteT_205))+(Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary-vocabulary-WriteT_207))+(ref_PCollection_PCollection_113/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_108/Read)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary-vocabulary-WriteT_208))+(ref_PCollection_PCollection_114/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_108/Read)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary-vocabulary-WriteT_209))+(ref_PCollection_PCollection_115/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.07184123992919922 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.50 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary-vocabulary-Create_211)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary-vocabulary-Create_212))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary-vocabulary-Create_214))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary-vocabulary-WaitFo_215))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-compute_and_apply_vocabulary-vocabulary-temporary__217))+(ref_PCollection_PCollection_120/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex7-Write-WriteImpl-DoOnce-Impulse_1017)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex7-Write-WriteImpl-DoOnce-FlatMap-_1018))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex7-Write-WriteImpl-DoOnce-Map-deco_1020))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex7-Write-WriteImpl-InitializeWrite_1021))+(ref_PCollection_PCollection_586/Write))+(ref_PCollection_PCollection_587/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((((((((Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_5#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Group/Read)+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_5#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Merge))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_5#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/ExtractOutputs))+(Analyze/FlattenCache[VocabularyMerge[compute_and_apply_vocabulary_5#vocabulary]]/Flatten/Transcode/0))+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_5-vocabu_840))+(ref_AppliedPTransform_Analyze-FlattenCache-VocabularyMerge-compute_and_apply_vocabulary_5-vocabulary_529))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_5#vocabulary]/MergeCountPerToken/Precombine))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_5#vocabulary]/MergeCountPerToken/Group/Write))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex7-Write-WriteImpl-WindowInto-Wind_1022))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex7-Write-WriteImpl-WriteBundles_1023))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex7-Write-WriteImpl-Pair_1024))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex7]/Write/WriteImpl/GroupByKey/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((((((((Analyze/VocabularyMerge[compute_and_apply_vocabulary_5#vocabulary]/MergeCountPerToken/Group/Read)+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_5#vocabulary]/MergeCountPerToken/Merge))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_5#vocabulary]/MergeCountPerToken/ExtractOutputs))+(ref_AppliedPTransform_Analyze-VocabularyMerge-compute_and_apply_vocabulary_5-vocabulary-SwapTokensAn_536))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_5-vocabulary-TotalVocabSi_540))+(ref_AppliedPTransform_Analyze-VocabularyPrune-compute_and_apply_vocabulary_5-vocabulary-KeepOnlyVali_556))+(Analyze/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Precombine))+(Analyze/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Write))+(ref_AppliedPTransform_Analyze-VocabularyPrune-compute_and_apply_vocabulary_5-vocabulary-ApplyThresho_558))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_5-vocabulary-Batc_562))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_5-vocabulary-Batc_563))+(ref_PCollection_PCollection_311/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((Analyze/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Read)+(Analyze/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Merge))+(Analyze/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_5-vocabulary-TotalVocabSi_545))+(ref_PCollection_PCollection_301/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_5-vocabulary-TotalVocabSi_547)+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_5-vocabulary-TotalVocabSi_548))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_5-vocabulary-TotalVocabSi_550))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_5-vocabulary-TotalVocabSi_551))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_5-vocabulary-ToInt64_552))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-compute_and_apply_vocabulary_5-vocabulary-temporar_554))+(ref_PCollection_PCollection_307/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex5-Write-WriteImpl-DoOnce-Impulse_985)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex5-Write-WriteImpl-DoOnce-FlatMap-_986))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex5-Write-WriteImpl-DoOnce-Map-deco_988))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex5-Write-WriteImpl-InitializeWrite_989))+(ref_PCollection_PCollection_564/Write))+(ref_PCollection_PCollection_565/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex4-Write-WriteImpl-DoOnce-Impulse_969)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex4-Write-WriteImpl-DoOnce-FlatMap-_970))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex4-Write-WriteImpl-DoOnce-Map-deco_972))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex4-Write-WriteImpl-InitializeWrite_973))+(ref_PCollection_PCollection_553/Write))+(ref_PCollection_PCollection_554/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex9-Write-WriteImpl-DoOnce-Impulse_1049)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex9-Write-WriteImpl-DoOnce-FlatMap-_1050))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex9-Write-WriteImpl-DoOnce-Map-deco_1052))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex9-Write-WriteImpl-InitializeWrite_1053))+(ref_PCollection_PCollection_608/Write))+(ref_PCollection_PCollection_609/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((Analyze/PackedCombineAccumulate[ApplySavedModel[Phase0][AnalysisIndex0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PreCombineFn)/Group/Read)+(Analyze/PackedCombineAccumulate[ApplySavedModel[Phase0][AnalysisIndex0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PreCombineFn)/Merge))+(Analyze/PackedCombineAccumulate[ApplySavedModel[Phase0][AnalysisIndex0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PreCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_Analyze-PackedCombineAccumulate-ApplySavedModel-Phase0-AnalysisIndex0-InitialP_72))+(ref_AppliedPTransform_Analyze-PackedCombineAccumulate-ApplySavedModel-Phase0-AnalysisIndex0-InitialP_73))+(Analyze/PackedCombineAccumulate[ApplySavedModel[Phase0][AnalysisIndex0]]/InitialPackedCombineGlobally/CombinePerKey/Flatten/Transcode/1))+(Analyze/PackedCombineAccumulate[ApplySavedModel[Phase0][AnalysisIndex0]]/InitialPackedCombineGlobally/CombinePerKey/Flatten/Write/1)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((Analyze/PackedCombineAccumulate[ApplySavedModel[Phase0][AnalysisIndex0]]/InitialPackedCombineGlobally/CombinePerKey/Flatten/Read)+(Analyze/PackedCombineAccumulate[ApplySavedModel[Phase0][AnalysisIndex0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/Precombine))+(Analyze/PackedCombineAccumulate[ApplySavedModel[Phase0][AnalysisIndex0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/Group/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((Analyze/PackedCombineAccumulate[ApplySavedModel[Phase0][AnalysisIndex0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/Group/Read)+(Analyze/PackedCombineAccumulate[ApplySavedModel[Phase0][AnalysisIndex0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/Merge))+(Analyze/PackedCombineAccumulate[ApplySavedModel[Phase0][AnalysisIndex0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_Analyze-PackedCombineAccumulate-ApplySavedModel-Phase0-AnalysisIndex0-InitialP_79))+(ref_PCollection_PCollection_42/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((((((((((((((((((((((((((ref_AppliedPTransform_Analyze-PackedCombineAccumulate-ApplySavedModel-Phase0-AnalysisIndex0-InitialP_81)+(ref_AppliedPTransform_Analyze-PackedCombineAccumulate-ApplySavedModel-Phase0-AnalysisIndex0-InitialP_82))+(ref_AppliedPTransform_Analyze-PackedCombineAccumulate-ApplySavedModel-Phase0-AnalysisIndex0-InitialP_84))+(ref_AppliedPTransform_Analyze-PackedCombineAccumulate-ApplySavedModel-Phase0-AnalysisIndex0-InitialP_85))+(ref_AppliedPTransform_Analyze-CacheableCombineAccumulate-scale_to_z_score-mean_and_var-AnalysisIndex_94))+(ref_AppliedPTransform_Analyze-CacheableCombineAccumulate-scale_to_z_score_1-mean_and_var-AnalysisInd_100))+(ref_AppliedPTransform_Analyze-CacheableCombineAccumulate-scale_to_z_score_2-mean_and_var-AnalysisInd_106))+(ref_AppliedPTransform_Analyze-FlattenCache-CacheableCombineMerge-scale_to_z_score-mean_and_var-Flatt_96))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_z_score-mean_and_var-A_813))+(ref_AppliedPTransform_Analyze-AddKey-CacheableCombineMerge-scale_to_z_score-mean_and_var-AddKey_98))+(Analyze/FlattenInputForPackedCombineMerge[3]/Flatten/Write/0))+(ref_AppliedPTransform_Analyze-FlattenCache-CacheableCombineMerge-scale_to_z_score_1-mean_and_var-Fla_102))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_z_score_1-mean_and_var_822))+(ref_AppliedPTransform_Analyze-AddKey-CacheableCombineMerge-scale_to_z_score_1-mean_and_var-AddKey_104))+(Analyze/FlattenInputForPackedCombineMerge[3]/Flatten/Write/1))+(ref_AppliedPTransform_Analyze-FlattenCache-CacheableCombineMerge-scale_to_z_score_2-mean_and_var-Fla_108))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_z_score_2-mean_and_var_858))+(ref_AppliedPTransform_Analyze-AddKey-CacheableCombineMerge-scale_to_z_score_2-mean_and_var-AddKey_110))+(Analyze/FlattenInputForPackedCombineMerge[3]/Flatten/Write/2))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex4-Write-WriteImpl-WindowInto-Wind_974))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex5-Write-WriteImpl-WindowInto-Wind_990))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex9-Write-WriteImpl-WindowInto-Wind_1054))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex4-Write-WriteImpl-WriteBundles_975))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex4-Write-WriteImpl-Pair_976))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex4]/Write/WriteImpl/GroupByKey/Write))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex5-Write-WriteImpl-WriteBundles_991))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex5-Write-WriteImpl-Pair_992))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex5]/Write/WriteImpl/GroupByKey/Write))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex9-Write-WriteImpl-WriteBundles_1055))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex9-Write-WriteImpl-Pair_1056))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex9]/Write/WriteImpl/GroupByKey/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((Analyze/FlattenInputForPackedCombineMerge[3]/Flatten/Read)+(ref_AppliedPTransform_Analyze-PackedCombineMerge-3-MergePackedCombinesGlobally-KeyWithVoid_115))+(Analyze/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/Precombine))+(Analyze/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/Group/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((Analyze/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/Group/Read)+(Analyze/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/Merge))+(Analyze/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_Analyze-PackedCombineMerge-3-MergePackedCombinesGlobally-UnKey_120))+(ref_PCollection_PCollection_64/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((((((((((((((((((ref_AppliedPTransform_Analyze-PackedCombineMerge-3-MergePackedCombinesGlobally-DoOnce-Impulse_122)+(ref_AppliedPTransform_Analyze-PackedCombineMerge-3-MergePackedCombinesGlobally-DoOnce-FlatMap-lambda_123))+(ref_AppliedPTransform_Analyze-PackedCombineMerge-3-MergePackedCombinesGlobally-DoOnce-Map-decode-_125))+(ref_AppliedPTransform_Analyze-PackedCombineMerge-3-MergePackedCombinesGlobally-InjectDefault_126))+(ref_AppliedPTransform_Analyze-ExtractFromDict-CacheableCombineMerge-scale_to_z_score_2-mean_and_var-_135))+(ref_AppliedPTransform_Analyze-ExtractFromDict-CacheableCombineMerge-scale_to_z_score-mean_and_var-Ex_744))+(ref_AppliedPTransform_Analyze-ExtractFromDict-CacheableCombineMerge-scale_to_z_score_1-mean_and_var-_753))+(ref_AppliedPTransform_Analyze-ExtractPackedCombineMergeOutputs-CacheableCombineMerge-scale_to_z_scor_138))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_z_score_2-mean_and_var-temporary_analyzer_142))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_z_score_2-mean_and_var-temporary_analyzer_140))+(ref_PCollection_PCollection_77/Write))+(ref_PCollection_PCollection_78/Write))+(ref_AppliedPTransform_Analyze-ExtractPackedCombineMergeOutputs-CacheableCombineMerge-scale_to_z_scor_747))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_z_score-mean_and_var-temporary_analyzer_o_751))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_z_score-mean_and_var-temporary_analyzer_o_749))+(ref_PCollection_PCollection_419/Write))+(ref_PCollection_PCollection_420/Write))+(ref_AppliedPTransform_Analyze-ExtractPackedCombineMergeOutputs-CacheableCombineMerge-scale_to_z_scor_756))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_z_score_1-mean_and_var-temporary_analyzer_758))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_z_score_1-mean_and_var-temporary_analyzer_760))+(ref_PCollection_PCollection_425/Write))+(ref_PCollection_PCollection_426/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((Analyze/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Read)+(Analyze/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Merge))+(Analyze/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary-vocabulary-TotalVocabSize_170))+(ref_PCollection_PCollection_91/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary-vocabulary-TotalVocabSize_172)+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary-vocabulary-TotalVocabSize_173))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary-vocabulary-TotalVocabSize_175))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary-vocabulary-TotalVocabSize_176))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary-vocabulary-ToInt64_177))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-compute_and_apply_vocabulary-vocabulary-temporary__179))+(ref_PCollection_PCollection_97/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex6-Write-WriteImpl-DoOnce-Impulse_1001)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex6-Write-WriteImpl-DoOnce-FlatMap-_1002))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex6-Write-WriteImpl-DoOnce-Map-deco_1004))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex6-Write-WriteImpl-InitializeWrite_1005))+(ref_PCollection_PCollection_575/Write))+(ref_PCollection_PCollection_576/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((((((((Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_4#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Group/Read)+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_4#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Merge))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_4#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/ExtractOutputs))+(Analyze/FlattenCache[VocabularyMerge[compute_and_apply_vocabulary_4#vocabulary]]/Flatten/Transcode/0))+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_4-vocabu_831))+(ref_AppliedPTransform_Analyze-FlattenCache-VocabularyMerge-compute_and_apply_vocabulary_4-vocabulary_454))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_4#vocabulary]/MergeCountPerToken/Precombine))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_4#vocabulary]/MergeCountPerToken/Group/Write))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex6-Write-WriteImpl-WindowInto-Wind_1006))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex6-Write-WriteImpl-WriteBundles_1007))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex6-Write-WriteImpl-Pair_1008))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex6]/Write/WriteImpl/GroupByKey/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((((((((Analyze/VocabularyMerge[compute_and_apply_vocabulary_4#vocabulary]/MergeCountPerToken/Group/Read)+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_4#vocabulary]/MergeCountPerToken/Merge))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_4#vocabulary]/MergeCountPerToken/ExtractOutputs))+(ref_AppliedPTransform_Analyze-VocabularyMerge-compute_and_apply_vocabulary_4-vocabulary-SwapTokensAn_461))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_4-vocabulary-TotalVocabSi_465))+(ref_AppliedPTransform_Analyze-VocabularyPrune-compute_and_apply_vocabulary_4-vocabulary-KeepOnlyVali_481))+(Analyze/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Precombine))+(Analyze/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Write))+(ref_AppliedPTransform_Analyze-VocabularyPrune-compute_and_apply_vocabulary_4-vocabulary-ApplyThresho_483))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_4-vocabulary-Batc_487))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_4-vocabulary-Batc_488))+(ref_PCollection_PCollection_269/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((Analyze/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Read)+(Analyze/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Merge))+(Analyze/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_4-vocabulary-TotalVocabSi_470))+(ref_PCollection_PCollection_259/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_4-vocabulary-TotalVocabSi_472)+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_4-vocabulary-TotalVocabSi_473))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_4-vocabulary-TotalVocabSi_475))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_4-vocabulary-TotalVocabSi_476))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_4-vocabulary-ToInt64_477))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-compute_and_apply_vocabulary_4-vocabulary-temporar_479))+(ref_PCollection_PCollection_265/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex2-Write-WriteImpl-DoOnce-Impulse_937)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex2-Write-WriteImpl-DoOnce-FlatMap-_938))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex2-Write-WriteImpl-DoOnce-Map-deco_940))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex2-Write-WriteImpl-InitializeWrite_941))+(ref_PCollection_PCollection_531/Write))+(ref_PCollection_PCollection_532/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((((((((Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_2#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Group/Read)+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_2#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Merge))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_2#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/ExtractOutputs))+(Analyze/FlattenCache[VocabularyMerge[compute_and_apply_vocabulary_2#vocabulary]]/Flatten/Transcode/0))+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_2-vocabu_795))+(ref_AppliedPTransform_Analyze-FlattenCache-VocabularyMerge-compute_and_apply_vocabulary_2-vocabulary_304))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_2#vocabulary]/MergeCountPerToken/Precombine))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_2#vocabulary]/MergeCountPerToken/Group/Write))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex2-Write-WriteImpl-WindowInto-Wind_942))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex2-Write-WriteImpl-WriteBundles_943))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex2-Write-WriteImpl-Pair_944))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex2]/Write/WriteImpl/GroupByKey/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((((((((Analyze/VocabularyMerge[compute_and_apply_vocabulary_2#vocabulary]/MergeCountPerToken/Group/Read)+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_2#vocabulary]/MergeCountPerToken/Merge))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_2#vocabulary]/MergeCountPerToken/ExtractOutputs))+(ref_AppliedPTransform_Analyze-VocabularyMerge-compute_and_apply_vocabulary_2-vocabulary-SwapTokensAn_311))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_2-vocabulary-TotalVocabSi_315))+(ref_AppliedPTransform_Analyze-VocabularyPrune-compute_and_apply_vocabulary_2-vocabulary-ApplyThresho_332))+(Analyze/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Precombine))+(Analyze/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Write))+(ref_AppliedPTransform_Analyze-VocabularyPrune-compute_and_apply_vocabulary_2-vocabulary-ApplyThresho_333))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_2-vocabulary-Batc_337))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_2-vocabulary-Batc_338))+(ref_PCollection_PCollection_185/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((Analyze/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Read)+(Analyze/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Merge))+(Analyze/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_2-vocabulary-TotalVocabSi_320))+(ref_PCollection_PCollection_175/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_2-vocabulary-TotalVocabSi_322)+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_2-vocabulary-TotalVocabSi_323))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_2-vocabulary-TotalVocabSi_325))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_2-vocabulary-TotalVocabSi_326))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_2-vocabulary-ToInt64_327))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-compute_and_apply_vocabulary_2-vocabulary-temporar_329))+(ref_PCollection_PCollection_181/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_4-vocabulary-Writ_499)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_4-vocabulary-Writ_500))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_4-vocabulary-Writ_502))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_4-vocabulary-Writ_503))+(ref_PCollection_PCollection_276/Write))+(ref_PCollection_PCollection_277/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_4-vocabulary-Prep_490)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_4-vocabulary-Prep_491))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_4-vocabulary-Prep_493))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_4-vocabulary-Orde_494))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_4-vocabulary-Writ_504))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_4-vocabulary-Writ_505))+(Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_4-vocabulary-Writ_507))+(ref_PCollection_PCollection_281/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_276/Read)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_4-vocabulary-Writ_508))+(ref_PCollection_PCollection_282/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_276/Read)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_4-vocabulary-Writ_509))+(ref_PCollection_PCollection_283/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.07097768783569336 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.50 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_4-vocabulary-Crea_511)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_4-vocabulary-Crea_512))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_4-vocabulary-Crea_514))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_4-vocabulary-Wait_515))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-compute_and_apply_vocabulary_4-vocabulary-temporar_517))+(ref_PCollection_PCollection_288/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_5-vocabulary-Writ_574)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_5-vocabulary-Writ_575))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_5-vocabulary-Writ_577))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_5-vocabulary-Writ_578))+(ref_PCollection_PCollection_318/Write))+(ref_PCollection_PCollection_319/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_5-vocabulary-Prep_565)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_5-vocabulary-Prep_566))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_5-vocabulary-Prep_568))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_5-vocabulary-Orde_569))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_5-vocabulary-Writ_579))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_5-vocabulary-Writ_580))+(Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_5-vocabulary-Writ_582))+(ref_PCollection_PCollection_323/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_318/Read)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_5-vocabulary-Writ_583))+(ref_PCollection_PCollection_324/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_318/Read)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_5-vocabulary-Writ_584))+(ref_PCollection_PCollection_325/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.07343363761901855 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.50 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_5-vocabulary-Crea_586)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_5-vocabulary-Crea_587))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_5-vocabulary-Crea_589))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_5-vocabulary-Wait_590))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-compute_and_apply_vocabulary_5-vocabulary-temporar_592))+(ref_PCollection_PCollection_330/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_3-vocabulary-Writ_424)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_3-vocabulary-Writ_425))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_3-vocabulary-Writ_427))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_3-vocabulary-Writ_428))+(ref_PCollection_PCollection_234/Write))+(ref_PCollection_PCollection_235/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex3-Write-WriteImpl-DoOnce-Impulse_953)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex3-Write-WriteImpl-DoOnce-FlatMap-_954))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex3-Write-WriteImpl-DoOnce-Map-deco_956))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex3-Write-WriteImpl-InitializeWrite_957))+(ref_PCollection_PCollection_542/Write))+(ref_PCollection_PCollection_543/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((((((((Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_3#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Group/Read)+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_3#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Merge))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_3#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/ExtractOutputs))+(Analyze/FlattenCache[VocabularyMerge[compute_and_apply_vocabulary_3#vocabulary]]/Flatten/Transcode/0))+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_3-vocabu_804))+(ref_AppliedPTransform_Analyze-FlattenCache-VocabularyMerge-compute_and_apply_vocabulary_3-vocabulary_379))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_3#vocabulary]/MergeCountPerToken/Precombine))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_3#vocabulary]/MergeCountPerToken/Group/Write))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex3-Write-WriteImpl-WindowInto-Wind_958))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex3-Write-WriteImpl-WriteBundles_959))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex3-Write-WriteImpl-Pair_960))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex3]/Write/WriteImpl/GroupByKey/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((((((((Analyze/VocabularyMerge[compute_and_apply_vocabulary_3#vocabulary]/MergeCountPerToken/Group/Read)+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_3#vocabulary]/MergeCountPerToken/Merge))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_3#vocabulary]/MergeCountPerToken/ExtractOutputs))+(ref_AppliedPTransform_Analyze-VocabularyMerge-compute_and_apply_vocabulary_3-vocabulary-SwapTokensAn_386))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_3-vocabulary-TotalVocabSi_390))+(ref_AppliedPTransform_Analyze-VocabularyPrune-compute_and_apply_vocabulary_3-vocabulary-ApplyThresho_407))+(Analyze/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Precombine))+(Analyze/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Write))+(ref_AppliedPTransform_Analyze-VocabularyPrune-compute_and_apply_vocabulary_3-vocabulary-ApplyThresho_408))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_3-vocabulary-Batc_412))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_3-vocabulary-Batc_413))+(ref_PCollection_PCollection_227/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_3-vocabulary-Prep_415)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_3-vocabulary-Prep_416))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_3-vocabulary-Prep_418))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_3-vocabulary-Orde_419))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_3-vocabulary-Writ_429))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_3-vocabulary-Writ_430))+(Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_3-vocabulary-Writ_432))+(ref_PCollection_PCollection_239/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_234/Read)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_3-vocabulary-Writ_433))+(ref_PCollection_PCollection_240/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_234/Read)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_3-vocabulary-Writ_434))+(ref_PCollection_PCollection_241/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.07601022720336914 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.60 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_3-vocabulary-Crea_436)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_3-vocabulary-Crea_437))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_3-vocabulary-Crea_439))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_3-vocabulary-Wait_440))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-compute_and_apply_vocabulary_3-vocabulary-temporar_442))+(ref_PCollection_PCollection_246/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_2-vocabulary-Writ_349)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_2-vocabulary-Writ_350))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_2-vocabulary-Writ_352))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_2-vocabulary-Writ_353))+(ref_PCollection_PCollection_192/Write))+(ref_PCollection_PCollection_193/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_2-vocabulary-Prep_340)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_2-vocabulary-Prep_341))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_2-vocabulary-Prep_343))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_2-vocabulary-Orde_344))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_2-vocabulary-Writ_354))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_2-vocabulary-Writ_355))+(Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_2-vocabulary-Writ_357))+(ref_PCollection_PCollection_197/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_192/Read)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_2-vocabulary-Writ_358))+(ref_PCollection_PCollection_198/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_192/Read)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_2-vocabulary-Writ_359))+(ref_PCollection_PCollection_199/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.07392072677612305 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.50 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_2-vocabulary-Crea_361)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_2-vocabulary-Crea_362))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_2-vocabulary-Crea_364))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_2-vocabulary-Wait_365))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-compute_and_apply_vocabulary_2-vocabulary-temporar_367))+(ref_PCollection_PCollection_204/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((Analyze/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Read)+(Analyze/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Merge))+(Analyze/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_3-vocabulary-TotalVocabSi_395))+(ref_PCollection_PCollection_217/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_3-vocabulary-TotalVocabSi_397)+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_3-vocabulary-TotalVocabSi_398))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_3-vocabulary-TotalVocabSi_400))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_3-vocabulary-TotalVocabSi_401))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_3-vocabulary-ToInt64_402))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-compute_and_apply_vocabulary_3-vocabulary-temporar_404))+(ref_PCollection_PCollection_223/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex1-Write-WriteImpl-DoOnce-Impulse_921)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex1-Write-WriteImpl-DoOnce-FlatMap-_922))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex1-Write-WriteImpl-DoOnce-Map-deco_924))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex1-Write-WriteImpl-InitializeWrite_925))+(ref_PCollection_PCollection_520/Write))+(ref_PCollection_PCollection_521/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((((((((Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_1#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Group/Read)+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_1#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Merge))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_1#vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/ExtractOutputs))+(Analyze/FlattenCache[VocabularyMerge[compute_and_apply_vocabulary_1#vocabulary]]/Flatten/Transcode/0))+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_1-vocabu_786))+(ref_AppliedPTransform_Analyze-FlattenCache-VocabularyMerge-compute_and_apply_vocabulary_1-vocabulary_229))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_1#vocabulary]/MergeCountPerToken/Precombine))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_1#vocabulary]/MergeCountPerToken/Group/Write))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex1-Write-WriteImpl-WindowInto-Wind_926))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex1-Write-WriteImpl-WriteBundles_927))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex1-Write-WriteImpl-Pair_928))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex1]/Write/WriteImpl/GroupByKey/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((((((((Analyze/VocabularyMerge[compute_and_apply_vocabulary_1#vocabulary]/MergeCountPerToken/Group/Read)+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_1#vocabulary]/MergeCountPerToken/Merge))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_1#vocabulary]/MergeCountPerToken/ExtractOutputs))+(ref_AppliedPTransform_Analyze-VocabularyMerge-compute_and_apply_vocabulary_1-vocabulary-SwapTokensAn_236))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_1-vocabulary-TotalVocabSi_240))+(ref_AppliedPTransform_Analyze-VocabularyPrune-compute_and_apply_vocabulary_1-vocabulary-ApplyThresho_257))+(Analyze/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Precombine))+(Analyze/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Write))+(ref_AppliedPTransform_Analyze-VocabularyPrune-compute_and_apply_vocabulary_1-vocabulary-ApplyThresho_258))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_1-vocabulary-Batc_262))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_1-vocabulary-Batc_263))+(ref_PCollection_PCollection_143/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((Analyze/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Read)+(Analyze/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Merge))+(Analyze/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_1-vocabulary-TotalVocabSi_245))+(ref_PCollection_PCollection_133/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_1-vocabulary-TotalVocabSi_247)+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_1-vocabulary-TotalVocabSi_248))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_1-vocabulary-TotalVocabSi_250))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_1-vocabulary-TotalVocabSi_251))+(ref_AppliedPTransform_Analyze-VocabularyCount-compute_and_apply_vocabulary_1-vocabulary-ToInt64_252))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-compute_and_apply_vocabulary_1-vocabulary-temporar_254))+(ref_PCollection_PCollection_139/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_6-vocabulary-Writ_649)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_6-vocabulary-Writ_650))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_6-vocabulary-Writ_652))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_6-vocabulary-Writ_653))+(ref_PCollection_PCollection_360/Write))+(ref_PCollection_PCollection_361/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_6-vocabulary-Prep_640)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_6-vocabulary-Prep_641))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_6-vocabulary-Prep_643))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_6-vocabulary-Orde_644))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_6-vocabulary-Writ_654))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_6-vocabulary-Writ_655))+(Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_6-vocabulary-Writ_657))+(ref_PCollection_PCollection_365/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_360/Read)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_6-vocabulary-Writ_658))+(ref_PCollection_PCollection_366/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_360/Read)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_6-vocabulary-Writ_659))+(ref_PCollection_PCollection_367/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.07271194458007812 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.50 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_6-vocabulary-Crea_661)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_6-vocabulary-Crea_662))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_6-vocabulary-Crea_664))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_6-vocabulary-Wait_665))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-compute_and_apply_vocabulary_6-vocabulary-temporar_667))+(ref_PCollection_PCollection_372/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_1-vocabulary-Prep_265)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_1-vocabulary-Prep_266))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_1-vocabulary-Prep_268))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_1-vocabulary-Orde_269))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_1-vocabulary-Writ_279))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_1-vocabulary-Writ_280))+(Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_1-vocabulary-Writ_274)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_1-vocabulary-Writ_275))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_1-vocabulary-Writ_277))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_1-vocabulary-Writ_278))+(ref_PCollection_PCollection_150/Write))+(ref_PCollection_PCollection_151/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_1-vocabulary-Writ_282))+(ref_PCollection_PCollection_155/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_150/Read)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_1-vocabulary-Writ_283))+(ref_PCollection_PCollection_156/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_150/Read)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_1-vocabulary-Writ_284))+(ref_PCollection_PCollection_157/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.07420468330383301 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.40 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_1-vocabulary-Crea_286)+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_1-vocabulary-Crea_287))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_1-vocabulary-Crea_289))+(ref_AppliedPTransform_Analyze-VocabularyOrderAndWrite-compute_and_apply_vocabulary_1-vocabulary-Wait_290))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-compute_and_apply_vocabulary_1-vocabulary-temporar_292))+(ref_PCollection_PCollection_162/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((((((((ref_AppliedPTransform_Analyze-CreateSavedModel-tf_v2_only-CreateSole-Impulse_763)+(ref_AppliedPTransform_Analyze-CreateSavedModel-tf_v2_only-CreateSole-FlatMap-lambda-at-core-py-2979-_764))+(ref_AppliedPTransform_Analyze-CreateSavedModel-tf_v2_only-CreateSole-Map-decode-_766))+(ref_AppliedPTransform_Analyze-CreateSavedModel-tf_v2_only-ReplaceWithConstants_767))+(ref_AppliedPTransform_Analyze-CreateSavedModel-tf_v2_only-CreateSavedModel_768))+(ref_AppliedPTransform_Analyze-ComputeDeferredMetadata-compat_v1-False-_875))+(ref_AppliedPTransform_Analyze-MakeCheapBarrier_876))+(ref_AppliedPTransform_WriteTransformFn-WriteTransformFnToTemp_893))+(ref_PCollection_PCollection_431/Write))+(ref_AppliedPTransform_WriteTransformFn-WriteMetadataToTemp-WriteMetadata_892))+(ref_PCollection_PCollection_492/Write))+(ref_PCollection_PCollection_501/Write))+(ref_PCollection_PCollection_502/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Assets written to: gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/DataTransformer/transform_graph/9/.temp_path/tftransform_tmp/3a85c4f27b664043bb98f567a38c7d96/assets\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((ref_AppliedPTransform_WriteTransformFn-CreateSole-Impulse_895)+(ref_AppliedPTransform_WriteTransformFn-CreateSole-FlatMap-lambda-at-core-py-2979-_896))+(ref_AppliedPTransform_WriteTransformFn-CreateSole-Map-decode-_898))+(ref_AppliedPTransform_WriteTransformFn-PublishMetadataAndTransformFn_899))+(ref_PCollection_PCollection_506/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_Materialize-TransformIndex0-Write-Write-WriteImpl-DoOnce-Impulse_1370)+(ref_AppliedPTransform_Materialize-TransformIndex0-Write-Write-WriteImpl-DoOnce-FlatMap-lambda-at-cor_1371))+(ref_AppliedPTransform_Materialize-TransformIndex0-Write-Write-WriteImpl-DoOnce-Map-decode-_1373))+(ref_AppliedPTransform_Materialize-TransformIndex0-Write-Write-WriteImpl-InitializeWrite_1374))+(ref_PCollection_PCollection_802/Write))+(ref_PCollection_PCollection_803/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((ref_AppliedPTransform_TFXIOReadAndDecode-TransformIndex0-RawRecordBeamSource-ReadRawRecords-ReadFrom_1185)+(ref_AppliedPTransform_TFXIOReadAndDecode-TransformIndex0-RawRecordBeamSource-ReadRawRecords-ReadFrom_1186))+(TFXIOReadAndDecode[TransformIndex0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(TFXIOReadAndDecode[TransformIndex0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_692_split/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.07492280006408691 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.0686647891998291 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((((((((((((((ref_PCollection_PCollection_692_split/Read)+(TFXIOReadAndDecode[TransformIndex0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_TFXIOReadAndDecode-TransformIndex0-RawRecordBeamSource-ReadRawRecords-FlattenP_1189))+(ref_AppliedPTransform_TFXIOReadAndDecode-TransformIndex0-RawRecordBeamSource-CollectRawRecordTelemet_1191))+(ref_AppliedPTransform_TFXIOReadAndDecode-TransformIndex0-RawRecordToRecordBatch-RawRecordToRecordBat_1195))+(ref_AppliedPTransform_TFXIOReadAndDecode-TransformIndex0-RawRecordToRecordBatch-RawRecordToRecordBat_1196))+(ref_AppliedPTransform_TFXIOReadAndDecode-TransformIndex0-RawRecordToRecordBatch-CollectRecordBatchTe_1198))+(ref_AppliedPTransform_Transform-TransformIndex0-Transform_1200))+(ref_AppliedPTransform_Transform-TransformIndex0-ConvertToRecordBatch_1201))+(ref_AppliedPTransform_Transform-TransformIndex0-MakeCheapBarrier_1202))+(ref_AppliedPTransform_ExtractRecordBatches-TransformIndex0-Keys_1239))+(ref_AppliedPTransform_EncodeAndSerialize-TransformIndex0-_1362))+(ref_PCollection_PCollection_701/Write))+(FlattenTransformedDatasets/Write/0))+(ref_AppliedPTransform_Materialize-TransformIndex0-Values-Values_1365))+(ref_AppliedPTransform_Materialize-TransformIndex0-Write-Write-WriteImpl-WindowInto-WindowIntoFn-_1375))+(ref_AppliedPTransform_Materialize-TransformIndex0-Write-Write-WriteImpl-WriteBundles_1376))+(ref_AppliedPTransform_Materialize-TransformIndex0-Write-Write-WriteImpl-Pair_1377))+(Materialize[TransformIndex0]/Write/Write/WriteImpl/GroupByKey/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((ref_AppliedPTransform_TFXIOReadAndDecode-TransformIndex1-RawRecordBeamSource-ReadRawRecords-ReadFrom_1214)+(ref_AppliedPTransform_TFXIOReadAndDecode-TransformIndex1-RawRecordBeamSource-ReadRawRecords-ReadFrom_1215))+(TFXIOReadAndDecode[TransformIndex1]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(TFXIOReadAndDecode[TransformIndex1]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_707_split/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.07274079322814941 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.07897734642028809 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_Materialize-TransformIndex1-Write-Write-WriteImpl-DoOnce-Impulse_1390)+(ref_AppliedPTransform_Materialize-TransformIndex1-Write-Write-WriteImpl-DoOnce-FlatMap-lambda-at-cor_1391))+(ref_AppliedPTransform_Materialize-TransformIndex1-Write-Write-WriteImpl-DoOnce-Map-decode-_1393))+(ref_AppliedPTransform_Materialize-TransformIndex1-Write-Write-WriteImpl-InitializeWrite_1394))+(ref_PCollection_PCollection_815/Write))+(ref_PCollection_PCollection_816/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((((((((((((((ref_PCollection_PCollection_707_split/Read)+(TFXIOReadAndDecode[TransformIndex1]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_TFXIOReadAndDecode-TransformIndex1-RawRecordBeamSource-ReadRawRecords-FlattenP_1218))+(ref_AppliedPTransform_TFXIOReadAndDecode-TransformIndex1-RawRecordBeamSource-CollectRawRecordTelemet_1220))+(ref_AppliedPTransform_TFXIOReadAndDecode-TransformIndex1-RawRecordToRecordBatch-RawRecordToRecordBat_1224))+(ref_AppliedPTransform_TFXIOReadAndDecode-TransformIndex1-RawRecordToRecordBatch-RawRecordToRecordBat_1225))+(ref_AppliedPTransform_TFXIOReadAndDecode-TransformIndex1-RawRecordToRecordBatch-CollectRecordBatchTe_1227))+(ref_AppliedPTransform_Transform-TransformIndex1-Transform_1229))+(ref_AppliedPTransform_Transform-TransformIndex1-ConvertToRecordBatch_1230))+(ref_AppliedPTransform_Transform-TransformIndex1-MakeCheapBarrier_1231))+(ref_AppliedPTransform_ExtractRecordBatches-TransformIndex1-Keys_1241))+(ref_AppliedPTransform_EncodeAndSerialize-TransformIndex1-_1382))+(ref_PCollection_PCollection_716/Write))+(FlattenTransformedDatasets/Write/1))+(ref_AppliedPTransform_Materialize-TransformIndex1-Values-Values_1385))+(ref_AppliedPTransform_Materialize-TransformIndex1-Write-Write-WriteImpl-WindowInto-WindowIntoFn-_1395))+(ref_AppliedPTransform_Materialize-TransformIndex1-Write-Write-WriteImpl-WriteBundles_1396))+(ref_AppliedPTransform_Materialize-TransformIndex1-Write-Write-WriteImpl-Pair_1397))+(Materialize[TransformIndex1]/Write/Write/WriteImpl/GroupByKey/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((((((((FlattenTransformedDatasets/Read)+(ref_AppliedPTransform_WaitForTransformWrite_1243))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-FilterInternalColumn_1245))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1248))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1251))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1276))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/Precombine))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/Group/Write))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/Flatten/Transcode/0))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1277))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PreCombineFn)/Precombine))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PreCombineFn)/Group/Write))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/Flatten/Write/0)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PreCombineFn)/Group/Read)+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PreCombineFn)/Merge))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PreCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1282))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1283))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/Flatten/Transcode/1))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/Flatten/Write/1)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/Flatten/Read)+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PostCombineFn)/Precombine))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PostCombineFn)/Group/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PostCombineFn)/Group/Read)+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PostCombineFn)/Merge))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PostCombineFn)/ExtractOutputs))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Transcode/1))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Write/1)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((((((GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/Group/Read)+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/Merge))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/ExtractOutputs))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1256))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/Precombine))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1265))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/Group/Write))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1267))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Precombine))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Group/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/Group/Read)+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/Merge))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1262))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1263))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesFeatureStatsProtos/Write/0)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Group/Read)+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Merge))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1272))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesFeatureStatsProtos/Write/1)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesFeatureStatsProtos/Read)+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Transcode/0))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Write/0)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Read)+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Precombine))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Group/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Group/Read)+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Merge))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/ExtractOutputs))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1294))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1297))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Precombine))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Read)+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Merge))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1302))+(ref_PCollection_PCollection_758/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((((((((ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1304)+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1305))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1307))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1308))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1309))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteStats-WriteStats-Wr_1320))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-ValidateStatistics_1346))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteStats-WriteStats-Wr_1321))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/WriteStats/WriteStats/Write/WriteImpl/GroupByKey/Write))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteValidation-Write-Wr_1356))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteValidation-Write-Wr_1357))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/WriteValidation/Write/WriteImpl/GroupByKey/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((GenerateAndValidateStats[FlattenedTransformedDatasets]/WriteStats/WriteStats/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteStats-WriteStats-Wr_1323))+(ref_PCollection_PCollection_771/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_766/Read)+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteStats-WriteStats-Wr_1324))+(ref_PCollection_PCollection_772/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex6]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex6-Write-WriteImpl-Extract_1010))+(ref_PCollection_PCollection_581/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex7]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex7-Write-WriteImpl-Extract_1026))+(ref_PCollection_PCollection_592/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_586/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex7-Write-WriteImpl-PreFinalize_1027))+(ref_PCollection_PCollection_593/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.0802910327911377 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_586/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex7-Write-WriteImpl-FinalizeWrite_1028)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.07370638847351074 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.07533621788024902 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.40 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Transform-TransformIndex1-PrepareToClearSharedKeepAlives-Impulse_1233)+(ref_AppliedPTransform_Transform-TransformIndex1-PrepareToClearSharedKeepAlives-FlatMap-lambda-at-cor_1234))+(ref_AppliedPTransform_Transform-TransformIndex1-PrepareToClearSharedKeepAlives-Map-decode-_1236))+(ref_AppliedPTransform_Transform-TransformIndex1-WaitAndClearSharedKeepAlives_1237)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-CreateSchema-Impulse_1327)+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-CreateSchema-FlatMap-lam_1328))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-CreateSchema-Map-decode-_1330))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteSchema-Write-WriteI_1340))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteSchema-Write-WriteI_1341))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/WriteSchema/Write/WriteImpl/GroupByKey/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((GenerateAndValidateStats[FlattenedTransformedDatasets]/WriteSchema/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteSchema-Write-WriteI_1343))+(ref_PCollection_PCollection_784/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_779/Read)+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteSchema-Write-WriteI_1344))+(ref_PCollection_PCollection_785/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_779/Read)+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteSchema-Write-WriteI_1345)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.0742485523223877 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.50 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_766/Read)+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteStats-WriteStats-Wr_1325)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.07562518119812012 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.50 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_575/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex6-Write-WriteImpl-PreFinalize_1011))+(ref_PCollection_PCollection_582/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.07544803619384766 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_575/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex6-Write-WriteImpl-FinalizeWrite_1012)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.06737470626831055 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.07223176956176758 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.50 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PreCombineFn)/Group/Read)+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PreCombineFn)/Merge))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PreCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-G_1116))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-G_1117))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/Flatten/Transcode/1))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/Flatten/Write/1)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/Flatten/Read)+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PostCombineFn)/Precombine))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PostCombineFn)/Group/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PostCombineFn)/Group/Read)+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PostCombineFn)/Merge))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PostCombineFn)/ExtractOutputs))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Transcode/1))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Write/1)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((((((GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/Group/Read)+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/Merge))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/ExtractOutputs))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-G_1090))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/Precombine))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-G_1099))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/Group/Write))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-G_1101))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Precombine))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Group/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Group/Read)+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Merge))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-G_1106))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesFeatureStatsProtos/Write/1)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/Group/Read)+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/Merge))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-G_1096))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-G_1097))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesFeatureStatsProtos/Write/0)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesFeatureStatsProtos/Read)+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Transcode/0))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Write/0)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Read)+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Precombine))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Group/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Group/Read)+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Merge))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/ExtractOutputs))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-G_1128))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-G_1131))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Precombine))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Read)+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Merge))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-G_1136))+(ref_PCollection_PCollection_662/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((((ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-G_1138)+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-G_1139))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-G_1141))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-G_1142))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-G_1143))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-WriteStats-WriteStats-Write-WriteImpl-M_1154))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-WriteStats-WriteStats-Write-WriteImpl-W_1155))+(GenerateStats[FlattenedAnalysisDataset]/WriteStats/WriteStats/Write/WriteImpl/GroupByKey/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_2-vocabu_798)+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_2-vocabu_799))+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_2-vocabu_801))+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_2-vocabu_802)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex9]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex9-Write-WriteImpl-Extract_1058))+(ref_PCollection_PCollection_614/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_608/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex9-Write-WriteImpl-PreFinalize_1059))+(ref_PCollection_PCollection_615/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.07546353340148926 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_608/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex9-Write-WriteImpl-FinalizeWrite_1060)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.07687807083129883 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.07133293151855469 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.40 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_1-vocabu_789)+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_1-vocabu_790))+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_1-vocabu_792))+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_1-vocabu_793)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_3-vocabu_807)+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_3-vocabu_808))+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_3-vocabu_810))+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_3-vocabu_811)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex5]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex5-Write-WriteImpl-Extract_994))+(ref_PCollection_PCollection_570/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_564/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex5-Write-WriteImpl-PreFinalize_995))+(ref_PCollection_PCollection_571/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.07480382919311523 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteValidation-Write-Wr_1351)+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteValidation-Write-Wr_1352))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteValidation-Write-Wr_1354))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteValidation-Write-Wr_1355))+(ref_PCollection_PCollection_790/Write))+(ref_PCollection_PCollection_791/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((GenerateAndValidateStats[FlattenedTransformedDatasets]/WriteValidation/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteValidation-Write-Wr_1359))+(ref_PCollection_PCollection_795/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_790/Read)+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteValidation-Write-Wr_1360))+(ref_PCollection_PCollection_796/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex1]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex1-Write-WriteImpl-Extract_930))+(ref_PCollection_PCollection_526/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_520/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex1-Write-WriteImpl-PreFinalize_931))+(ref_PCollection_PCollection_527/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.07139801979064941 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_520/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex1-Write-WriteImpl-FinalizeWrite_932)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.07104825973510742 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.06843066215515137 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.40 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_564/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex5-Write-WriteImpl-FinalizeWrite_996)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.08022475242614746 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.06671404838562012 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.50 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex8]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex8-Write-WriteImpl-Extract_1042))+(ref_PCollection_PCollection_603/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_597/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex8-Write-WriteImpl-PreFinalize_1043))+(ref_PCollection_PCollection_604/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.07050776481628418 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_597/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex8-Write-WriteImpl-FinalizeWrite_1044)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.07381105422973633 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.0772395133972168 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.50 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex4]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex4-Write-WriteImpl-Extract_978))+(ref_PCollection_PCollection_559/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_553/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex4-Write-WriteImpl-PreFinalize_979))+(ref_PCollection_PCollection_560/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.08110618591308594 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_553/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex4-Write-WriteImpl-FinalizeWrite_980)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.07943034172058105 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.0812678337097168 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.40 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_7-vocabu_870)+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_7-vocabu_871))+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_7-vocabu_873))+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_7-vocabu_874)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-WriteStats-WriteStats-Write-WriteImpl-D_1149)+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-WriteStats-WriteStats-Write-WriteImpl-D_1150))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-WriteStats-WriteStats-Write-WriteImpl-D_1152))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-WriteStats-WriteStats-Write-WriteImpl-I_1153))+(ref_PCollection_PCollection_670/Write))+(ref_PCollection_PCollection_671/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((GenerateStats[FlattenedAnalysisDataset]/WriteStats/WriteStats/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-WriteStats-WriteStats-Write-WriteImpl-W_1157))+(ref_PCollection_PCollection_675/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_670/Read)+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-WriteStats-WriteStats-Write-WriteImpl-P_1158))+(ref_PCollection_PCollection_676/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_670/Read)+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-WriteStats-WriteStats-Write-WriteImpl-F_1159)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.10564899444580078 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.50 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Analyze-CreateSavedModelForAnalyzerInputs-Phase0-tf_v2_only-Count-CreateSole-I_51)+(ref_AppliedPTransform_Analyze-CreateSavedModelForAnalyzerInputs-Phase0-tf_v2_only-Count-CreateSole-F_52))+(ref_AppliedPTransform_Analyze-CreateSavedModelForAnalyzerInputs-Phase0-tf_v2_only-Count-CreateSole-M_54))+(ref_AppliedPTransform_Analyze-CreateSavedModelForAnalyzerInputs-Phase0-tf_v2_only-Count-Count_55)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Analyze-InstrumentAPI-CreateSoleAPIUse-Impulse_37)+(ref_AppliedPTransform_Analyze-InstrumentAPI-CreateSoleAPIUse-FlatMap-lambda-at-core-py-2979-_38))+(ref_AppliedPTransform_Analyze-InstrumentAPI-CreateSoleAPIUse-Map-decode-_40))+(ref_AppliedPTransform_Analyze-InstrumentAPI-CountAPIUse_41)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Analyze-PrepareToClearSharedKeepAlives-Impulse_878)+(ref_AppliedPTransform_Analyze-PrepareToClearSharedKeepAlives-FlatMap-lambda-at-core-py-2979-_879))+(ref_AppliedPTransform_Analyze-PrepareToClearSharedKeepAlives-Map-decode-_881))+(ref_AppliedPTransform_Analyze-WaitAndClearSharedKeepAlives_882)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_z_score-mean_and_var-A_816)+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_z_score-mean_and_var-A_817))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_z_score-mean_and_var-A_819))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_z_score-mean_and_var-A_820)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex0]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex0-Write-WriteImpl-Extract_914))+(ref_PCollection_PCollection_515/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex3]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex3-Write-WriteImpl-Extract_962))+(ref_PCollection_PCollection_548/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_542/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex3-Write-WriteImpl-PreFinalize_963))+(ref_PCollection_PCollection_549/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.07013678550720215 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_509/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex0-Write-WriteImpl-PreFinalize_915))+(ref_PCollection_PCollection_516/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.07076549530029297 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_509/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex0-Write-WriteImpl-FinalizeWrite_916)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.07530832290649414 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.07416033744812012 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.40 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_WriteMetadata-Create-Impulse_885)+(ref_AppliedPTransform_WriteMetadata-Create-FlatMap-lambda-at-core-py-2979-_886))+(ref_AppliedPTransform_WriteMetadata-Create-Map-decode-_888))+(ref_AppliedPTransform_WriteMetadata-WriteMetadata_889)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_790/Read)+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteValidation-Write-Wr_1361)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.07383346557617188 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.40 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_z_score_1-mean_and_var_825)+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_z_score_1-mean_and_var_826))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_z_score_1-mean_and_var_828))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_z_score_1-mean_and_var_829)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_AppliedPTransform_OptimizeRun-WorkaroundForBug170304777-Impulse_4)+(ref_AppliedPTransform_OptimizeRun-WorkaroundForBug170304777-FlatMap-lambda-at-core-py-2979-_5))+(ref_AppliedPTransform_OptimizeRun-WorkaroundForBug170304777-Map-decode-_7)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex10]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex10-Write-WriteImpl-Extract_1074))+(ref_PCollection_PCollection_625/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_619/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex10-Write-WriteImpl-PreFinalize_1075))+(ref_PCollection_PCollection_626/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.07367873191833496 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex2]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex2-Write-WriteImpl-Extract_946))+(ref_PCollection_PCollection_537/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_531/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex2-Write-WriteImpl-PreFinalize_947))+(ref_PCollection_PCollection_538/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.07325983047485352 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_619/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex10-Write-WriteImpl-FinalizeWrite_1076)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.07459688186645508 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.07097697257995605 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.40 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_531/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex2-Write-WriteImpl-FinalizeWrite_948)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.0777137279510498 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.07545614242553711 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.50 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_542/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex3-Write-WriteImpl-FinalizeWrite_964)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.06589007377624512 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.07696127891540527 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.50 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_4-vocabu_834)+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_4-vocabu_835))+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_4-vocabu_837))+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_4-vocabu_838)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_6-vocabu_852)+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_6-vocabu_853))+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_6-vocabu_855))+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_6-vocabu_856)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_z_score_2-mean_and_var_861)+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_z_score_2-mean_and_var_862))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_z_score_2-mean_and_var_864))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_z_score_2-mean_and_var_865)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Analyze-PackedCombineMerge-3-Count-CreateSole-Impulse_129)+(ref_AppliedPTransform_Analyze-PackedCombineMerge-3-Count-CreateSole-FlatMap-lambda-at-core-py-2979-_130))+(ref_AppliedPTransform_Analyze-PackedCombineMerge-3-Count-CreateSole-Map-decode-_132))+(ref_AppliedPTransform_Analyze-PackedCombineMerge-3-Count-Count_133)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_5-vocabu_843)+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_5-vocabu_844))+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_5-vocabu_846))+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary_5-vocabu_847)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-WriteSchema-Write-WriteImpl-DoOnce-Impu_1169)+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-WriteSchema-Write-WriteImpl-DoOnce-Flat_1170))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-WriteSchema-Write-WriteImpl-DoOnce-Map-_1172))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-WriteSchema-Write-WriteImpl-InitializeW_1173))+(ref_PCollection_PCollection_683/Write))+(ref_PCollection_PCollection_684/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-CreateSchema-Impulse_1161)+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-CreateSchema-FlatMap-lambda-at-core-py-_1162))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-CreateSchema-Map-decode-_1164))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-WriteSchema-Write-WriteImpl-Map-lambda-_1174))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-WriteSchema-Write-WriteImpl-WindowInto-_1175))+(GenerateStats[FlattenedAnalysisDataset]/WriteSchema/Write/WriteImpl/GroupByKey/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((GenerateStats[FlattenedAnalysisDataset]/WriteSchema/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-WriteSchema-Write-WriteImpl-WriteBundle_1177))+(ref_PCollection_PCollection_688/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((Materialize[TransformIndex1]/Write/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_Materialize-TransformIndex1-Write-Write-WriteImpl-Extract_1399))+(ref_PCollection_PCollection_821/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_815/Read)+(ref_AppliedPTransform_Materialize-TransformIndex1-Write-Write-WriteImpl-PreFinalize_1400))+(ref_PCollection_PCollection_822/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.06750273704528809 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((Materialize[TransformIndex0]/Write/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_Materialize-TransformIndex0-Write-Write-WriteImpl-Extract_1379))+(ref_PCollection_PCollection_808/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_802/Read)+(ref_AppliedPTransform_Materialize-TransformIndex0-Write-Write-WriteImpl-PreFinalize_1380))+(ref_PCollection_PCollection_809/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.07344913482666016 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_683/Read)+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-WriteSchema-Write-WriteImpl-PreFinalize_1178))+(ref_PCollection_PCollection_689/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_683/Read)+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-WriteSchema-Write-WriteImpl-FinalizeWri_1179)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.07727956771850586 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.60 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary-vocabula_780)+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary-vocabula_781))+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary-vocabula_783))+(ref_AppliedPTransform_Analyze-EncodeCache-VocabularyAccumulate-compute_and_apply_vocabulary-vocabula_784)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_IncrementPipelineMetrics-CreateSole-Impulse_10)+(ref_AppliedPTransform_IncrementPipelineMetrics-CreateSole-FlatMap-lambda-at-core-py-2979-_11))+(ref_AppliedPTransform_IncrementPipelineMetrics-CreateSole-Map-decode-_13))+(ref_AppliedPTransform_IncrementPipelineMetrics-Count_14)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Analyze-PackedCombineAccumulate-ApplySavedModel-Phase0-AnalysisIndex0-Count-Cr_88)+(ref_AppliedPTransform_Analyze-PackedCombineAccumulate-ApplySavedModel-Phase0-AnalysisIndex0-Count-Cr_89))+(ref_AppliedPTransform_Analyze-PackedCombineAccumulate-ApplySavedModel-Phase0-AnalysisIndex0-Count-Cr_91))+(ref_AppliedPTransform_Analyze-PackedCombineAccumulate-ApplySavedModel-Phase0-AnalysisIndex0-Count-Co_92)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_815/Read)+(ref_AppliedPTransform_Materialize-TransformIndex1-Write-Write-WriteImpl-FinalizeWrite_1401)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.07878327369689941 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.0720374584197998 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.50 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Transform-TransformIndex0-PrepareToClearSharedKeepAlives-Impulse_1204)+(ref_AppliedPTransform_Transform-TransformIndex0-PrepareToClearSharedKeepAlives-FlatMap-lambda-at-cor_1205))+(ref_AppliedPTransform_Transform-TransformIndex0-PrepareToClearSharedKeepAlives-Map-decode-_1207))+(ref_AppliedPTransform_Transform-TransformIndex0-WaitAndClearSharedKeepAlives_1208)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Analyze-CreateSavedModel-tf_v2_only-Count-CreateSole-Impulse_771)+(ref_AppliedPTransform_Analyze-CreateSavedModel-tf_v2_only-Count-CreateSole-FlatMap-lambda-at-core-py_772))+(ref_AppliedPTransform_Analyze-CreateSavedModel-tf_v2_only-Count-CreateSole-Map-decode-_774))+(ref_AppliedPTransform_Analyze-CreateSavedModel-tf_v2_only-Count-Count_775)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_802/Read)+(ref_AppliedPTransform_Materialize-TransformIndex0-Write-Write-WriteImpl-FinalizeWrite_1381)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.07049059867858887 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.07245135307312012 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.50 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": stateful_working_dir /workspace/mlops-with-vertex-ai/gs:/grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/DataTransformer/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Step #3 - \"Local Test E2E Pipeline\": I0330 17:23:20.500741     1 rdbms_metadata_access_object.cc:686] No property is defined for the Type\n",
      "Step #3 - \"Local Test E2E Pipeline\": I0330 17:23:20.507601     1 rdbms_metadata_access_object.cc:686] No property is defined for the Type\n",
      "Step #3 - \"Local Test E2E Pipeline\": Artifact type Model is not found in MLMD.\n",
      "Step #3 - \"Local Test E2E Pipeline\": I0330 17:23:20.544103     1 rdbms_metadata_access_object.cc:686] No property is defined for the Type\n",
      "Step #3 - \"Local Test E2E Pipeline\": Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
      "Step #3 - \"Local Test E2E Pipeline\": Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
      "Step #3 - \"Local Test E2E Pipeline\": Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
      "Step #3 - \"Local Test E2E Pipeline\": E0330 17:23:27.587220707       1 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "Step #3 - \"Local Test E2E Pipeline\": Processing /tmp/tmpc28zp51v/tfx_user_code_ModelTrainer-0.0+5bd9ac13044cc46c88b21ccdcff2b74d85a1c15be76527e33008964fa7da7d12-py3-none-any.whl\n",
      "Step #3 - \"Local Test E2E Pipeline\": Installing collected packages: tfx-user-code-ModelTrainer\n",
      "Step #3 - \"Local Test E2E Pipeline\": Successfully installed tfx-user-code-ModelTrainer-0.0+5bd9ac13044cc46c88b21ccdcff2b74d85a1c15be76527e33008964fa7da7d12\n",
      "Step #3 - \"Local Test E2E Pipeline\": WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "Step #3 - \"Local Test E2E Pipeline\": Runner started...\n",
      "Step #3 - \"Local Test E2E Pipeline\": fn_args: FnArgs(working_dir=None, train_files=['gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/DataTransformer/transformed_examples/9/Split-train/*'], eval_files=['gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/DataTransformer/transformed_examples/9/Split-eval/*'], train_steps=None, eval_steps=None, schema_path='src/raw_schema/schema.pbtxt', schema_file='src/raw_schema/schema.pbtxt', transform_graph_path='gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/DataTransformer/transform_graph/9', transform_output='gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/DataTransformer/transform_graph/9', data_accessor=DataAccessor(tf_dataset_factory=<function get_tf_dataset_factory_from_artifact.<locals>.dataset_factory at 0x7f5e131cfcb0>, record_batch_factory=<function get_record_batch_factory_from_artifact.<locals>.record_batch_factory at 0x7f5e11b8d680>, data_view_decode_fn=None), serving_model_dir='gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/ModelTrainer/model/10/Format-Serving', eval_model_dir='gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/ModelTrainer/model/10/Format-TFMA', model_run_dir='gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/ModelTrainer/model_run/10', base_model=None, hyperparameters={'num_epochs': 1, 'batch_size': 512, 'learning_rate': 0.001, 'hidden_units': [128, 128]}, custom_config=None)\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": Hyperparameter:\n",
      "Step #3 - \"Local Test E2E Pipeline\": {'num_epochs': 1, 'batch_size': 512, 'learning_rate': 0.001, 'hidden_units': [128, 128]}\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": Runner executing trainer...\n",
      "Step #3 - \"Local Test E2E Pipeline\": Loading tft output from gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/DataTransformer/transform_graph/9\n",
      "Step #3 - \"Local Test E2E Pipeline\": 2022-03-30 17:23:31.061225: I tensorflow/core/profiler/lib/profiler_session.cc:126] Profiler session initializing.\n",
      "Step #3 - \"Local Test E2E Pipeline\": 2022-03-30 17:23:31.061328: I tensorflow/core/profiler/lib/profiler_session.cc:141] Profiler session started.\n",
      "Step #3 - \"Local Test E2E Pipeline\": 2022-03-30 17:23:31.061425: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session tear down.\n",
      "Step #3 - \"Local Test E2E Pipeline\": max_tokens is deprecated, please use num_tokens instead.\n",
      "Step #3 - \"Local Test E2E Pipeline\": max_tokens is deprecated, please use num_tokens instead.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Model training started...\n",
      "Step #3 - \"Local Test E2E Pipeline\": 2022-03-30 17:23:37.615659: I tensorflow/core/profiler/lib/profiler_session.cc:126] Profiler session initializing.\n",
      "Step #3 - \"Local Test E2E Pipeline\": 2022-03-30 17:23:37.615731: I tensorflow/core/profiler/lib/profiler_session.cc:141] Profiler session started.\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.7491 - accuracy: 0.8809\n",
      "Step #3 - \"Local Test E2E Pipeline\": Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "Step #3 - \"Local Test E2E Pipeline\": 2022-03-30 17:23:39.654692: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "Step #3 - \"Local Test E2E Pipeline\": 2022-03-30 17:23:39.663212: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session tear down.\n",
      "Step #3 - \"Local Test E2E Pipeline\": 2022-03-30 17:23:41.606695: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/ModelTrainer/model_run/10/train/plugins/profile/2022_03_30_17_23_40\n",
      "Step #3 - \"Local Test E2E Pipeline\": 2022-03-30 17:23:41.873329: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for trace.json.gz to gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/ModelTrainer/model_run/10/train/plugins/profile/2022_03_30_17_23_40/a8cf452f19d9.trace.json.gz\n",
      "Step #3 - \"Local Test E2E Pipeline\": 2022-03-30 17:23:42.015901: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/ModelTrainer/model_run/10/train/plugins/profile/2022_03_30_17_23_40\n",
      "Step #3 - \"Local Test E2E Pipeline\": 2022-03-30 17:23:42.253122: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for memory_profile.json.gz to gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/ModelTrainer/model_run/10/train/plugins/profile/2022_03_30_17_23_40/a8cf452f19d9.memory_profile.json.gz\n",
      "Step #3 - \"Local Test E2E Pipeline\": 2022-03-30 17:23:43.846169: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/ModelTrainer/model_run/10/train/plugins/profile/2022_03_30_17_23_40Dumped tool data for xplane.pb to gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/ModelTrainer/model_run/10/train/plugins/profile/2022_03_30_17_23_40/a8cf452f19d9.xplane.pb\n",
      "Step #3 - \"Local Test E2E Pipeline\": Dumped tool data for overview_page.pb to gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/ModelTrainer/model_run/10/train/plugins/profile/2022_03_30_17_23_40/a8cf452f19d9.overview_page.pb\n",
      "Step #3 - \"Local Test E2E Pipeline\": Dumped tool data for input_pipeline.pb to gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/ModelTrainer/model_run/10/train/plugins/profile/2022_03_30_17_23_40/a8cf452f19d9.input_pipeline.pb\n",
      "Step #3 - \"Local Test E2E Pipeline\": Dumped tool data for tensorflow_stats.pb to gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/ModelTrainer/model_run/10/train/plugins/profile/2022_03_30_17_23_40/a8cf452f19d9.tensorflow_stats.pb\n",
      "Step #3 - \"Local Test E2E Pipeline\": Dumped tool data for kernel_stats.pb to gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/ModelTrainer/model_run/10/train/plugins/profile/2022_03_30_17_23_40/a8cf452f19d9.kernel_stats.pb\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": Model training completed.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Runner executing exporter...\n",
      "Step #3 - \"Local Test E2E Pipeline\": Model export started...\n",
      "Step #3 - \"Local Test E2E Pipeline\": Assets written to: gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/ModelTrainer/model/10/Format-Serving/assets\n",
      "Step #3 - \"Local Test E2E Pipeline\": Model export completed.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Runner completed.\n",
      "Step #3 - \"Local Test E2E Pipeline\": stateful_working_dir /workspace/mlops-with-vertex-ai/gs:/grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/ModelTrainer/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Step #3 - \"Local Test E2E Pipeline\": I0330 17:24:11.918556     1 rdbms_metadata_access_object.cc:686] No property is defined for the Type\n",
      "Step #3 - \"Local Test E2E Pipeline\": I0330 17:24:11.924962     1 rdbms_metadata_access_object.cc:686] No property is defined for the Type\n",
      "Step #3 - \"Local Test E2E Pipeline\": I0330 17:24:11.965267     1 rdbms_metadata_access_object.cc:686] No property is defined for the Type\n",
      "Step #3 - \"Local Test E2E Pipeline\": E0330 17:24:18.820245697       1 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "Step #3 - \"Local Test E2E Pipeline\": There are change thresholds, but the baseline is missing. This is allowed only when rubber stamping (first run).\n",
      "Step #3 - \"Local Test E2E Pipeline\": Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": Two checkpoint references resolved to different objects (<tensorflow.python.keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7f5e11ce5290> and <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f5e11e41e90>).\n",
      "Step #3 - \"Local Test E2E Pipeline\": Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.0915830135345459 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": Two checkpoint references resolved to different objects (<tensorflow.python.keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7f5e101050d0> and <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f5dad737790>).\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.Type[typing.Union[tensorflow_model_analysis.metrics.metric_types.MetricKey, tensorflow_model_analysis.metrics.metric_types.PlotKey, tensorflow_model_analysis.metrics.metric_types.AttributionsKey]]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.Type[typing.Union[tensorflow_model_analysis.metrics.metric_types.MetricKey, tensorflow_model_analysis.metrics.metric_types.PlotKey, tensorflow_model_analysis.metrics.metric_types.AttributionsKey]]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.Type[typing.Union[tensorflow_model_analysis.metrics.metric_types.MetricKey, tensorflow_model_analysis.metrics.metric_types.PlotKey, tensorflow_model_analysis.metrics.metric_types.AttributionsKey]]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Default Python SDK image for environment is apache/beam_python3.7_sdk:2.31.0\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function annotate_downstream_side_inputs at 0x7f5e1692f8c0> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function fix_side_input_pcoll_coders at 0x7f5e1692f9e0> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function pack_combiners at 0x7f5e1692fe60> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function lift_combiners at 0x7f5e1692fef0> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function expand_sdf at 0x7f5e169380e0> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function expand_gbk at 0x7f5e16938170> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function sink_flattens at 0x7f5e16938290> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function greedily_fuse at 0x7f5e16938320> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function read_to_impulse at 0x7f5e169383b0> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function impulse_to_input at 0x7f5e16938440> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function sort_stages at 0x7f5e16938680> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function setup_timer_mapping at 0x7f5e169385f0> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function populate_data_channel_coders at 0x7f5e16938710> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": Creating state cache with size 100\n",
      "Step #3 - \"Local Test E2E Pipeline\": Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7f5dafac2590> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteEvalConfig-WriteEvalConfig-Wr_124)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteEvalConfig-WriteEvalConfig-Wr_125))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteEvalConfig-WriteEvalConfig-Wr_127))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteEvalConfig-WriteEvalConfig-Wr_128))+(ref_PCollection_PCollection_65/Write))+(ref_PCollection_PCollection_66/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteEvalConfig-CreateEvalConfig-I_116)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteEvalConfig-CreateEvalConfig-F_117))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteEvalConfig-CreateEvalConfig-M_119))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteEvalConfig-WriteEvalConfig-Wr_129))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteEvalConfig-WriteEvalConfig-Wr_130))+(ExtractEvaluateAndWriteResults/WriteResults/WriteEvalConfig/WriteEvalConfig/Write/WriteImpl/GroupByKey/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ExtractEvaluateAndWriteResults/WriteResults/WriteEvalConfig/WriteEvalConfig/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteEvalConfig-WriteEvalConfig-Wr_132))+(ref_PCollection_PCollection_70/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((ref_AppliedPTransform_ReadFromTFRecordToArrow-test-0-RawRecordBeamSource-ReadRawRecords-ReadFromTFRe_7)+(ref_AppliedPTransform_ReadFromTFRecordToArrow-test-0-RawRecordBeamSource-ReadRawRecords-ReadFromTFRe_8))+(ReadFromTFRecordToArrow[test][0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(ReadFromTFRecordToArrow[test][0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_2_split/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.07405209541320801 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.07665014266967773 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((((((((((((((((((((((((((((ref_PCollection_PCollection_2_split/Read)+(ReadFromTFRecordToArrow[test][0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_ReadFromTFRecordToArrow-test-0-RawRecordBeamSource-ReadRawRecords-FlattenPColl_11))+(ref_AppliedPTransform_ReadFromTFRecordToArrow-test-0-RawRecordBeamSource-CollectRawRecordTelemetry-P_13))+(ref_AppliedPTransform_ReadFromTFRecordToArrow-test-0-RawRecordToRecordBatch-RawRecordToRecordBatch-B_17))+(ref_AppliedPTransform_ReadFromTFRecordToArrow-test-0-RawRecordToRecordBatch-RawRecordToRecordBatch-D_18))+(ref_AppliedPTransform_ReadFromTFRecordToArrow-test-0-RawRecordToRecordBatch-CollectRecordBatchTeleme_20))+(ref_AppliedPTransform_FlattenExamples_21))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-BatchedInputsToExtracts-AddArrowRecordBatchKey_24))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-ExtractFeatures-ExtractFeatu_27))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-ExtractTransformedFeatures-P_29))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-ExtractLabels-ExtractLabels_31))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-ExtractExampleWeights-Extrac_33))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-ExtractPredictions-Predict_35))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-ExtractUnbatchedInputs-Unbat_37))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-ExtractSliceKeys-ParDo-Extra_39))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_42))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_44))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_47))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_72))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_92))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_49))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/FanoutSlices/TrackDistinctSliceKeys/RemoveDuplicates/Group/Precombine))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/FanoutSlices/TrackDistinctSliceKeys/RemoveDuplicates/Group/Group/Write))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_74))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CountPerSliceKey/CombinePerKey(CountCombineFn)/Precombine))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CountPerSliceKey/CombinePerKey(CountCombineFn)/Group/Write))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CombineMetricsPerSlice/Flatten/Transcode/0))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_93))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CombineMetricsPerSlice/CombinePerKey(PreCombineFn)/Precombine))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CombineMetricsPerSlice/CombinePerKey(PreCombineFn)/Group/Write))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CombineMetricsPerSlice/Flatten/Write/0)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Exception ignored in: <function CapturableResource.__del__ at 0x7f5e366ad710>\n",
      "Step #3 - \"Local Test E2E Pipeline\": Traceback (most recent call last):\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py\", line 277, in __del__\n",
      "Step #3 - \"Local Test E2E Pipeline\":     self._destroy_resource()\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 889, in __call__\n",
      "Step #3 - \"Local Test E2E Pipeline\":     result = self._call(*args, **kwds)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 924, in _call\n",
      "Step #3 - \"Local Test E2E Pipeline\":     results = self._stateful_fn(*args, **kwds)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3022, in __call__\n",
      "Step #3 - \"Local Test E2E Pipeline\":     filtered_flat_args) = self._maybe_define_function(args, kwargs)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3444, in _maybe_define_function\n",
      "Step #3 - \"Local Test E2E Pipeline\":     graph_function = self._create_graph_function(args, kwargs)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3289, in _create_graph_function\n",
      "Step #3 - \"Local Test E2E Pipeline\":     capture_by_value=self._capture_by_value),\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 999, in func_graph_from_py_func\n",
      "Step #3 - \"Local Test E2E Pipeline\":     func_outputs = python_func(*func_args, **func_kwargs)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 672, in wrapped_fn\n",
      "Step #3 - \"Local Test E2E Pipeline\":     out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n",
      "Step #3 - \"Local Test E2E Pipeline\": AttributeError: 'NoneType' object has no attribute '__wrapped__'\n",
      "Step #3 - \"Local Test E2E Pipeline\": Exception ignored in: <function CapturableResource.__del__ at 0x7f5e366ad710>\n",
      "Step #3 - \"Local Test E2E Pipeline\": Traceback (most recent call last):\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py\", line 277, in __del__\n",
      "Step #3 - \"Local Test E2E Pipeline\":     self._destroy_resource()\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 889, in __call__\n",
      "Step #3 - \"Local Test E2E Pipeline\":     result = self._call(*args, **kwds)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 924, in _call\n",
      "Step #3 - \"Local Test E2E Pipeline\":     results = self._stateful_fn(*args, **kwds)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3022, in __call__\n",
      "Step #3 - \"Local Test E2E Pipeline\":     filtered_flat_args) = self._maybe_define_function(args, kwargs)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3444, in _maybe_define_function\n",
      "Step #3 - \"Local Test E2E Pipeline\":     graph_function = self._create_graph_function(args, kwargs)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3289, in _create_graph_function\n",
      "Step #3 - \"Local Test E2E Pipeline\":     capture_by_value=self._capture_by_value),\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 999, in func_graph_from_py_func\n",
      "Step #3 - \"Local Test E2E Pipeline\":     func_outputs = python_func(*func_args, **func_kwargs)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 672, in wrapped_fn\n",
      "Step #3 - \"Local Test E2E Pipeline\":     out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n",
      "Step #3 - \"Local Test E2E Pipeline\": AttributeError: 'NoneType' object has no attribute '__wrapped__'\n",
      "Step #3 - \"Local Test E2E Pipeline\": Exception ignored in: <function CapturableResource.__del__ at 0x7f5e366ad710>\n",
      "Step #3 - \"Local Test E2E Pipeline\": Traceback (most recent call last):\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py\", line 277, in __del__\n",
      "Step #3 - \"Local Test E2E Pipeline\":     self._destroy_resource()\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 889, in __call__\n",
      "Step #3 - \"Local Test E2E Pipeline\":     result = self._call(*args, **kwds)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 924, in _call\n",
      "Step #3 - \"Local Test E2E Pipeline\":     results = self._stateful_fn(*args, **kwds)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3022, in __call__\n",
      "Step #3 - \"Local Test E2E Pipeline\":     filtered_flat_args) = self._maybe_define_function(args, kwargs)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3444, in _maybe_define_function\n",
      "Step #3 - \"Local Test E2E Pipeline\":     graph_function = self._create_graph_function(args, kwargs)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3289, in _create_graph_function\n",
      "Step #3 - \"Local Test E2E Pipeline\":     capture_by_value=self._capture_by_value),\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 999, in func_graph_from_py_func\n",
      "Step #3 - \"Local Test E2E Pipeline\":     func_outputs = python_func(*func_args, **func_kwargs)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 672, in wrapped_fn\n",
      "Step #3 - \"Local Test E2E Pipeline\":     out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n",
      "Step #3 - \"Local Test E2E Pipeline\": AttributeError: 'NoneType' object has no attribute '__wrapped__'\n",
      "Step #3 - \"Local Test E2E Pipeline\": Exception ignored in: <function CapturableResource.__del__ at 0x7f5e366ad710>\n",
      "Step #3 - \"Local Test E2E Pipeline\": Traceback (most recent call last):\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py\", line 277, in __del__\n",
      "Step #3 - \"Local Test E2E Pipeline\":     self._destroy_resource()\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 889, in __call__\n",
      "Step #3 - \"Local Test E2E Pipeline\":     result = self._call(*args, **kwds)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 924, in _call\n",
      "Step #3 - \"Local Test E2E Pipeline\":     results = self._stateful_fn(*args, **kwds)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3022, in __call__\n",
      "Step #3 - \"Local Test E2E Pipeline\":     filtered_flat_args) = self._maybe_define_function(args, kwargs)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3444, in _maybe_define_function\n",
      "Step #3 - \"Local Test E2E Pipeline\":     graph_function = self._create_graph_function(args, kwargs)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3289, in _create_graph_function\n",
      "Step #3 - \"Local Test E2E Pipeline\":     capture_by_value=self._capture_by_value),\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 999, in func_graph_from_py_func\n",
      "Step #3 - \"Local Test E2E Pipeline\":     func_outputs = python_func(*func_args, **func_kwargs)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 672, in wrapped_fn\n",
      "Step #3 - \"Local Test E2E Pipeline\":     out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n",
      "Step #3 - \"Local Test E2E Pipeline\": AttributeError: 'NoneType' object has no attribute '__wrapped__'\n",
      "Step #3 - \"Local Test E2E Pipeline\": Exception ignored in: <function CapturableResource.__del__ at 0x7f5e366ad710>\n",
      "Step #3 - \"Local Test E2E Pipeline\": Traceback (most recent call last):\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py\", line 277, in __del__\n",
      "Step #3 - \"Local Test E2E Pipeline\":     self._destroy_resource()\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 889, in __call__\n",
      "Step #3 - \"Local Test E2E Pipeline\":     result = self._call(*args, **kwds)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 924, in _call\n",
      "Step #3 - \"Local Test E2E Pipeline\":     results = self._stateful_fn(*args, **kwds)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3022, in __call__\n",
      "Step #3 - \"Local Test E2E Pipeline\":     filtered_flat_args) = self._maybe_define_function(args, kwargs)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3444, in _maybe_define_function\n",
      "Step #3 - \"Local Test E2E Pipeline\":     graph_function = self._create_graph_function(args, kwargs)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3289, in _create_graph_function\n",
      "Step #3 - \"Local Test E2E Pipeline\":     capture_by_value=self._capture_by_value),\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 999, in func_graph_from_py_func\n",
      "Step #3 - \"Local Test E2E Pipeline\":     func_outputs = python_func(*func_args, **func_kwargs)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 672, in wrapped_fn\n",
      "Step #3 - \"Local Test E2E Pipeline\":     out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n",
      "Step #3 - \"Local Test E2E Pipeline\": AttributeError: 'NoneType' object has no attribute '__wrapped__'\n",
      "Step #3 - \"Local Test E2E Pipeline\": Exception ignored in: <function CapturableResource.__del__ at 0x7f5e366ad710>\n",
      "Step #3 - \"Local Test E2E Pipeline\": Traceback (most recent call last):\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py\", line 277, in __del__\n",
      "Step #3 - \"Local Test E2E Pipeline\":     self._destroy_resource()\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 889, in __call__\n",
      "Step #3 - \"Local Test E2E Pipeline\":     result = self._call(*args, **kwds)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 924, in _call\n",
      "Step #3 - \"Local Test E2E Pipeline\":     results = self._stateful_fn(*args, **kwds)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3022, in __call__\n",
      "Step #3 - \"Local Test E2E Pipeline\":     filtered_flat_args) = self._maybe_define_function(args, kwargs)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3444, in _maybe_define_function\n",
      "Step #3 - \"Local Test E2E Pipeline\":     graph_function = self._create_graph_function(args, kwargs)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3289, in _create_graph_function\n",
      "Step #3 - \"Local Test E2E Pipeline\":     capture_by_value=self._capture_by_value),\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 999, in func_graph_from_py_func\n",
      "Step #3 - \"Local Test E2E Pipeline\":     func_outputs = python_func(*func_args, **func_kwargs)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 672, in wrapped_fn\n",
      "Step #3 - \"Local Test E2E Pipeline\":     out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n",
      "Step #3 - \"Local Test E2E Pipeline\": AttributeError: 'NoneType' object has no attribute '__wrapped__'\n",
      "Step #3 - \"Local Test E2E Pipeline\": Exception ignored in: <function CapturableResource.__del__ at 0x7f5e366ad710>\n",
      "Step #3 - \"Local Test E2E Pipeline\": Traceback (most recent call last):\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py\", line 277, in __del__\n",
      "Step #3 - \"Local Test E2E Pipeline\":     self._destroy_resource()\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 889, in __call__\n",
      "Step #3 - \"Local Test E2E Pipeline\":     result = self._call(*args, **kwds)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 924, in _call\n",
      "Step #3 - \"Local Test E2E Pipeline\":     results = self._stateful_fn(*args, **kwds)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3022, in __call__\n",
      "Step #3 - \"Local Test E2E Pipeline\":     filtered_flat_args) = self._maybe_define_function(args, kwargs)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3444, in _maybe_define_function\n",
      "Step #3 - \"Local Test E2E Pipeline\":     graph_function = self._create_graph_function(args, kwargs)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3289, in _create_graph_function\n",
      "Step #3 - \"Local Test E2E Pipeline\":     capture_by_value=self._capture_by_value),\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 999, in func_graph_from_py_func\n",
      "Step #3 - \"Local Test E2E Pipeline\":     func_outputs = python_func(*func_args, **func_kwargs)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 672, in wrapped_fn\n",
      "Step #3 - \"Local Test E2E Pipeline\":     out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n",
      "Step #3 - \"Local Test E2E Pipeline\": AttributeError: 'NoneType' object has no attribute '__wrapped__'\n",
      "Step #3 - \"Local Test E2E Pipeline\": Exception ignored in: <function CapturableResource.__del__ at 0x7f5e366ad710>\n",
      "Step #3 - \"Local Test E2E Pipeline\": Traceback (most recent call last):\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py\", line 277, in __del__\n",
      "Step #3 - \"Local Test E2E Pipeline\":     self._destroy_resource()\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 889, in __call__\n",
      "Step #3 - \"Local Test E2E Pipeline\":     result = self._call(*args, **kwds)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 924, in _call\n",
      "Step #3 - \"Local Test E2E Pipeline\":     results = self._stateful_fn(*args, **kwds)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3022, in __call__\n",
      "Step #3 - \"Local Test E2E Pipeline\":     filtered_flat_args) = self._maybe_define_function(args, kwargs)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3444, in _maybe_define_function\n",
      "Step #3 - \"Local Test E2E Pipeline\":     graph_function = self._create_graph_function(args, kwargs)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3289, in _create_graph_function\n",
      "Step #3 - \"Local Test E2E Pipeline\":     capture_by_value=self._capture_by_value),\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 999, in func_graph_from_py_func\n",
      "Step #3 - \"Local Test E2E Pipeline\":     func_outputs = python_func(*func_args, **func_kwargs)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 672, in wrapped_fn\n",
      "Step #3 - \"Local Test E2E Pipeline\":     out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n",
      "Step #3 - \"Local Test E2E Pipeline\": AttributeError: 'NoneType' object has no attribute '__wrapped__'\n",
      "Step #3 - \"Local Test E2E Pipeline\": Exception ignored in: <function CapturableResource.__del__ at 0x7f5e366ad710>\n",
      "Step #3 - \"Local Test E2E Pipeline\": Traceback (most recent call last):\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py\", line 277, in __del__\n",
      "Step #3 - \"Local Test E2E Pipeline\":     self._destroy_resource()\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 889, in __call__\n",
      "Step #3 - \"Local Test E2E Pipeline\":     result = self._call(*args, **kwds)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 924, in _call\n",
      "Step #3 - \"Local Test E2E Pipeline\":     results = self._stateful_fn(*args, **kwds)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3022, in __call__\n",
      "Step #3 - \"Local Test E2E Pipeline\":     filtered_flat_args) = self._maybe_define_function(args, kwargs)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3444, in _maybe_define_function\n",
      "Step #3 - \"Local Test E2E Pipeline\":     graph_function = self._create_graph_function(args, kwargs)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3289, in _create_graph_function\n",
      "Step #3 - \"Local Test E2E Pipeline\":     capture_by_value=self._capture_by_value),\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 999, in func_graph_from_py_func\n",
      "Step #3 - \"Local Test E2E Pipeline\":     func_outputs = python_func(*func_args, **func_kwargs)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 672, in wrapped_fn\n",
      "Step #3 - \"Local Test E2E Pipeline\":     out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n",
      "Step #3 - \"Local Test E2E Pipeline\": AttributeError: 'NoneType' object has no attribute '__wrapped__'\n",
      "Step #3 - \"Local Test E2E Pipeline\": Exception ignored in: <function CapturableResource.__del__ at 0x7f5e366ad710>\n",
      "Step #3 - \"Local Test E2E Pipeline\": Traceback (most recent call last):\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py\", line 277, in __del__\n",
      "Step #3 - \"Local Test E2E Pipeline\":     self._destroy_resource()\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 889, in __call__\n",
      "Step #3 - \"Local Test E2E Pipeline\":     result = self._call(*args, **kwds)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 924, in _call\n",
      "Step #3 - \"Local Test E2E Pipeline\":     results = self._stateful_fn(*args, **kwds)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3022, in __call__\n",
      "Step #3 - \"Local Test E2E Pipeline\":     filtered_flat_args) = self._maybe_define_function(args, kwargs)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3444, in _maybe_define_function\n",
      "Step #3 - \"Local Test E2E Pipeline\":     graph_function = self._create_graph_function(args, kwargs)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3289, in _create_graph_function\n",
      "Step #3 - \"Local Test E2E Pipeline\":     capture_by_value=self._capture_by_value),\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 999, in func_graph_from_py_func\n",
      "Step #3 - \"Local Test E2E Pipeline\":     func_outputs = python_func(*func_args, **func_kwargs)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 672, in wrapped_fn\n",
      "Step #3 - \"Local Test E2E Pipeline\":     out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n",
      "Step #3 - \"Local Test E2E Pipeline\": AttributeError: 'NoneType' object has no attribute '__wrapped__'\n",
      "Step #3 - \"Local Test E2E Pipeline\": Exception ignored in: <function CapturableResource.__del__ at 0x7f5e366ad710>\n",
      "Step #3 - \"Local Test E2E Pipeline\": Traceback (most recent call last):\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py\", line 277, in __del__\n",
      "Step #3 - \"Local Test E2E Pipeline\":     self._destroy_resource()\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 889, in __call__\n",
      "Step #3 - \"Local Test E2E Pipeline\":     result = self._call(*args, **kwds)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 924, in _call\n",
      "Step #3 - \"Local Test E2E Pipeline\":     results = self._stateful_fn(*args, **kwds)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3022, in __call__\n",
      "Step #3 - \"Local Test E2E Pipeline\":     filtered_flat_args) = self._maybe_define_function(args, kwargs)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3444, in _maybe_define_function\n",
      "Step #3 - \"Local Test E2E Pipeline\":     graph_function = self._create_graph_function(args, kwargs)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3289, in _create_graph_function\n",
      "Step #3 - \"Local Test E2E Pipeline\":     capture_by_value=self._capture_by_value),\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 999, in func_graph_from_py_func\n",
      "Step #3 - \"Local Test E2E Pipeline\":     func_outputs = python_func(*func_args, **func_kwargs)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 672, in wrapped_fn\n",
      "Step #3 - \"Local Test E2E Pipeline\":     out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n",
      "Step #3 - \"Local Test E2E Pipeline\": AttributeError: 'NoneType' object has no attribute '__wrapped__'\n",
      "Step #3 - \"Local Test E2E Pipeline\": Exception ignored in: <function CapturableResource.__del__ at 0x7f5e366ad710>\n",
      "Step #3 - \"Local Test E2E Pipeline\": Traceback (most recent call last):\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py\", line 277, in __del__\n",
      "Step #3 - \"Local Test E2E Pipeline\":     self._destroy_resource()\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 889, in __call__\n",
      "Step #3 - \"Local Test E2E Pipeline\":     result = self._call(*args, **kwds)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 924, in _call\n",
      "Step #3 - \"Local Test E2E Pipeline\":     results = self._stateful_fn(*args, **kwds)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3022, in __call__\n",
      "Step #3 - \"Local Test E2E Pipeline\":     filtered_flat_args) = self._maybe_define_function(args, kwargs)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3444, in _maybe_define_function\n",
      "Step #3 - \"Local Test E2E Pipeline\":     graph_function = self._create_graph_function(args, kwargs)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3289, in _create_graph_function\n",
      "Step #3 - \"Local Test E2E Pipeline\":     capture_by_value=self._capture_by_value),\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 999, in func_graph_from_py_func\n",
      "Step #3 - \"Local Test E2E Pipeline\":     func_outputs = python_func(*func_args, **func_kwargs)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 672, in wrapped_fn\n",
      "Step #3 - \"Local Test E2E Pipeline\":     out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n",
      "Step #3 - \"Local Test E2E Pipeline\": AttributeError: 'NoneType' object has no attribute '__wrapped__'\n",
      "Step #3 - \"Local Test E2E Pipeline\": Exception ignored in: <function CapturableResource.__del__ at 0x7f5e366ad710>\n",
      "Step #3 - \"Local Test E2E Pipeline\": Traceback (most recent call last):\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py\", line 277, in __del__\n",
      "Step #3 - \"Local Test E2E Pipeline\":     self._destroy_resource()\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 889, in __call__\n",
      "Step #3 - \"Local Test E2E Pipeline\":     result = self._call(*args, **kwds)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 924, in _call\n",
      "Step #3 - \"Local Test E2E Pipeline\":     results = self._stateful_fn(*args, **kwds)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3022, in __call__\n",
      "Step #3 - \"Local Test E2E Pipeline\":     filtered_flat_args) = self._maybe_define_function(args, kwargs)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3444, in _maybe_define_function\n",
      "Step #3 - \"Local Test E2E Pipeline\":     graph_function = self._create_graph_function(args, kwargs)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3289, in _create_graph_function\n",
      "Step #3 - \"Local Test E2E Pipeline\":     capture_by_value=self._capture_by_value),\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 999, in func_graph_from_py_func\n",
      "Step #3 - \"Local Test E2E Pipeline\":     func_outputs = python_func(*func_args, **func_kwargs)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 672, in wrapped_fn\n",
      "Step #3 - \"Local Test E2E Pipeline\":     out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n",
      "Step #3 - \"Local Test E2E Pipeline\": AttributeError: 'NoneType' object has no attribute '__wrapped__'\n",
      "Step #3 - \"Local Test E2E Pipeline\": Exception ignored in: <function CapturableResource.__del__ at 0x7f5e366ad710>\n",
      "Step #3 - \"Local Test E2E Pipeline\": Traceback (most recent call last):\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py\", line 277, in __del__\n",
      "Step #3 - \"Local Test E2E Pipeline\":     self._destroy_resource()\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 889, in __call__\n",
      "Step #3 - \"Local Test E2E Pipeline\":     result = self._call(*args, **kwds)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 924, in _call\n",
      "Step #3 - \"Local Test E2E Pipeline\":     results = self._stateful_fn(*args, **kwds)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3022, in __call__\n",
      "Step #3 - \"Local Test E2E Pipeline\":     filtered_flat_args) = self._maybe_define_function(args, kwargs)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3444, in _maybe_define_function\n",
      "Step #3 - \"Local Test E2E Pipeline\":     graph_function = self._create_graph_function(args, kwargs)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3289, in _create_graph_function\n",
      "Step #3 - \"Local Test E2E Pipeline\":     capture_by_value=self._capture_by_value),\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 999, in func_graph_from_py_func\n",
      "Step #3 - \"Local Test E2E Pipeline\":     func_outputs = python_func(*func_args, **func_kwargs)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 672, in wrapped_fn\n",
      "Step #3 - \"Local Test E2E Pipeline\":     out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n",
      "Step #3 - \"Local Test E2E Pipeline\": AttributeError: 'NoneType' object has no attribute '__wrapped__'\n",
      "Step #3 - \"Local Test E2E Pipeline\": Exception ignored in: <function CapturableResource.__del__ at 0x7f5e366ad710>\n",
      "Step #3 - \"Local Test E2E Pipeline\": Traceback (most recent call last):\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py\", line 277, in __del__\n",
      "Step #3 - \"Local Test E2E Pipeline\":     self._destroy_resource()\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 889, in __call__\n",
      "Step #3 - \"Local Test E2E Pipeline\":     result = self._call(*args, **kwds)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 924, in _call\n",
      "Step #3 - \"Local Test E2E Pipeline\":     results = self._stateful_fn(*args, **kwds)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3022, in __call__\n",
      "Step #3 - \"Local Test E2E Pipeline\":     filtered_flat_args) = self._maybe_define_function(args, kwargs)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3444, in _maybe_define_function\n",
      "Step #3 - \"Local Test E2E Pipeline\":     graph_function = self._create_graph_function(args, kwargs)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3289, in _create_graph_function\n",
      "Step #3 - \"Local Test E2E Pipeline\":     capture_by_value=self._capture_by_value),\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 999, in func_graph_from_py_func\n",
      "Step #3 - \"Local Test E2E Pipeline\":     func_outputs = python_func(*func_args, **func_kwargs)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 672, in wrapped_fn\n",
      "Step #3 - \"Local Test E2E Pipeline\":     out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n",
      "Step #3 - \"Local Test E2E Pipeline\": AttributeError: 'NoneType' object has no attribute '__wrapped__'\n",
      "Step #3 - \"Local Test E2E Pipeline\": Exception ignored in: <function CapturableResource.__del__ at 0x7f5e366ad710>\n",
      "Step #3 - \"Local Test E2E Pipeline\": Traceback (most recent call last):\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py\", line 277, in __del__\n",
      "Step #3 - \"Local Test E2E Pipeline\":     self._destroy_resource()\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 889, in __call__\n",
      "Step #3 - \"Local Test E2E Pipeline\":     result = self._call(*args, **kwds)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 924, in _call\n",
      "Step #3 - \"Local Test E2E Pipeline\":     results = self._stateful_fn(*args, **kwds)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3022, in __call__\n",
      "Step #3 - \"Local Test E2E Pipeline\":     filtered_flat_args) = self._maybe_define_function(args, kwargs)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3444, in _maybe_define_function\n",
      "Step #3 - \"Local Test E2E Pipeline\":     graph_function = self._create_graph_function(args, kwargs)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3289, in _create_graph_function\n",
      "Step #3 - \"Local Test E2E Pipeline\":     capture_by_value=self._capture_by_value),\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 999, in func_graph_from_py_func\n",
      "Step #3 - \"Local Test E2E Pipeline\":     func_outputs = python_func(*func_args, **func_kwargs)\n",
      "Step #3 - \"Local Test E2E Pipeline\":   File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 672, in wrapped_fn\n",
      "Step #3 - \"Local Test E2E Pipeline\":     out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n",
      "Step #3 - \"Local Test E2E Pipeline\": AttributeError: 'NoneType' object has no attribute '__wrapped__'\n",
      "Step #3 - \"Local Test E2E Pipeline\": Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": Two checkpoint references resolved to different objects (<tensorflow.python.keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7f5dafa6ae90> and <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f5daf9e44d0>).\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/FanoutSlices/TrackDistinctSliceKeys/RemoveDuplicates/Group/Group/Read)+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/FanoutSlices/TrackDistinctSliceKeys/RemoveDuplicates/Group/Merge))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/FanoutSlices/TrackDistinctSliceKeys/RemoveDuplicates/Group/ExtractOutputs))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_55))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_58))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/FanoutSlices/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/CombinePerKey/Precombine))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/FanoutSlices/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/FanoutSlices/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Read)+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/FanoutSlices/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/CombinePerKey/Merge))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/FanoutSlices/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_63))+(ref_PCollection_PCollection_28/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_65/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteEvalConfig-WriteEvalConfig-Wr_133))+(ref_PCollection_PCollection_71/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CombineMetricsPerSlice/CombinePerKey(PreCombineFn)/Group/Read)+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CombineMetricsPerSlice/CombinePerKey(PreCombineFn)/Merge))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CombineMetricsPerSlice/CombinePerKey(PreCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_98))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_99))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CombineMetricsPerSlice/Flatten/Transcode/1))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CombineMetricsPerSlice/Flatten/Write/1)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": Two checkpoint references resolved to different objects (<tensorflow.python.keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7f5e12038e10> and <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f5daf7d4810>).\n",
      "Step #3 - \"Local Test E2E Pipeline\": Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": Two checkpoint references resolved to different objects (<tensorflow.python.keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7f5d7ed93d90> and <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f5d7ede6e50>).\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CombineMetricsPerSlice/Flatten/Read)+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CombineMetricsPerSlice/CombinePerKey(PostCombineFn)/Precombine))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CombineMetricsPerSlice/CombinePerKey(PostCombineFn)/Group/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": Two checkpoint references resolved to different objects (<tensorflow.python.keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7f5d7cf0b5d0> and <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f5d7cfa6c10>).\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((((((((((((((((((((ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CombineMetricsPerSlice/CombinePerKey(PostCombineFn)/Group/Read)+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CombineMetricsPerSlice/CombinePerKey(PostCombineFn)/Merge))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CombineMetricsPerSlice/CombinePerKey(PostCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_106))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_108))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_109))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_110))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_111))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Vali_112))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-ConvertSliceM_136))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-ConvertSliceP_152))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-ConvertSliceA_168))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-MergeValidati_185))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteMetrics-_146))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteMetrics-_147))+(ExtractEvaluateAndWriteResults/WriteResults/WriteMetricsAndPlots/WriteMetrics/Write/WriteImpl/GroupByKey/Write))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WritePlotsToT_162))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WritePlotsToT_163))+(ExtractEvaluateAndWriteResults/WriteResults/WriteMetricsAndPlots/WritePlotsToTFRecord/Write/WriteImpl/GroupByKey/Write))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteAttribut_178))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteAttribut_179))+(ExtractEvaluateAndWriteResults/WriteResults/WriteMetricsAndPlots/WriteAttributionsToTFRecord/Write/WriteImpl/GroupByKey/Write))+(ExtractEvaluateAndWriteResults/WriteResults/WriteMetricsAndPlots/MergeValidationResults/CombinePerKey/Precombine))+(ExtractEvaluateAndWriteResults/WriteResults/WriteMetricsAndPlots/MergeValidationResults/CombinePerKey/Group/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": Two checkpoint references resolved to different objects (<tensorflow.python.keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7f5d7c528f90> and <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f5d7c5b8190>).\n",
      "Step #3 - \"Local Test E2E Pipeline\": Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": Two checkpoint references resolved to different objects (<tensorflow.python.keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7f5d73b8dd50> and <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f5dafa4e890>).\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((ExtractEvaluateAndWriteResults/WriteResults/WriteMetricsAndPlots/MergeValidationResults/CombinePerKey/Group/Read)+(ExtractEvaluateAndWriteResults/WriteResults/WriteMetricsAndPlots/MergeValidationResults/CombinePerKey/Merge))+(ExtractEvaluateAndWriteResults/WriteResults/WriteMetricsAndPlots/MergeValidationResults/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-MergeValidati_190))+(ref_PCollection_PCollection_109/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_65)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_66))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_68))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_69))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_70)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteMetrics-_141)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteMetrics-_142))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteMetrics-_144))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteMetrics-_145))+(ref_PCollection_PCollection_76/Write))+(ref_PCollection_PCollection_77/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ExtractEvaluateAndWriteResults/WriteResults/WriteMetricsAndPlots/WriteMetrics/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteMetrics-_149))+(ref_PCollection_PCollection_81/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_76/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteMetrics-_150))+(ref_PCollection_PCollection_82/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_76/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteMetrics-_151)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.0733027458190918 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.40 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_81)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_82))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_84))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_85)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CountPerSliceKey/CombinePerKey(CountCombineFn)/Group/Read)+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CountPerSliceKey/CombinePerKey(CountCombineFn)/Merge))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CountPerSliceKey/CombinePerKey(CountCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_88))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_89)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-MergeValidati_192)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-MergeValidati_193))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-MergeValidati_195))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-MergeValidati_196))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteValidati_206))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteValidati_207))+(ExtractEvaluateAndWriteResults/WriteResults/WriteMetricsAndPlots/WriteValidationsToTFRecord/Write/WriteImpl/GroupByKey/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteValidati_201)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteValidati_202))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteValidati_204))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteValidati_205))+(ref_PCollection_PCollection_116/Write))+(ref_PCollection_PCollection_117/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ExtractEvaluateAndWriteResults/WriteResults/WriteMetricsAndPlots/WriteValidationsToTFRecord/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteValidati_209))+(ref_PCollection_PCollection_121/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_116/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteValidati_210))+(ref_PCollection_PCollection_122/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_116/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteValidati_211)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.08609127998352051 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.50 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WritePlotsToT_157)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WritePlotsToT_158))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WritePlotsToT_160))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WritePlotsToT_161))+(ref_PCollection_PCollection_87/Write))+(ref_PCollection_PCollection_88/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ExtractEvaluateAndWriteResults/WriteResults/WriteMetricsAndPlots/WritePlotsToTFRecord/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WritePlotsToT_165))+(ref_PCollection_PCollection_92/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_87/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WritePlotsToT_166))+(ref_PCollection_PCollection_93/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_87/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WritePlotsToT_167)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.07605791091918945 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.40 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteAttribut_173)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteAttribut_174))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteAttribut_176))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteAttribut_177))+(ref_PCollection_PCollection_98/Write))+(ref_PCollection_PCollection_99/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ExtractEvaluateAndWriteResults/WriteResults/WriteMetricsAndPlots/WriteAttributionsToTFRecord/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteAttribut_181))+(ref_PCollection_PCollection_103/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_98/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteAttribut_182))+(ref_PCollection_PCollection_104/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_98/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteAttribut_183)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.07418656349182129 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.50 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_65/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteEvalConfig-WriteEvalConfig-Wr_134)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.07708859443664551 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.40 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": From /opt/conda/lib/python3.7/site-packages/tensorflow_model_analysis/writers/metrics_plots_and_validations_writer.py:113: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Instructions for updating:\n",
      "Step #3 - \"Local Test E2E Pipeline\": Use eager execution and: \n",
      "Step #3 - \"Local Test E2E Pipeline\": `tf.data.TFRecordDataset(path)`\n",
      "Step #3 - \"Local Test E2E Pipeline\": stateful_working_dir /workspace/mlops-with-vertex-ai/gs:/grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/ModelEvaluator/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Step #3 - \"Local Test E2E Pipeline\": I0330 17:27:38.104410     1 rdbms_metadata_access_object.cc:686] No property is defined for the Type\n",
      "Step #3 - \"Local Test E2E Pipeline\": I0330 17:27:38.110183     1 rdbms_metadata_access_object.cc:686] No property is defined for the Type\n",
      "Step #3 - \"Local Test E2E Pipeline\": I0330 17:27:38.140285     1 rdbms_metadata_access_object.cc:686] No property is defined for the Type\n",
      "Step #3 - \"Local Test E2E Pipeline\": stateful_working_dir /workspace/mlops-with-vertex-ai/gs:/grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline/ModelPusher/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Step #3 - \"Local Test E2E Pipeline\": I0330 17:28:10.991616     1 rdbms_metadata_access_object.cc:686] No property is defined for the Type\n",
      "Step #3 - \"Local Test E2E Pipeline\": Model output: gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/e2e_tests/model_registry/chicago-taxi-tips-classifier-v01\n",
      "Step #3 - \"Local Test E2E Pipeline\": .\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": =============================== warnings summary ===============================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ../../opt/conda/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py:22\n",
      "Step #3 - \"Local Test E2E Pipeline\":   /opt/conda/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py:22: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "Step #3 - \"Local Test E2E Pipeline\":     import imp\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": ../../opt/conda/lib/python3.7/site-packages/apache_beam/typehints/typehints.py:678\n",
      "Step #3 - \"Local Test E2E Pipeline\":   /opt/conda/lib/python3.7/site-packages/apache_beam/typehints/typehints.py:678: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "Step #3 - \"Local Test E2E Pipeline\":     if not isinstance(type_params, collections.Iterable):\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": ../../opt/conda/lib/python3.7/site-packages/apache_beam/typehints/typehints.py:517\n",
      "Step #3 - \"Local Test E2E Pipeline\":   /opt/conda/lib/python3.7/site-packages/apache_beam/typehints/typehints.py:517: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "Step #3 - \"Local Test E2E Pipeline\":     if not isinstance(type_params, (collections.Sequence, set)):\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": ../../opt/conda/lib/python3.7/site-packages/hdfs/client.py:208\n",
      "Step #3 - \"Local Test E2E Pipeline\":   /opt/conda/lib/python3.7/site-packages/hdfs/client.py:208: DeprecationWarning: invalid escape sequence \\*\n",
      "Step #3 - \"Local Test E2E Pipeline\":     \"\"\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": ../../opt/conda/lib/python3.7/site-packages/hdfs/client.py:510\n",
      "Step #3 - \"Local Test E2E Pipeline\":   /opt/conda/lib/python3.7/site-packages/hdfs/client.py:510: DeprecationWarning: invalid escape sequence \\*\n",
      "Step #3 - \"Local Test E2E Pipeline\":     \"\"\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": ../../opt/conda/lib/python3.7/site-packages/hdfs/client.py:744\n",
      "Step #3 - \"Local Test E2E Pipeline\":   /opt/conda/lib/python3.7/site-packages/hdfs/client.py:744: DeprecationWarning: invalid escape sequence \\*\n",
      "Step #3 - \"Local Test E2E Pipeline\":     \"\"\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": ../../opt/conda/lib/python3.7/site-packages/hdfs/client.py:1199\n",
      "Step #3 - \"Local Test E2E Pipeline\":   /opt/conda/lib/python3.7/site-packages/hdfs/client.py:1199: DeprecationWarning: invalid escape sequence \\*\n",
      "Step #3 - \"Local Test E2E Pipeline\":     \"\"\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": ../../opt/conda/lib/python3.7/site-packages/hdfs/client.py:1222\n",
      "Step #3 - \"Local Test E2E Pipeline\":   /opt/conda/lib/python3.7/site-packages/hdfs/client.py:1222: DeprecationWarning: invalid escape sequence \\*\n",
      "Step #3 - \"Local Test E2E Pipeline\":     \"\"\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": ../../opt/conda/lib/python3.7/site-packages/hdfs/config.py:188\n",
      "Step #3 - \"Local Test E2E Pipeline\":   /opt/conda/lib/python3.7/site-packages/hdfs/config.py:188: DeprecationWarning: invalid escape sequence \\*\n",
      "Step #3 - \"Local Test E2E Pipeline\":     \"\"\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": ../../opt/conda/lib/python3.7/site-packages/google/cloud/pubsub_v1/proto/pubsub_pb2.py:3680\n",
      "Step #3 - \"Local Test E2E Pipeline\":   /opt/conda/lib/python3.7/site-packages/google/cloud/pubsub_v1/proto/pubsub_pb2.py:3680: DeprecationWarning: invalid escape sequence \\ \n",
      "Step #3 - \"Local Test E2E Pipeline\":     \"\"\",\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": ../../opt/conda/lib/python3.7/site-packages/google/cloud/pubsub_v1/proto/pubsub_pb2.py:4083\n",
      "Step #3 - \"Local Test E2E Pipeline\":   /opt/conda/lib/python3.7/site-packages/google/cloud/pubsub_v1/proto/pubsub_pb2.py:4083: DeprecationWarning: invalid escape sequence \\ \n",
      "Step #3 - \"Local Test E2E Pipeline\":     \"\"\",\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": ../../opt/conda/lib/python3.7/site-packages/google/cloud/pubsub_v1/proto/pubsub_pb2.py:4216\n",
      "Step #3 - \"Local Test E2E Pipeline\":   /opt/conda/lib/python3.7/site-packages/google/cloud/pubsub_v1/proto/pubsub_pb2.py:4216: DeprecationWarning: invalid escape sequence \\ \n",
      "Step #3 - \"Local Test E2E Pipeline\":     \"\"\",\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": ../../opt/conda/lib/python3.7/site-packages/google/cloud/pubsub_v1/gapic/subscriber_client.py:398\n",
      "Step #3 - \"Local Test E2E Pipeline\":   /opt/conda/lib/python3.7/site-packages/google/cloud/pubsub_v1/gapic/subscriber_client.py:398: DeprecationWarning: invalid escape sequence \\ \n",
      "Step #3 - \"Local Test E2E Pipeline\":     \"\"\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "Step #3 - \"Local Test E2E Pipeline\":   /opt/conda/lib/python3.7/site-packages/tfx/utils/deprecation_utils.py:188: TfxDeprecationWarning: From /opt/conda/lib/python3.7/site-packages/tfx/orchestration/portable/resolver_node_handler.py:66: resolve_input_artifacts (from tfx.orchestration.portable.inputs_utils) is deprecated and will be removed after 2021-06-01. Instructions for updating:\n",
      "Step #3 - \"Local Test E2E Pipeline\":   Use resolve_input_artifacts_v2() instead.\n",
      "Step #3 - \"Local Test E2E Pipeline\":     warnings.warn(msg, TfxDeprecationWarning)\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "Step #3 - \"Local Test E2E Pipeline\": src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "Step #3 - \"Local Test E2E Pipeline\":   /opt/conda/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery.py:1927: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "Step #3 - \"Local Test E2E Pipeline\":     temp_location = pcoll.pipeline.options.view_as(\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "Step #3 - \"Local Test E2E Pipeline\": src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "Step #3 - \"Local Test E2E Pipeline\":   /opt/conda/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery.py:1929: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "Step #3 - \"Local Test E2E Pipeline\":     job_name = pcoll.pipeline.options.view_as(GoogleCloudOptions).job_name\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "Step #3 - \"Local Test E2E Pipeline\": src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "Step #3 - \"Local Test E2E Pipeline\":   /opt/conda/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery.py:1959: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "Step #3 - \"Local Test E2E Pipeline\":     | _PassThroughThenCleanup(files_to_remove_pcoll))\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": -- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "Step #3 - \"Local Test E2E Pipeline\": ================== 1 passed, 20 warnings in 639.09s (0:10:39) ==================\n",
      "Step #3 - \"Local Test E2E Pipeline\": Unresolved object in checkpoint: (root).layer-0._init_input_shape\n",
      "Step #3 - \"Local Test E2E Pipeline\": A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Unresolved object in checkpoint: (root).layer-0._init_input_shape\n",
      "Step #3 - \"Local Test E2E Pipeline\": A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Unresolved object in checkpoint: (root).layer-0._init_input_shape\n",
      "Step #3 - \"Local Test E2E Pipeline\": A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Unresolved object in checkpoint: (root).layer-0._init_input_shape\n",
      "Step #3 - \"Local Test E2E Pipeline\": A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Unresolved object in checkpoint: (root).layer-0._init_input_shape\n",
      "Step #3 - \"Local Test E2E Pipeline\": A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Unresolved object in checkpoint: (root).layer-0._init_input_shape\n",
      "Step #3 - \"Local Test E2E Pipeline\": A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Unresolved object in checkpoint: (root).layer-0._init_input_shape\n",
      "Step #3 - \"Local Test E2E Pipeline\": A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "Finished Step #3 - \"Local Test E2E Pipeline\"\n",
      "Starting Step #5 - \"Compile Pipeline\"\n",
      "Starting Step #4 - \"Build TFX Image\"\n",
      "Step #5 - \"Compile Pipeline\": Already have image (with digest): gcr.io/grandelli-demo-295810/cicd:latest\n",
      "Step #4 - \"Build TFX Image\": Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Step #4 - \"Build TFX Image\": Sending build context to Docker daemon  915.5kB\n",
      "Step #4 - \"Build TFX Image\": Step 1/5 : FROM gcr.io/tfx-oss-public/tfx:1.2.0\n",
      "Step #4 - \"Build TFX Image\": 1.2.0: Pulling from tfx-oss-public/tfx\n",
      "Step #4 - \"Build TFX Image\": 25fa05cd42bd: Already exists\n",
      "Step #4 - \"Build TFX Image\": 2d6e353a95ec: Already exists\n",
      "Step #4 - \"Build TFX Image\": 14d7996407de: Already exists\n",
      "Step #4 - \"Build TFX Image\": 0c9c6fc70f16: Already exists\n",
      "Step #4 - \"Build TFX Image\": c3c76be11512: Already exists\n",
      "Step #4 - \"Build TFX Image\": ab6e5a9c78ee: Already exists\n",
      "Step #4 - \"Build TFX Image\": 7bc1690abd59: Already exists\n",
      "Step #4 - \"Build TFX Image\": f5b4dd7682bc: Already exists\n",
      "Step #4 - \"Build TFX Image\": d6897660f71d: Already exists\n",
      "Step #4 - \"Build TFX Image\": 174d792fb622: Already exists\n",
      "Step #4 - \"Build TFX Image\": 5f8143275aca: Already exists\n",
      "Step #4 - \"Build TFX Image\": 56646f115483: Already exists\n",
      "Step #4 - \"Build TFX Image\": 798922b52524: Already exists\n",
      "Step #4 - \"Build TFX Image\": e2699a9f592b: Already exists\n",
      "Step #4 - \"Build TFX Image\": f43e7d1c07e4: Already exists\n",
      "Step #4 - \"Build TFX Image\": 1e71d5e9923d: Already exists\n",
      "Step #4 - \"Build TFX Image\": bf6ae2a2e250: Already exists\n",
      "Step #4 - \"Build TFX Image\": e49679b748d5: Already exists\n",
      "Step #4 - \"Build TFX Image\": 80208bd6f7fb: Already exists\n",
      "Step #4 - \"Build TFX Image\": b83c16bef138: Already exists\n",
      "Step #4 - \"Build TFX Image\": 9d1427033824: Already exists\n",
      "Step #4 - \"Build TFX Image\": c0028679f003: Already exists\n",
      "Step #4 - \"Build TFX Image\": 09c222e7ff04: Already exists\n",
      "Step #4 - \"Build TFX Image\": ae6048a3aec1: Already exists\n",
      "Step #4 - \"Build TFX Image\": 1ced637de50b: Already exists\n",
      "Step #4 - \"Build TFX Image\": 762ff1eb7f16: Already exists\n",
      "Step #4 - \"Build TFX Image\": f6f8f4265c8c: Already exists\n",
      "Step #4 - \"Build TFX Image\": 595b1c49222a: Already exists\n",
      "Step #4 - \"Build TFX Image\": b6c7eb38f366: Already exists\n",
      "Step #4 - \"Build TFX Image\": 520be5017b4d: Already exists\n",
      "Step #4 - \"Build TFX Image\": 0a1aacb7e387: Already exists\n",
      "Step #4 - \"Build TFX Image\": 8f605134caf9: Already exists\n",
      "Step #4 - \"Build TFX Image\": 189ba322d030: Already exists\n",
      "Step #4 - \"Build TFX Image\": af92447b9198: Already exists\n",
      "Step #4 - \"Build TFX Image\": Digest: sha256:eba9e7b7d9131eb5b05434feaafc4676268ad805e4b97218f58994ad2714be67\n",
      "Step #4 - \"Build TFX Image\": Status: Downloaded newer image for gcr.io/tfx-oss-public/tfx:1.2.0\n",
      "Step #4 - \"Build TFX Image\":  ---> 0e86fadcc60c\n",
      "Step #4 - \"Build TFX Image\": Step 2/5 : COPY requirements.txt requirements.txt\n",
      "Step #4 - \"Build TFX Image\":  ---> f3f80dbc44f6\n",
      "Step #4 - \"Build TFX Image\": Step 3/5 : RUN pip install -r requirements.txt\n",
      "Step #4 - \"Build TFX Image\":  ---> Running in bbf93f770940\n",
      "Step #4 - \"Build TFX Image\": Collecting kfp==1.8.1\n",
      "Step #4 - \"Build TFX Image\":   Downloading kfp-1.8.1.tar.gz (248 kB)\n",
      "Step #5 - \"Compile Pipeline\": 2022-03-30 17:28:19.522914: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "Step #4 - \"Build TFX Image\": Collecting google-cloud-bigquery==2.26.0\n",
      "Step #4 - \"Build TFX Image\":   Downloading google_cloud_bigquery-2.26.0-py2.py3-none-any.whl (201 kB)\n",
      "Step #4 - \"Build TFX Image\": Collecting google-cloud-bigquery-storage==2.7.0\n",
      "Step #4 - \"Build TFX Image\":   Downloading google_cloud_bigquery_storage-2.7.0-py2.py3-none-any.whl (125 kB)\n",
      "Step #4 - \"Build TFX Image\": Collecting google-cloud-aiplatform==1.4.2\n",
      "Step #4 - \"Build TFX Image\":   Downloading google_cloud_aiplatform-1.4.2-py2.py3-none-any.whl (1.4 MB)\n",
      "Step #4 - \"Build TFX Image\": Collecting cloudml-hypertune==0.1.0.dev6\n",
      "Step #4 - \"Build TFX Image\":   Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "Step #4 - \"Build TFX Image\": Collecting pytest\n",
      "Step #4 - \"Build TFX Image\":   Downloading pytest-7.1.1-py3-none-any.whl (297 kB)\n",
      "Step #4 - \"Build TFX Image\": Collecting absl-py<=0.11,>=0.9\n",
      "Step #4 - \"Build TFX Image\":   Downloading absl_py-0.11.0-py3-none-any.whl (127 kB)\n",
      "Step #4 - \"Build TFX Image\": Requirement already satisfied: PyYAML<6,>=5.3 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.1->-r requirements.txt (line 1)) (5.4.1)\n",
      "Step #4 - \"Build TFX Image\": Requirement already satisfied: google-cloud-storage<2,>=1.20.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.1->-r requirements.txt (line 1)) (1.41.1)\n",
      "Step #4 - \"Build TFX Image\": Requirement already satisfied: kubernetes<19,>=8.0.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.1->-r requirements.txt (line 1)) (12.0.1)\n",
      "Step #4 - \"Build TFX Image\": Requirement already satisfied: google-api-python-client<2,>=1.7.8 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.1->-r requirements.txt (line 1)) (1.12.8)\n",
      "Step #4 - \"Build TFX Image\": Requirement already satisfied: google-auth<2,>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.1->-r requirements.txt (line 1)) (1.34.0)\n",
      "Step #4 - \"Build TFX Image\": Collecting requests-toolbelt<1,>=0.8.0\n",
      "Step #4 - \"Build TFX Image\":   Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "Step #4 - \"Build TFX Image\": Requirement already satisfied: cloudpickle<2,>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.1->-r requirements.txt (line 1)) (1.6.0)\n",
      "Step #4 - \"Build TFX Image\": Collecting kfp-server-api<2.0.0,>=1.1.2\n",
      "Step #4 - \"Build TFX Image\":   Downloading kfp-server-api-1.8.1.tar.gz (54 kB)\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.Sequence[~T]\n",
      "Step #4 - \"Build TFX Image\": Requirement already satisfied: jsonschema<4,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.1->-r requirements.txt (line 1)) (3.2.0)\n",
      "Step #4 - \"Build TFX Image\": Requirement already satisfied: tabulate<1,>=0.8.6 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.1->-r requirements.txt (line 1)) (0.8.9)\n",
      "Step #4 - \"Build TFX Image\": Requirement already satisfied: click<8,>=7.1.1 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.1->-r requirements.txt (line 1)) (7.1.2)\n",
      "Step #4 - \"Build TFX Image\": Collecting Deprecated<2,>=1.2.7\n",
      "Step #4 - \"Build TFX Image\":   Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Step #4 - \"Build TFX Image\": Collecting strip-hints<1,>=0.1.8\n",
      "Step #4 - \"Build TFX Image\":   Downloading strip-hints-0.1.10.tar.gz (29 kB)\n",
      "Step #4 - \"Build TFX Image\": Collecting docstring-parser<1,>=0.7.3\n",
      "Step #4 - \"Build TFX Image\":   Downloading docstring_parser-0.13.tar.gz (23 kB)\n",
      "Step #4 - \"Build TFX Image\":   Installing build dependencies: started\n",
      "Step #5 - \"Compile Pipeline\": INFO:absl:tensorflow_ranking is not available: No module named 'tensorflow_ranking'\n",
      "Step #5 - \"Compile Pipeline\": INFO:absl:tensorflow_text is not available: No module named 'tensorflow_text'\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #5 - \"Compile Pipeline\": INFO:absl:tensorflow_text is not available: No module named 'tensorflow_text'\n",
      "Step #5 - \"Compile Pipeline\": INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "Step #5 - \"Compile Pipeline\": INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "Step #5 - \"Compile Pipeline\": INFO:root:Pipeline components: ['HyperparamsGen', 'TrainDataGen', 'TestDataGen', 'StatisticsGen', 'SchemaImporter', 'ExampleValidator', 'DataTransformer', 'WarmstartModelResolver', 'ModelTrainer', 'BaselineModelResolver', 'ModelEvaluator', 'ModelPusher', 'VertexUploader']\n",
      "Step #5 - \"Compile Pipeline\": INFO:root:Beam pipeline args: ['--project=grandelli-demo-295810', '--temp_location=gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/temp', '--region=us-central1', '--runner=DataflowRunner']\n",
      "Step #5 - \"Compile Pipeline\": INFO:root:{'displayName': 'chicago-taxi-tips-classifier-v01-train-pipeline', 'pipelineSpec': {'schemaVersion': '2.0.0', 'pipelineInfo': {'name': 'chicago-taxi-tips-classifier-v01-train-pipeline'}, 'deploymentSpec': {'executors': {'ExampleValidator_executor': {'container': {'command': ['python', '-m', 'tfx.orchestration.kubeflow.v2.container.kubeflow_v2_run_executor'], 'image': 'gcr.io/grandelli-demo-295810/chicago-taxi-tips:tfx-1.2', 'args': ['--executor_class_path', 'tfx.components.example_validator.executor.Executor', '--json_serialized_invocation_args', '{{$}}', '--project=grandelli-demo-295810', '--temp_location=gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/temp', '--region=us-central1', '--runner=DataflowRunner']}}, 'WarmstartModelResolver_executor': {'resolver': {'outputArtifactQueries': {'latest_model': {'filter': 'schema_title=\"tfx.Model\" AND state=LIVE'}}}}, 'SchemaImporter_executor': {'importer': {'artifactUri': {'constantValue': {'stringValue': 'src/raw_schema'}}, 'typeSchema': {'instanceSchema': 'title: tfx.Schema\\ntype: object\\n'}}}, 'ModelTrainer_executor': {'container': {'args': ['--executor_class_path', 'tfx.extensions.google_cloud_ai_platform.trainer.executor.GenericExecutor', '--json_serialized_invocation_args', '{{$}}', '--project=grandelli-demo-295810', '--temp_location=gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/temp', '--region=us-central1', '--runner=DataflowRunner'], 'command': ['python', '-m', 'tfx.orchestration.kubeflow.v2.container.kubeflow_v2_run_executor'], 'image': 'gcr.io/grandelli-demo-295810/chicago-taxi-tips:tfx-1.2'}}, 'StatisticsGen_executor': {'container': {'args': ['--executor_class_path', 'tfx.components.statistics_gen.executor.Executor', '--json_serialized_invocation_args', '{{$}}', '--project=grandelli-demo-295810', '--temp_location=gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/temp', '--region=us-central1', '--runner=DataflowRunner'], 'image': 'gcr.io/grandelli-demo-295810/chicago-taxi-tips:tfx-1.2', 'command': ['python', '-m', 'tfx.orchestration.kubeflow.v2.container.kubeflow_v2_run_executor']}}, 'DataTransformer_executor': {'container': {'args': ['--executor_class_path', 'tfx.components.transform.executor.Executor', '--json_serialized_invocation_args', '{{$}}', '--project=grandelli-demo-295810', '--temp_location=gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/temp', '--region=us-central1', '--runner=DataflowRunner'], 'image': 'gcr.io/grandelli-demo-295810/chicago-taxi-tips:tfx-1.2', 'command': ['python', '-m', 'tfx.orchestration.kubeflow.v2.container.kubeflow_v2_run_executor']}}, 'BaselineModelResolver-model-resolver_executor': {'resolver': {'outputArtifactQueries': {'model': {'filter': 'schema_title=\"tfx.Model\" AND state=LIVE AND name=\"{{$.inputs.artifacts[\\'input\\'].metadata[\\'current_model_id\\']}}\"'}}}}, 'ModelPusher_executor': {'container': {'image': 'gcr.io/grandelli-demo-295810/chicago-taxi-tips:tfx-1.2', 'command': ['python', '-m', 'tfx.orchestration.kubeflow.v2.container.kubeflow_v2_run_executor'], 'args': ['--executor_class_path', 'tfx.components.pusher.executor.Executor', '--json_serialized_invocation_args', '{{$}}', '--project=grandelli-demo-295810', '--temp_location=gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/temp', '--region=us-central1', '--runner=DataflowRunner']}}, 'ModelEvaluator_executor': {'container': {'image': 'gcr.io/grandelli-demo-295810/chicago-taxi-tips:tfx-1.2', 'command': ['python', '-m', 'tfx.orchestration.kubeflow.v2.container.kubeflow_v2_run_executor'], 'args': ['--executor_class_path', 'tfx.components.evaluator.executor.Executor', '--json_serialized_invocation_args', '{{$}}', '--project=grandelli-demo-295810', '--temp_location=gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/temp', '--region=us-central1', '--runner=DataflowRunner']}}, 'BaselineModelResolver-model-blessing-resolver_executor': {'resolver': {'outputArtifactQueries': {'model_blessing': {'filter': 'schema_title=\"tfx.ModelBlessing\" AND state=LIVE AND metadata.blessed.number_value=1'}}}}, 'VertexUploader_executor': {'container': {'image': 'gcr.io/grandelli-demo-295810/chicago-taxi-tips:tfx-1.2', 'command': ['python', '-m', 'tfx.orchestration.kubeflow.v2.container.kubeflow_v2_run_executor'], 'args': ['--executor_class_path', 'src.tfx_pipelines.components.vertex_model_uploader_Executor', '--json_serialized_invocation_args', '{{$}}', '--project=grandelli-demo-295810', '--temp_location=gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/temp', '--region=us-central1', '--runner=DataflowRunner']}}, 'TrainDataGen_executor': {'container': {'command': ['python', '-m', 'tfx.orchestration.kubeflow.v2.container.kubeflow_v2_run_executor'], 'args': ['--executor_class_path', 'tfx.extensions.google_cloud_big_query.example_gen.executor.Executor', '--json_serialized_invocation_args', '{{$}}', '--project=grandelli-demo-295810', '--temp_location=gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/temp', '--region=us-central1', '--runner=DataflowRunner'], 'image': 'gcr.io/grandelli-demo-295810/chicago-taxi-tips:tfx-1.2'}}, 'TestDataGen_executor': {'container': {'command': ['python', '-m', 'tfx.orchestration.kubeflow.v2.container.kubeflow_v2_run_executor'], 'image': 'gcr.io/grandelli-demo-295810/chicago-taxi-tips:tfx-1.2', 'args': ['--executor_class_path', 'tfx.extensions.google_cloud_big_query.example_gen.executor.Executor', '--json_serialized_invocation_args', '{{$}}', '--project=grandelli-demo-295810', '--temp_location=gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/temp', '--region=us-central1', '--runner=DataflowRunner']}}, 'HyperparamsGen_executor': {'container': {'image': 'gcr.io/grandelli-demo-295810/chicago-taxi-tips:tfx-1.2', 'args': ['--executor_class_path', 'src.tfx_pipelines.components.hyperparameters_gen_Executor', '--json_serialized_invocation_args', '{{$}}', '--project=grandelli-demo-295810', '--temp_location=gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/temp', '--region=us-central1', '--runner=DataflowRunner'], 'command': ['python', '-m', 'tfx.orchestration.kubeflow.v2.container.kubeflow_v2_run_executor']}}}}, 'root': {'dag': {'tasks': {'ModelEvaluator': {'taskInfo': {'name': 'ModelEvaluator'}, 'inputs': {'parameters': {'fairness_indicator_thresholds': {'runtimeValue': {'constantValue': {'stringValue': 'null'}}}, 'example_splits': {'runtimeValue': {'constantValue': {'stringValue': '[\"test\"]'}}}, 'eval_config': {'runtimeValue': {'constantValue': {'stringValue': '{\\n  \"metrics_specs\": [\\n    {\\n      \"metrics\": [\\n        {\\n          \"class_name\": \"ExampleCount\"\\n        },\\n        {\\n          \"class_name\": \"BinaryAccuracy\",\\n          \"threshold\": {\\n            \"change_threshold\": {\\n              \"absolute\": -1e-10,\\n              \"direction\": \"HIGHER_IS_BETTER\"\\n            },\\n            \"value_threshold\": {\\n              \"lower_bound\": 0.8\\n            }\\n          }\\n        }\\n      ]\\n    }\\n  ],\\n  \"model_specs\": [\\n    {\\n      \"label_key\": \"tip_bin\",\\n      \"prediction_key\": \"probabilities\",\\n      \"signature_name\": \"serving_tf_example\"\\n    }\\n  ],\\n  \"slicing_specs\": [\\n    {}\\n  ]\\n}'}}}}, 'artifacts': {'examples': {'taskOutputArtifact': {'producerTask': 'TestDataGen', 'outputArtifactKey': 'examples'}}, 'schema': {'taskOutputArtifact': {'outputArtifactKey': 'result', 'producerTask': 'SchemaImporter'}}, 'model': {'taskOutputArtifact': {'outputArtifactKey': 'model', 'producerTask': 'ModelTrainer'}}, 'baseline_model': {'taskOutputArtifact': {'outputArtifactKey': 'model', 'producerTask': 'BaselineModelResolver-model-resolver'}}}}, 'dependentTasks': ['BaselineModelResolver-model-resolver', 'ModelTrainer', 'SchemaImporter', 'TestDataGen'], 'componentRef': {'name': 'ModelEvaluator'}}, 'DataTransformer': {'componentRef': {'name': 'DataTransformer'}, 'inputs': {'artifacts': {'schema': {'taskOutputArtifact': {'producerTask': 'SchemaImporter', 'outputArtifactKey': 'result'}}, 'examples': {'taskOutputArtifact': {'outputArtifactKey': 'examples', 'producerTask': 'TrainDataGen'}}}, 'parameters': {'module_file': {'runtimeValue': {'constantValue': {'stringValue': 'src/preprocessing/transformations.py'}}}, 'disable_statistics': {'runtimeValue': {'constantValue': {'intValue': '0'}}}, 'splits_config': {'runtimeValue': {'constantValue': {'stringValue': '{\\n  \"analyze\": [\\n    \"train\"\\n  ],\\n  \"transform\": [\\n    \"train\",\\n    \"eval\"\\n  ]\\n}'}}}, 'force_tf_compat_v1': {'runtimeValue': {'constantValue': {'intValue': '1'}}}, 'custom_config': {'runtimeValue': {'constantValue': {'stringValue': 'null'}}}}}, 'dependentTasks': ['ExampleValidator', 'SchemaImporter', 'TrainDataGen'], 'taskInfo': {'name': 'DataTransformer'}}, 'StatisticsGen': {'dependentTasks': ['TrainDataGen'], 'taskInfo': {'name': 'StatisticsGen'}, 'componentRef': {'name': 'StatisticsGen'}, 'inputs': {'artifacts': {'examples': {'taskOutputArtifact': {'producerTask': 'TrainDataGen', 'outputArtifactKey': 'examples'}}}, 'parameters': {'exclude_splits': {'runtimeValue': {'constantValue': {'stringValue': '[]'}}}}}}, 'SchemaImporter': {'inputs': {'parameters': {'reimport': {'runtimeValue': {'constantValue': {'intValue': '0'}}}, 'artifact_uri': {'runtimeValue': {'constantValue': {'stringValue': 'src/raw_schema'}}}}}, 'taskInfo': {'name': 'SchemaImporter'}, 'componentRef': {'name': 'SchemaImporter'}}, 'BaselineModelResolver-model-resolver': {'taskInfo': {'name': 'BaselineModelResolver-model-resolver'}, 'componentRef': {'name': 'BaselineModelResolver-model-resolver'}, 'inputs': {'artifacts': {'input': {'taskOutputArtifact': {'producerTask': 'BaselineModelResolver-model-blessing-resolver', 'outputArtifactKey': 'model_blessing'}}}}}, 'TestDataGen': {'componentRef': {'name': 'TestDataGen'}, 'taskInfo': {'name': 'TestDataGen'}, 'inputs': {'parameters': {'input_config': {'runtimeValue': {'constantValue': {'stringValue': '{\\n  \"splits\": [\\n    {\\n      \"name\": \"single_split\",\\n      \"pattern\": \"\\\\n    SELECT \\\\n        IF(trip_month IS NULL, -1, trip_month) trip_month,\\\\n        IF(trip_day IS NULL, -1, trip_day) trip_day,\\\\n        IF(trip_day_of_week IS NULL, -1, trip_day_of_week) trip_day_of_week,\\\\n        IF(trip_hour IS NULL, -1, trip_hour) trip_hour,\\\\n        IF(trip_seconds IS NULL, -1, trip_seconds) trip_seconds,\\\\n        IF(trip_miles IS NULL, -1, trip_miles) trip_miles,\\\\n        IF(payment_type IS NULL, \\'NA\\', payment_type) payment_type,\\\\n        IF(pickup_grid IS NULL, \\'NA\\', pickup_grid) pickup_grid,\\\\n        IF(dropoff_grid IS NULL, \\'NA\\', dropoff_grid) dropoff_grid,\\\\n        IF(euclidean IS NULL, -1, euclidean) euclidean,\\\\n        IF(loc_cross IS NULL, \\'NA\\', loc_cross) loc_cross,\\\\n        tip_bin\\\\n    FROM partner_training.chicago_taxitrips_prep \\\\n    WHERE ML_use = \\'TEST\\'\\\\n    \"\\n    }\\n  ]\\n}'}}}, 'output_file_format': {'runtimeValue': {'constantValue': {'intValue': '5'}}}, 'output_config': {'runtimeValue': {'constantValue': {'stringValue': '{\\n  \"split_config\": {\\n    \"splits\": [\\n      {\\n        \"hash_buckets\": 1,\\n        \"name\": \"test\"\\n      }\\n    ]\\n  }\\n}'}}}, 'output_data_format': {'runtimeValue': {'constantValue': {'intValue': '6'}}}}}}, 'BaselineModelResolver-model-blessing-resolver': {'componentRef': {'name': 'BaselineModelResolver-model-blessing-resolver'}, 'taskInfo': {'name': 'BaselineModelResolver-model-blessing-resolver'}}, 'ModelTrainer': {'inputs': {'parameters': {'module_file': {'runtimeValue': {'constantValue': {'stringValue': 'src/model_training/runner.py'}}}, 'custom_config': {'runtimeValue': {'constantValue': {'stringValue': '{\"ai_platform_training_args\": {\"project\": \"grandelli-demo-295810\", \"worker_pool_specs\": [{\"container_spec\": {\"image_uri\": \"gcr.io/grandelli-demo-295810/chicago-taxi-tips:tfx-1.2\"}, \"machine_spec\": {\"machine_type\": \"n1-standard-4\"}, \"replica_count\": 1}]}, \"ai_platform_training_enable_ucaip\": true, \"ai_platform_training_ucaip_region\": \"us-central1\", \"use_gpu\": false}'}}}, 'eval_args': {'runtimeValue': {'constantValue': {'stringValue': '{}'}}}, 'train_args': {'runtimeValue': {'constantValue': {'stringValue': '{}'}}}}, 'artifacts': {'base_model': {'taskOutputArtifact': {'producerTask': 'WarmstartModelResolver', 'outputArtifactKey': 'latest_model'}}, 'hyperparameters': {'taskOutputArtifact': {'producerTask': 'HyperparamsGen', 'outputArtifactKey': 'hyperparameters'}}, 'schema': {'taskOutputArtifact': {'outputArtifactKey': 'result', 'producerTask': 'SchemaImporter'}}, 'transform_graph': {'taskOutputArtifact': {'outputArtifactKey': 'transform_graph', 'producerTask': 'DataTransformer'}}, 'examples': {'taskOutputArtifact': {'outputArtifactKey': 'transformed_examples', 'producerTask': 'DataTransformer'}}}}, 'componentRef': {'name': 'ModelTrainer'}, 'dependentTasks': ['DataTransformer', 'HyperparamsGen', 'SchemaImporter', 'WarmstartModelResolver'], 'taskInfo': {'name': 'ModelTrainer'}}, 'WarmstartModelResolver': {'taskInfo': {'name': 'WarmstartModelResolver'}, 'componentRef': {'name': 'WarmstartModelResolver'}, 'inputs': {'parameters': {'resolver_class': {'runtimeValue': {'constantValue': {'stringValue': '{\"__class__\": \"LatestArtifactStrategy\", \"__module__\": \"tfx.dsl.input_resolution.strategies.latest_artifact_strategy\", \"__tfx_object_type__\": \"class\"}'}}}, 'source_uri': {'runtimeValue': {'constantValue': {'stringValue': '{}'}}}}}}, 'ModelPusher': {'taskInfo': {'name': 'ModelPusher'}, 'inputs': {'parameters': {'push_destination': {'runtimeValue': {'constantValue': {'stringValue': '{\\n  \"filesystem\": {\\n    \"base_directory\": \"gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/model_registry/chicago-taxi-tips-classifier-v01\"\\n  }\\n}'}}}, 'custom_config': {'runtimeValue': {'constantValue': {'stringValue': 'null'}}}}, 'artifacts': {'model_blessing': {'taskOutputArtifact': {'outputArtifactKey': 'blessing', 'producerTask': 'ModelEvaluator'}}, 'model': {'taskOutputArtifact': {'producerTask': 'ModelTrainer', 'outputArtifactKey': 'model'}}}}, 'componentRef': {'name': 'ModelPusher'}, 'dependentTasks': ['ModelEvaluator', 'ModelTrainer']}, 'HyperparamsGen': {'componentRef': {'name': 'HyperparamsGen'}, 'inputs': {'parameters': {'learning_rate': {'componentInputParameter': 'learning_rate'}, 'batch_size': {'componentInputParameter': 'batch_size'}, 'hidden_units': {'componentInputParameter': 'hidden_units'}, 'num_epochs': {'componentInputParameter': 'num_epochs'}}}, 'taskInfo': {'name': 'HyperparamsGen'}}, 'TrainDataGen': {'componentRef': {'name': 'TrainDataGen'}, 'inputs': {'parameters': {'output_file_format': {'runtimeValue': {'constantValue': {'intValue': '5'}}}, 'output_data_format': {'runtimeValue': {'constantValue': {'intValue': '6'}}}, 'output_config': {'runtimeValue': {'constantValue': {'stringValue': '{\\n  \"split_config\": {\\n    \"splits\": [\\n      {\\n        \"hash_buckets\": 4,\\n        \"name\": \"train\"\\n      },\\n      {\\n        \"hash_buckets\": 1,\\n        \"name\": \"eval\"\\n      }\\n    ]\\n  }\\n}'}}}, 'input_config': {'runtimeValue': {'constantValue': {'stringValue': '{\\n  \"splits\": [\\n    {\\n      \"name\": \"single_split\",\\n      \"pattern\": \"\\\\n    SELECT \\\\n        IF(trip_month IS NULL, -1, trip_month) trip_month,\\\\n        IF(trip_day IS NULL, -1, trip_day) trip_day,\\\\n        IF(trip_day_of_week IS NULL, -1, trip_day_of_week) trip_day_of_week,\\\\n        IF(trip_hour IS NULL, -1, trip_hour) trip_hour,\\\\n        IF(trip_seconds IS NULL, -1, trip_seconds) trip_seconds,\\\\n        IF(trip_miles IS NULL, -1, trip_miles) trip_miles,\\\\n        IF(payment_type IS NULL, \\'NA\\', payment_type) payment_type,\\\\n        IF(pickup_grid IS NULL, \\'NA\\', pickup_grid) pickup_grid,\\\\n        IF(dropoff_grid IS NULL, \\'NA\\', dropoff_grid) dropoff_grid,\\\\n        IF(euclidean IS NULL, -1, euclidean) euclidean,\\\\n        IF(loc_cross IS NULL, \\'NA\\', loc_cross) loc_cross,\\\\n        tip_bin\\\\n    FROM partner_training.chicago_taxitrips_prep \\\\n    WHERE ML_use = \\'UNASSIGNED\\'\\\\n    \"\\n    }\\n  ]\\n}'}}}}}, 'taskInfo': {'name': 'TrainDataGen'}}, 'VertexUploader': {'inputs': {'parameters': {'region': {'runtimeValue': {'constantValue': {'stringValue': 'us-central1'}}}, 'project': {'runtimeValue': {'constantValue': {'stringValue': 'grandelli-demo-295810'}}}, 'labels': {'runtimeValue': {'constantValue': {'stringValue': '{\"dataset_name\": \"chicago-taxi-tips\", \"pipeline_name\": \"chicago-taxi-tips-classifier-v01-train-pipeline\", \"pipeline_root\": \"gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline\"}'}}}, 'serving_image_uri': {'runtimeValue': {'constantValue': {'stringValue': 'us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-5:latest'}}}, 'pushed_model_location': {'runtimeValue': {'constantValue': {'stringValue': 'gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/model_registry/chicago-taxi-tips-classifier-v01'}}}, 'model_display_name': {'runtimeValue': {'constantValue': {'stringValue': 'chicago-taxi-tips-classifier-v01'}}}, 'explanation_config': {'runtimeValue': {'constantValue': {'stringValue': '{\"inputs\": {\"trip_month\": {\"input_tensor_name\": \"trip_month\", \"encoding\": \"IDENTITY\", \"modality\": \"categorical\"}, \"trip_day\": {\"input_tensor_name\": \"trip_day\", \"encoding\": \"IDENTITY\", \"modality\": \"categorical\"}, \"trip_day_of_week\": {\"input_tensor_name\": \"trip_day_of_week\", \"encoding\": \"IDENTITY\", \"modality\": \"categorical\"}, \"trip_hour\": {\"input_tensor_name\": \"trip_hour\", \"encoding\": \"IDENTITY\", \"modality\": \"categorical\"}, \"trip_seconds\": {\"input_tensor_name\": \"trip_seconds\", \"modality\": \"numeric\"}, \"trip_miles\": {\"input_tensor_name\": \"trip_miles\", \"modality\": \"numeric\"}, \"payment_type\": {\"input_tensor_name\": \"payment_type\", \"encoding\": \"IDENTITY\", \"modality\": \"categorical\"}, \"pickup_grid\": {\"input_tensor_name\": \"pickup_grid\", \"encoding\": \"IDENTITY\", \"modality\": \"categorical\"}, \"dropoff_grid\": {\"input_tensor_name\": \"dropoff_grid\", \"encoding\": \"IDENTITY\", \"modality\": \"categorical\"}, \"euclidean\": {\"input_tensor_name\": \"euclidean\", \"modality\": \"numeric\"}, \"loc_cross\": {\"input_tensor_name\": \"loc_cross\", \"encoding\": \"IDENTITY\", \"modality\": \"categorical\"}}, \"outputs\": {\"scores\": {\"output_tensor_name\": \"scores\"}}, \"params\": {\"sampled_shapley_attribution\": {\"path_count\": 10}}}'}}}}, 'artifacts': {'model_blessing': {'taskOutputArtifact': {'outputArtifactKey': 'blessing', 'producerTask': 'ModelEvaluator'}}}}, 'dependentTasks': ['ModelEvaluator', 'ModelPusher'], 'taskInfo': {'name': 'VertexUploader'}, 'componentRef': {'name': 'VertexUploader'}}, 'ExampleValidator': {'componentRef': {'name': 'ExampleValidator'}, 'inputs': {'parameters': {'exclude_splits': {'runtimeValue': {'constantValue': {'stringValue': '[]'}}}}, 'artifacts': {'statistics': {'taskOutputArtifact': {'producerTask': 'StatisticsGen', 'outputArtifactKey': 'statistics'}}, 'schema': {'taskOutputArtifact': {'outputArtifactKey': 'result', 'producerTask': 'SchemaImporter'}}}}, 'taskInfo': {'name': 'ExampleValidator'}, 'dependentTasks': ['SchemaImporter', 'StatisticsGen']}}}, 'inputDefinitions': {'parameters': {'num_epochs': {'type': 'INT'}, 'batch_size': {'type': 'INT'}, 'learning_rate': {'type': 'DOUBLE'}, 'hidden_units': {'type': 'STRING'}}}}, 'sdkVersion': 'tfx-1.2.0', 'components': {'VertexUploader': {'executorLabel': 'VertexUploader_executor', 'inputDefinitions': {'artifacts': {'model_blessing': {'artifactType': {'instanceSchema': 'title: tfx.ModelBlessing\\ntype: object\\n'}}}, 'parameters': {'explanation_config': {'type': 'STRING'}, 'serving_image_uri': {'type': 'STRING'}, 'pushed_model_location': {'type': 'STRING'}, 'labels': {'type': 'STRING'}, 'region': {'type': 'STRING'}, 'model_display_name': {'type': 'STRING'}, 'project': {'type': 'STRING'}}}, 'outputDefinitions': {'artifacts': {'uploaded_model': {'artifactType': {'instanceSchema': 'title: tfx.File\\ntype: object\\n'}}}}}, 'TestDataGen': {'executorLabel': 'TestDataGen_executor', 'inputDefinitions': {'parameters': {'output_data_format': {'type': 'INT'}, 'input_config': {'type': 'STRING'}, 'output_file_format': {'type': 'INT'}, 'output_config': {'type': 'STRING'}}}, 'outputDefinitions': {'artifacts': {'examples': {'artifactType': {'instanceSchema': 'title: tfx.Examples\\ntype: object\\nproperties:\\n  span:\\n    type: integer\\n    description: Span for an artifact.\\n  version:\\n    type: integer\\n    description: Version for an artifact.\\n  split_names:\\n    type: string\\n    description: JSON-encoded list of splits for an artifact. Empty string means artifact has no split.\\n'}}}}}, 'WarmstartModelResolver': {'outputDefinitions': {'artifacts': {'latest_model': {'artifactType': {'instanceSchema': 'title: tfx.Model\\ntype: object\\n'}}}}, 'inputDefinitions': {'parameters': {'resolver_class': {'type': 'STRING'}, 'source_uri': {'type': 'STRING'}}}, 'executorLabel': 'WarmstartModelResolver_executor'}, 'StatisticsGen': {'executorLabel': 'StatisticsGen_executor', 'outputDefinitions': {'artifacts': {'statistics': {'artifactType': {'instanceSchema': 'title: tfx.ExampleStatistics\\ntype: object\\nproperties:\\n  span:\\n    type: integer\\n    description: Span for an artifact.\\n  split_names:\\n    type: string\\n    description: JSON-encoded list of splits for an artifact. Empty string means artifact has no split.\\n'}}}}, 'inputDefinitions': {'artifacts': {'examples': {'artifactType': {'instanceSchema': 'title: tfx.Examples\\ntype: object\\nproperties:\\n  span:\\n    type: integer\\n    description: Span for an artifact.\\n  version:\\n    type: integer\\n    description: Version for an artifact.\\n  split_names:\\n    type: string\\n    description: JSON-encoded list of splits for an artifact. Empty string means artifact has no split.\\n'}}}, 'parameters': {'exclude_splits': {'type': 'STRING'}}}}, 'ModelEvaluator': {'outputDefinitions': {'artifacts': {'evaluation': {'artifactType': {'instanceSchema': 'title: tfx.ModelEvaluation\\ntype: object\\n'}}, 'blessing': {'artifactType': {'instanceSchema': 'title: tfx.ModelBlessing\\ntype: object\\n'}}}}, 'executorLabel': 'ModelEvaluator_executor', 'inputDefinitions': {'parameters': {'eval_config': {'type': 'STRING'}, 'fairness_indicator_thresholds': {'type': 'STRING'}, 'example_splits': {'type': 'STRING'}}, 'artifacts': {'model': {'artifactType': {'instanceSchema': 'title: tfx.Model\\ntype: object\\n'}}, 'examples': {'artifactType': {'instanceSchema': 'title: tfx.Examples\\ntype: object\\nproperties:\\n  span:\\n    type: integer\\n    description: Span for an artifact.\\n  version:\\n    type: integer\\n    description: Version for an artifact.\\n  split_names:\\n    type: string\\n    description: JSON-encoded list of splits for an artifact. Empty string means artifact has no split.\\n'}}, 'schema': {'artifactType': {'instanceSchema': 'title: tfx.Schema\\ntype: object\\n'}}, 'baseline_model': {'artifactType': {'instanceSchema': 'title: tfx.Model\\ntype: object\\n'}}}}}, 'BaselineModelResolver-model-blessing-resolver': {'outputDefinitions': {'artifacts': {'model_blessing': {'artifactType': {'instanceSchema': 'title: tfx.ModelBlessing\\ntype: object\\n'}}}}, 'executorLabel': 'BaselineModelResolver-model-blessing-resolver_executor'}, 'BaselineModelResolver-model-resolver': {'executorLabel': 'BaselineModelResolver-model-resolver_executor', 'inputDefinitions': {'artifacts': {'input': {'artifactType': {'instanceSchema': 'title: tfx.ModelBlessing\\ntype: object\\n'}}}}, 'outputDefinitions': {'artifacts': {'model': {'artifactType': {'instanceSchema': 'title: tfx.Model\\ntype: object\\n'}}}}}, 'SchemaImporter': {'executorLabel': 'SchemaImporter_executor', 'outputDefinitions': {'artifacts': {'result': {'artifactType': {'instanceSchema': 'title: tfx.Schema\\ntype: object\\n'}}}}, 'inputDefinitions': {'parameters': {'reimport': {'type': 'INT'}, 'artifact_uri': {'type': 'STRING'}}}}, 'ExampleValidator': {'outputDefinitions': {'artifacts': {'anomalies': {'artifactType': {'instanceSchema': 'title: tfx.ExampleAnomalies\\ntype: object\\nproperties:\\n  span:\\n    type: integer\\n    description: Span for an artifact.\\n  split_names:\\n    type: string\\n    description: JSON-encoded list of splits for an artifact. Empty string means artifact has no split.\\n'}}}}, 'executorLabel': 'ExampleValidator_executor', 'inputDefinitions': {'parameters': {'exclude_splits': {'type': 'STRING'}}, 'artifacts': {'statistics': {'artifactType': {'instanceSchema': 'title: tfx.ExampleStatistics\\ntype: object\\nproperties:\\n  span:\\n    type: integer\\n    description: Span for an artifact.\\n  split_names:\\n    type: string\\n    description: JSON-encoded list of splits for an artifact. Empty string means artifact has no split.\\n'}}, 'schema': {'artifactType': {'instanceSchema': 'title: tfx.Schema\\ntype: object\\n'}}}}}, 'ModelPusher': {'executorLabel': 'ModelPusher_executor', 'inputDefinitions': {'artifacts': {'model': {'artifactType': {'instanceSchema': 'title: tfx.Model\\ntype: object\\n'}}, 'model_blessing': {'artifactType': {'instanceSchema': 'title: tfx.ModelBlessing\\ntype: object\\n'}}}, 'parameters': {'push_destination': {'type': 'STRING'}, 'custom_config': {'type': 'STRING'}}}, 'outputDefinitions': {'artifacts': {'pushed_model': {'artifactType': {'instanceSchema': 'title: tfx.PushedModel\\ntype: object\\n'}}}}}, 'DataTransformer': {'outputDefinitions': {'artifacts': {'pre_transform_schema': {'artifactType': {'instanceSchema': 'title: tfx.Schema\\ntype: object\\n'}}, 'post_transform_schema': {'artifactType': {'instanceSchema': 'title: tfx.Schema\\ntype: object\\n'}}, 'transform_graph': {'artifactType': {'instanceSchema': 'title: tfx.TransformGraph\\ntype: object\\n'}}, 'pre_transform_stats': {'artifactType': {'instanceSchema': 'title: tfx.ExampleStatistics\\ntype: object\\nproperties:\\n  span:\\n    type: integer\\n    description: Span for an artifact.\\n  split_names:\\n    type: string\\n    description: JSON-encoded list of splits for an artifact. Empty string means artifact has no split.\\n'}}, 'post_transform_stats': {'artifactType': {'instanceSchema': 'title: tfx.ExampleStatistics\\ntype: object\\nproperties:\\n  span:\\n    type: integer\\n    description: Span for an artifact.\\n  split_names:\\n    type: string\\n    description: JSON-encoded list of splits for an artifact. Empty string means artifact has no split.\\n'}}, 'updated_analyzer_cache': {'artifactType': {'instanceSchema': 'title: tfx.TransformCache\\ntype: object\\n'}}, 'transformed_examples': {'artifactType': {'instanceSchema': 'title: tfx.Examples\\ntype: object\\nproperties:\\n  span:\\n    type: integer\\n    description: Span for an artifact.\\n  version:\\n    type: integer\\n    description: Version for an artifact.\\n  split_names:\\n    type: string\\n    description: JSON-encoded list of splits for an artifact. Empty string means artifact has no split.\\n'}}, 'post_transform_anomalies': {'artifactType': {'instanceSchema': 'title: tfx.ExampleAnomalies\\ntype: object\\nproperties:\\n  span:\\n    type: integer\\n    description: Span for an artifact.\\n  split_names:\\n    type: string\\n    description: JSON-encoded list of splits for an artifact. Empty string means artifact has no split.\\n'}}}}, 'executorLabel': 'DataTransformer_executor', 'inputDefinitions': {'parameters': {'force_tf_compat_v1': {'type': 'INT'}, 'splits_config': {'type': 'STRING'}, 'module_file': {'type': 'STRING'}, 'disable_statistics': {'type': 'INT'}, 'custom_config': {'type': 'STRING'}}, 'artifacts': {'schema': {'artifactType': {'instanceSchema': 'title: tfx.Schema\\ntype: object\\n'}}, 'examples': {'artifactType': {'instanceSchema': 'title: tfx.Examples\\ntype: object\\nproperties:\\n  span:\\n    type: integer\\n    description: Span for an artifact.\\n  version:\\n    type: integer\\n    description: Version for an artifact.\\n  split_names:\\n    type: string\\n    description: JSON-encoded list of splits for an artifact. Empty string means artifact has no split.\\n'}}}}}, 'HyperparamsGen': {'executorLabel': 'HyperparamsGen_executor', 'inputDefinitions': {'parameters': {'learning_rate': {'type': 'DOUBLE'}, 'batch_size': {'type': 'INT'}, 'hidden_units': {'type': 'STRING'}, 'num_epochs': {'type': 'INT'}}}, 'outputDefinitions': {'artifacts': {'hyperparameters': {'artifactType': {'instanceSchema': 'title: tfx.HyperParameters\\ntype: object\\n'}}}}}, 'TrainDataGen': {'inputDefinitions': {'parameters': {'output_file_format': {'type': 'INT'}, 'output_config': {'type': 'STRING'}, 'output_data_format': {'type': 'INT'}, 'input_config': {'type': 'STRING'}}}, 'outputDefinitions': {'artifacts': {'examples': {'artifactType': {'instanceSchema': 'title: tfx.Examples\\ntype: object\\nproperties:\\n  span:\\n    type: integer\\n    description: Span for an artifact.\\n  version:\\n    type: integer\\n    description: Version for an artifact.\\n  split_names:\\n    type: string\\n    description: JSON-encoded list of splits for an artifact. Empty string means artifact has no split.\\n'}}}}, 'executorLabel': 'TrainDataGen_executor'}, 'ModelTrainer': {'executorLabel': 'ModelTrainer_executor', 'outputDefinitions': {'artifacts': {'model': {'artifactType': {'instanceSchema': 'title: tfx.Model\\ntype: object\\n'}}, 'model_run': {'artifactType': {'instanceSchema': 'title: tfx.ModelRun\\ntype: object\\n'}}}}, 'inputDefinitions': {'artifacts': {'examples': {'artifactType': {'instanceSchema': 'title: tfx.Examples\\ntype: object\\nproperties:\\n  span:\\n    type: integer\\n    description: Span for an artifact.\\n  version:\\n    type: integer\\n    description: Version for an artifact.\\n  split_names:\\n    type: string\\n    description: JSON-encoded list of splits for an artifact. Empty string means artifact has no split.\\n'}}, 'hyperparameters': {'artifactType': {'instanceSchema': 'title: tfx.HyperParameters\\ntype: object\\n'}}, 'schema': {'artifactType': {'instanceSchema': 'title: tfx.Schema\\ntype: object\\n'}}, 'base_model': {'artifactType': {'instanceSchema': 'title: tfx.Model\\ntype: object\\n'}}, 'transform_graph': {'artifactType': {'instanceSchema': 'title: tfx.TransformGraph\\ntype: object\\n'}}}, 'parameters': {'custom_config': {'type': 'STRING'}, 'module_file': {'type': 'STRING'}, 'train_args': {'type': 'STRING'}, 'eval_args': {'type': 'STRING'}}}}}}, 'labels': {'tfx_version': '1-2-0', 'tfx_py_version': '3-7', 'tfx_runner': 'kubeflow_v2'}, 'runtimeConfig': {'gcsOutputDirectory': 'gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline'}}\n",
      "Finished Step #5 - \"Compile Pipeline\"\n",
      "Starting Step #6 - \"Upload Pipeline to GCS\"\n",
      "Step #6 - \"Upload Pipeline to GCS\": Already have image (with digest): gcr.io/cloud-builders/gsutil\n",
      "Step #4 - \"Build TFX Image\":   Installing build dependencies: finished with status 'done'\n",
      "Step #4 - \"Build TFX Image\":   Getting requirements to build wheel: started\n",
      "Step #4 - \"Build TFX Image\":   Getting requirements to build wheel: finished with status 'done'\n",
      "Step #4 - \"Build TFX Image\":     Preparing wheel metadata: started\n",
      "Step #6 - \"Upload Pipeline to GCS\": Copying file://chicago-taxi-tips-classifier-v01-train-pipeline.json [Content-Type=application/json]...\n",
      "/ [1 files][ 30.0 KiB/ 30.0 KiB]                                                                                    \n",
      "Step #6 - \"Upload Pipeline to GCS\": Operation completed over 1 objects/30.0 KiB.                                     \n",
      "Step #4 - \"Build TFX Image\":     Preparing wheel metadata: finished with status 'done'\n",
      "Finished Step #6 - \"Upload Pipeline to GCS\"\n",
      "Step #4 - \"Build TFX Image\": Collecting kfp-pipeline-spec<0.2.0,>=0.1.10\n",
      "Step #4 - \"Build TFX Image\":   Downloading kfp_pipeline_spec-0.1.14-py3-none-any.whl (18 kB)\n",
      "Step #4 - \"Build TFX Image\": Collecting fire<1,>=0.3.1\n",
      "Step #4 - \"Build TFX Image\":   Downloading fire-0.4.0.tar.gz (87 kB)\n",
      "Step #4 - \"Build TFX Image\": Requirement already satisfied: protobuf<4,>=3.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.1->-r requirements.txt (line 1)) (3.16.0)\n",
      "Step #4 - \"Build TFX Image\": Requirement already satisfied: pydantic<2,>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.1->-r requirements.txt (line 1)) (1.8.2)\n",
      "Step #4 - \"Build TFX Image\": Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.26.0->-r requirements.txt (line 2)) (1.3.2)\n",
      "Step #4 - \"Build TFX Image\": Requirement already satisfied: google-api-core[grpc]<3.0.0dev,>=1.29.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.26.0->-r requirements.txt (line 2)) (1.31.1)\n",
      "Step #4 - \"Build TFX Image\": Collecting grpcio<2.0dev,>=1.38.1\n",
      "Step #4 - \"Build TFX Image\":   Downloading grpcio-1.44.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
      "Step #4 - \"Build TFX Image\": Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.26.0->-r requirements.txt (line 2)) (1.7.2)\n",
      "Step #4 - \"Build TFX Image\": Requirement already satisfied: proto-plus>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.26.0->-r requirements.txt (line 2)) (1.19.0)\n",
      "Step #4 - \"Build TFX Image\": Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.26.0->-r requirements.txt (line 2)) (20.9)\n",
      "Step #4 - \"Build TFX Image\": Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.26.0->-r requirements.txt (line 2)) (2.25.1)\n",
      "Step #4 - \"Build TFX Image\": Requirement already satisfied: libcst>=0.2.5 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery-storage==2.7.0->-r requirements.txt (line 3)) (0.3.19)\n",
      "Step #4 - \"Build TFX Image\": Collecting pluggy<2.0,>=0.12\n",
      "Step #4 - \"Build TFX Image\":   Downloading pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\n",
      "Step #4 - \"Build TFX Image\": Requirement already satisfied: importlib-metadata>=0.12 in /opt/conda/lib/python3.7/site-packages (from pytest->-r requirements.txt (line 6)) (4.6.3)\n",
      "Step #4 - \"Build TFX Image\": Collecting iniconfig\n",
      "Step #4 - \"Build TFX Image\":   Downloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)\n",
      "Step #4 - \"Build TFX Image\": Requirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.7/site-packages (from pytest->-r requirements.txt (line 6)) (20.3.0)\n",
      "Step #4 - \"Build TFX Image\": Collecting py>=1.8.2\n",
      "Step #4 - \"Build TFX Image\":   Downloading py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
      "Step #4 - \"Build TFX Image\": Requirement already satisfied: tomli>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from pytest->-r requirements.txt (line 6)) (1.1.0)\n",
      "Step #4 - \"Build TFX Image\": Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from absl-py<=0.11,>=0.9->kfp==1.8.1->-r requirements.txt (line 1)) (1.15.0)\n",
      "Step #4 - \"Build TFX Image\": Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated<2,>=1.2.7->kfp==1.8.1->-r requirements.txt (line 1)) (1.12.1)\n",
      "Step #4 - \"Build TFX Image\": Requirement already satisfied: termcolor in /opt/conda/lib/python3.7/site-packages (from fire<1,>=0.3.1->kfp==1.8.1->-r requirements.txt (line 1)) (1.1.0)\n",
      "Step #4 - \"Build TFX Image\": Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.29.0->google-cloud-bigquery==2.26.0->-r requirements.txt (line 2)) (2021.1)\n",
      "Step #4 - \"Build TFX Image\": Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.29.0->google-cloud-bigquery==2.26.0->-r requirements.txt (line 2)) (49.6.0.post20210108)\n",
      "Step #4 - \"Build TFX Image\": Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.29.0->google-cloud-bigquery==2.26.0->-r requirements.txt (line 2)) (1.53.0)\n",
      "Step #4 - \"Build TFX Image\": Requirement already satisfied: httplib2<1dev,>=0.15.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp==1.8.1->-r requirements.txt (line 1)) (0.19.1)\n",
      "Step #4 - \"Build TFX Image\": Requirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp==1.8.1->-r requirements.txt (line 1)) (0.1.0)\n",
      "Step #4 - \"Build TFX Image\": Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp==1.8.1->-r requirements.txt (line 1)) (3.0.1)\n",
      "Step #4 - \"Build TFX Image\": Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.8.1->-r requirements.txt (line 1)) (4.7.2)\n",
      "Step #4 - \"Build TFX Image\": Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.8.1->-r requirements.txt (line 1)) (4.2.2)\n",
      "Step #4 - \"Build TFX Image\": Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.8.1->-r requirements.txt (line 1)) (0.2.7)\n",
      "Step #4 - \"Build TFX Image\": Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery==2.26.0->-r requirements.txt (line 2)) (1.1.2)\n",
      "Step #4 - \"Build TFX Image\": Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery==2.26.0->-r requirements.txt (line 2)) (1.14.6)\n",
      "Step #4 - \"Build TFX Image\": Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery==2.26.0->-r requirements.txt (line 2)) (2.20)\n",
      "Step #4 - \"Build TFX Image\": Requirement already satisfied: pyparsing<3,>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from httplib2<1dev,>=0.15.0->google-api-python-client<2,>=1.7.8->kfp==1.8.1->-r requirements.txt (line 1)) (2.4.7)\n",
      "Step #4 - \"Build TFX Image\": Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.12->pytest->-r requirements.txt (line 6)) (3.7.4.3)\n",
      "Step #4 - \"Build TFX Image\": Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.12->pytest->-r requirements.txt (line 6)) (3.5.0)\n",
      "Step #4 - \"Build TFX Image\": Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp==1.8.1->-r requirements.txt (line 1)) (0.17.3)\n",
      "Step #4 - \"Build TFX Image\": Requirement already satisfied: urllib3>=1.15 in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.8.1->-r requirements.txt (line 1)) (1.26.6)\n",
      "Step #4 - \"Build TFX Image\": Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.8.1->-r requirements.txt (line 1)) (2021.5.30)\n",
      "Step #4 - \"Build TFX Image\": Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.8.1->-r requirements.txt (line 1)) (2.8.2)\n",
      "Step #4 - \"Build TFX Image\": Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<19,>=8.0.0->kfp==1.8.1->-r requirements.txt (line 1)) (1.3.0)\n",
      "Step #4 - \"Build TFX Image\": Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<19,>=8.0.0->kfp==1.8.1->-r requirements.txt (line 1)) (0.57.0)\n",
      "Step #4 - \"Build TFX Image\": Requirement already satisfied: typing-inspect>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from libcst>=0.2.5->google-cloud-bigquery-storage==2.7.0->-r requirements.txt (line 3)) (0.7.1)\n",
      "Step #4 - \"Build TFX Image\": Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.1->kfp==1.8.1->-r requirements.txt (line 1)) (0.4.8)\n",
      "Step #4 - \"Build TFX Image\": Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery==2.26.0->-r requirements.txt (line 2)) (4.0.0)\n",
      "Step #4 - \"Build TFX Image\": Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery==2.26.0->-r requirements.txt (line 2)) (2.10)\n",
      "Step #4 - \"Build TFX Image\": Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints<1,>=0.1.8->kfp==1.8.1->-r requirements.txt (line 1)) (0.36.2)\n",
      "Step #4 - \"Build TFX Image\": Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from typing-inspect>=0.4.0->libcst>=0.2.5->google-cloud-bigquery-storage==2.7.0->-r requirements.txt (line 3)) (0.4.3)\n",
      "Step #4 - \"Build TFX Image\": Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<19,>=8.0.0->kfp==1.8.1->-r requirements.txt (line 1)) (3.1.1)\n",
      "Step #4 - \"Build TFX Image\": Building wheels for collected packages: kfp, cloudml-hypertune, docstring-parser, fire, kfp-server-api, strip-hints\n",
      "Step #4 - \"Build TFX Image\":   Building wheel for kfp (setup.py): started\n",
      "Step #4 - \"Build TFX Image\":   Building wheel for kfp (setup.py): finished with status 'done'\n",
      "Step #4 - \"Build TFX Image\":   Created wheel for kfp: filename=kfp-1.8.1-py3-none-any.whl size=345340 sha256=e0ec57b04e619cfea6543e6ceae84c6dbf4a7b1ac6d985fdf9e053531fbbdb5e\n",
      "Step #4 - \"Build TFX Image\":   Stored in directory: /root/.cache/pip/wheels/02/dd/71/45fa445fd9ea0c60ab0bd00b31760b1102ef2999f8002ee892\n",
      "Step #4 - \"Build TFX Image\":   Building wheel for cloudml-hypertune (setup.py): started\n",
      "Step #4 - \"Build TFX Image\":   Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "Step #4 - \"Build TFX Image\":   Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3988 sha256=252bdfbc9aa9e23802cdc678f4f8b86999648aaa5a20b0aaea2045607816cbce\n",
      "Step #4 - \"Build TFX Image\":   Stored in directory: /root/.cache/pip/wheels/a7/ff/87/e7bed0c2741fe219b3d6da67c2431d7f7fedb183032e00f81e\n",
      "Step #4 - \"Build TFX Image\":   Building wheel for docstring-parser (PEP 517): started\n",
      "Step #4 - \"Build TFX Image\":   Building wheel for docstring-parser (PEP 517): finished with status 'done'\n",
      "Step #4 - \"Build TFX Image\":   Created wheel for docstring-parser: filename=docstring_parser-0.13-py3-none-any.whl size=31866 sha256=17d81e242e3cbaf6e03e59b1cdcdc897957f73d1a0484b3cbe37d4e104a76eae\n",
      "Step #4 - \"Build TFX Image\":   Stored in directory: /root/.cache/pip/wheels/bd/88/3c/d1aa049309f7945178cac9fbe6561a86424f432da57c18ca0f\n",
      "Step #4 - \"Build TFX Image\":   Building wheel for fire (setup.py): started\n",
      "Step #4 - \"Build TFX Image\":   Building wheel for fire (setup.py): finished with status 'done'\n",
      "Step #4 - \"Build TFX Image\":   Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115928 sha256=390e5a50fcabd331bbd4d6fddb2ef6e00a3e5c4db3c1b84806f665a10595b248\n",
      "Step #4 - \"Build TFX Image\":   Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
      "Step #4 - \"Build TFX Image\":   Building wheel for kfp-server-api (setup.py): started\n",
      "Step #4 - \"Build TFX Image\":   Building wheel for kfp-server-api (setup.py): finished with status 'done'\n",
      "Step #4 - \"Build TFX Image\":   Created wheel for kfp-server-api: filename=kfp_server_api-1.8.1-py3-none-any.whl size=95548 sha256=d6e51dc4da6a1c909afbf2e49da86f2c95ce19e3bed5ace9b9977424de21129a\n",
      "Step #4 - \"Build TFX Image\":   Stored in directory: /root/.cache/pip/wheels/f5/4e/2e/6795bd3ed456a43652e7de100aca275ec179c9a8dfbcc65626\n",
      "Step #4 - \"Build TFX Image\":   Building wheel for strip-hints (setup.py): started\n",
      "Step #4 - \"Build TFX Image\":   Building wheel for strip-hints (setup.py): finished with status 'done'\n",
      "Step #4 - \"Build TFX Image\":   Created wheel for strip-hints: filename=strip_hints-0.1.10-py2.py3-none-any.whl size=22279 sha256=b3ba2a8791f321598459fe831a4942c966990d49e0ae2a0116b4c17b48c1836b\n",
      "Step #4 - \"Build TFX Image\":   Stored in directory: /root/.cache/pip/wheels/5e/14/c3/6e44e9b2545f2d570b03f5b6d38c00b7534aa8abb376978363\n",
      "Step #4 - \"Build TFX Image\": Successfully built kfp cloudml-hypertune docstring-parser fire kfp-server-api strip-hints\n",
      "Step #4 - \"Build TFX Image\": Installing collected packages: grpcio, strip-hints, requests-toolbelt, py, pluggy, kfp-server-api, kfp-pipeline-spec, iniconfig, google-cloud-bigquery, fire, docstring-parser, Deprecated, absl-py, pytest, kfp, google-cloud-bigquery-storage, google-cloud-aiplatform, cloudml-hypertune\n",
      "Step #4 - \"Build TFX Image\":   Attempting uninstall: grpcio\n",
      "Step #4 - \"Build TFX Image\":     Found existing installation: grpcio 1.34.1\n",
      "Step #4 - \"Build TFX Image\":     Uninstalling grpcio-1.34.1:\n",
      "Step #4 - \"Build TFX Image\":       Successfully uninstalled grpcio-1.34.1\n",
      "Step #4 - \"Build TFX Image\":   Attempting uninstall: kfp-pipeline-spec\n",
      "Step #4 - \"Build TFX Image\":     Found existing installation: kfp-pipeline-spec 0.1.9\n",
      "Step #4 - \"Build TFX Image\":     Uninstalling kfp-pipeline-spec-0.1.9:\n",
      "Step #4 - \"Build TFX Image\":       Successfully uninstalled kfp-pipeline-spec-0.1.9\n",
      "Step #4 - \"Build TFX Image\":   Attempting uninstall: google-cloud-bigquery\n",
      "Step #4 - \"Build TFX Image\":     Found existing installation: google-cloud-bigquery 2.20.0\n",
      "Step #4 - \"Build TFX Image\":     Uninstalling google-cloud-bigquery-2.20.0:\n",
      "Step #4 - \"Build TFX Image\":       Successfully uninstalled google-cloud-bigquery-2.20.0\n",
      "Step #4 - \"Build TFX Image\":   Attempting uninstall: absl-py\n",
      "Step #4 - \"Build TFX Image\":     Found existing installation: absl-py 0.12.0\n",
      "Step #4 - \"Build TFX Image\":     Uninstalling absl-py-0.12.0:\n",
      "Step #4 - \"Build TFX Image\":       Successfully uninstalled absl-py-0.12.0\n",
      "Step #4 - \"Build TFX Image\":   Attempting uninstall: google-cloud-bigquery-storage\n",
      "Step #4 - \"Build TFX Image\":     Found existing installation: google-cloud-bigquery-storage 2.6.2\n",
      "Step #4 - \"Build TFX Image\":     Uninstalling google-cloud-bigquery-storage-2.6.2:\n",
      "Step #4 - \"Build TFX Image\":       Successfully uninstalled google-cloud-bigquery-storage-2.6.2\n",
      "Step #4 - \"Build TFX Image\":   Attempting uninstall: google-cloud-aiplatform\n",
      "Step #4 - \"Build TFX Image\":     Found existing installation: google-cloud-aiplatform 0.7.1\n",
      "Step #4 - \"Build TFX Image\":     Uninstalling google-cloud-aiplatform-0.7.1:\n",
      "Step #4 - \"Build TFX Image\":       Successfully uninstalled google-cloud-aiplatform-0.7.1\n",
      "Step #4 - \"Build TFX Image\": Successfully installed Deprecated-1.2.13 absl-py-0.11.0 cloudml-hypertune-0.1.0.dev6 docstring-parser-0.13 fire-0.4.0 google-cloud-aiplatform-1.4.2 google-cloud-bigquery-2.26.0 google-cloud-bigquery-storage-2.7.0 grpcio-1.44.0 iniconfig-1.1.1 kfp-1.8.1 kfp-pipeline-spec-0.1.14 kfp-server-api-1.8.1 pluggy-1.0.0 py-1.11.0 pytest-7.1.1 requests-toolbelt-0.9.1 strip-hints-0.1.10\n",
      "Step #4 - \"Build TFX Image\": ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "Step #4 - \"Build TFX Image\": tensorflow-io 0.18.0 requires tensorflow-io-gcs-filesystem==0.18.0, which is not installed.\n",
      "Step #4 - \"Build TFX Image\": tfx 1.2.0 requires google-cloud-aiplatform<0.8,>=0.5.0, but you have google-cloud-aiplatform 1.4.2 which is incompatible.\n",
      "Step #4 - \"Build TFX Image\": tfx 1.2.0 requires google-cloud-bigquery<2.21,>=1.28.0, but you have google-cloud-bigquery 2.26.0 which is incompatible.\n",
      "Step #4 - \"Build TFX Image\": tfx-bsl 1.2.0 requires google-cloud-bigquery<2.21,>=1.28.0, but you have google-cloud-bigquery 2.26.0 which is incompatible.\n",
      "Step #4 - \"Build TFX Image\": tensorflow 2.5.0 requires grpcio~=1.34.0, but you have grpcio 1.44.0 which is incompatible.\n",
      "Step #4 - \"Build TFX Image\": tensorflow-transform 1.2.0 requires google-cloud-bigquery<2.21,>=1.28.0, but you have google-cloud-bigquery 2.26.0 which is incompatible.\n",
      "Step #4 - \"Build TFX Image\": tensorflow-model-analysis 0.33.0 requires google-cloud-bigquery<2.21,>=1.28.0, but you have google-cloud-bigquery 2.26.0 which is incompatible.\n",
      "Step #4 - \"Build TFX Image\": tensorflow-data-validation 1.2.0 requires google-cloud-bigquery<2.21,>=1.28.0, but you have google-cloud-bigquery 2.26.0 which is incompatible.\n",
      "Step #4 - \"Build TFX Image\": WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "Step #4 - \"Build TFX Image\": WARNING: You are using pip version 21.2.4; however, version 22.0.4 is available.\n",
      "Step #4 - \"Build TFX Image\": You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\n",
      "Step #4 - \"Build TFX Image\": Removing intermediate container bbf93f770940\n",
      "Step #4 - \"Build TFX Image\":  ---> 5724f2de9db2\n",
      "Step #4 - \"Build TFX Image\": Step 4/5 : COPY src/ src/\n",
      "Step #4 - \"Build TFX Image\":  ---> 926696069013\n",
      "Step #4 - \"Build TFX Image\": Step 5/5 : ENV PYTHONPATH=\"/pipeline:${PYTHONPATH}\"\n",
      "Step #4 - \"Build TFX Image\":  ---> Running in 8d717ac903f1\n",
      "Step #4 - \"Build TFX Image\": Removing intermediate container 8d717ac903f1\n",
      "Step #4 - \"Build TFX Image\":  ---> 07b05d716427\n",
      "Step #4 - \"Build TFX Image\": Successfully built 07b05d716427\n",
      "Step #4 - \"Build TFX Image\": Successfully tagged gcr.io/grandelli-demo-295810/chicago-taxi-tips:tfx-1.2\n",
      "Finished Step #4 - \"Build TFX Image\"\n",
      "PUSH\n",
      "Pushing gcr.io/grandelli-demo-295810/chicago-taxi-tips:tfx-1.2\n",
      "The push refers to repository [gcr.io/grandelli-demo-295810/chicago-taxi-tips]\n",
      "40d4e620b519: Preparing\n",
      "d68d850c82bb: Preparing\n",
      "de6601a2c67a: Preparing\n",
      "d42c05f73feb: Preparing\n",
      "3119d30f29a9: Preparing\n",
      "121c9dfc7bce: Preparing\n",
      "868786b3710b: Preparing\n",
      "5bb1aa5df10d: Preparing\n",
      "f028010939aa: Preparing\n",
      "dc99c4ea3a81: Preparing\n",
      "37b508c5711b: Preparing\n",
      "756ab564e194: Preparing\n",
      "2ae86808a3d1: Preparing\n",
      "1dccbdf9b557: Preparing\n",
      "cfcbdbc2b748: Preparing\n",
      "937ab8f29c2e: Preparing\n",
      "5d417b2f7486: Preparing\n",
      "d6a297a3e6e4: Preparing\n",
      "6474a5e8117f: Preparing\n",
      "fe498124ed57: Preparing\n",
      "d5454704bb3d: Preparing\n",
      "fb896ef24b4b: Preparing\n",
      "5087113f67c8: Preparing\n",
      "2a92857a1d48: Preparing\n",
      "0ded97864c52: Preparing\n",
      "b50bbaac3e32: Preparing\n",
      "262ea1af4c10: Preparing\n",
      "b420a468ca49: Preparing\n",
      "608c205798d1: Preparing\n",
      "0760cd6d4269: Preparing\n",
      "fb4755c89c2a: Preparing\n",
      "22cfb9034da6: Preparing\n",
      "8bec4fbfce85: Preparing\n",
      "3b129ca3db46: Preparing\n",
      "64cb1a1930ab: Preparing\n",
      "600ef5a43f1f: Preparing\n",
      "8f8f0266f834: Preparing\n",
      "121c9dfc7bce: Waiting\n",
      "868786b3710b: Waiting\n",
      "5bb1aa5df10d: Waiting\n",
      "f028010939aa: Waiting\n",
      "dc99c4ea3a81: Waiting\n",
      "37b508c5711b: Waiting\n",
      "756ab564e194: Waiting\n",
      "2ae86808a3d1: Waiting\n",
      "1dccbdf9b557: Waiting\n",
      "cfcbdbc2b748: Waiting\n",
      "937ab8f29c2e: Waiting\n",
      "fb896ef24b4b: Waiting\n",
      "5087113f67c8: Waiting\n",
      "5d417b2f7486: Waiting\n",
      "2a92857a1d48: Waiting\n",
      "0760cd6d4269: Waiting\n",
      "d6a297a3e6e4: Waiting\n",
      "0ded97864c52: Waiting\n",
      "fb4755c89c2a: Waiting\n",
      "22cfb9034da6: Waiting\n",
      "6474a5e8117f: Waiting\n",
      "b50bbaac3e32: Waiting\n",
      "8bec4fbfce85: Waiting\n",
      "fe498124ed57: Waiting\n",
      "262ea1af4c10: Waiting\n",
      "3b129ca3db46: Waiting\n",
      "d5454704bb3d: Waiting\n",
      "64cb1a1930ab: Waiting\n",
      "b420a468ca49: Waiting\n",
      "8f8f0266f834: Waiting\n",
      "608c205798d1: Waiting\n",
      "3119d30f29a9: Layer already exists\n",
      "d42c05f73feb: Layer already exists\n",
      "121c9dfc7bce: Layer already exists\n",
      "868786b3710b: Layer already exists\n",
      "5bb1aa5df10d: Layer already exists\n",
      "f028010939aa: Layer already exists\n",
      "dc99c4ea3a81: Layer already exists\n",
      "37b508c5711b: Layer already exists\n",
      "756ab564e194: Layer already exists\n",
      "2ae86808a3d1: Layer already exists\n",
      "1dccbdf9b557: Layer already exists\n",
      "cfcbdbc2b748: Layer already exists\n",
      "937ab8f29c2e: Layer already exists\n",
      "d6a297a3e6e4: Layer already exists\n",
      "5d417b2f7486: Layer already exists\n",
      "fe498124ed57: Layer already exists\n",
      "6474a5e8117f: Layer already exists\n",
      "fb896ef24b4b: Layer already exists\n",
      "d5454704bb3d: Layer already exists\n",
      "5087113f67c8: Layer already exists\n",
      "2a92857a1d48: Layer already exists\n",
      "0ded97864c52: Layer already exists\n",
      "b50bbaac3e32: Layer already exists\n",
      "262ea1af4c10: Layer already exists\n",
      "b420a468ca49: Layer already exists\n",
      "608c205798d1: Layer already exists\n",
      "0760cd6d4269: Layer already exists\n",
      "fb4755c89c2a: Layer already exists\n",
      "22cfb9034da6: Layer already exists\n",
      "8bec4fbfce85: Layer already exists\n",
      "3b129ca3db46: Layer already exists\n",
      "64cb1a1930ab: Layer already exists\n",
      "40d4e620b519: Pushed\n",
      "600ef5a43f1f: Layer already exists\n",
      "de6601a2c67a: Pushed\n",
      "8f8f0266f834: Layer already exists\n",
      "d68d850c82bb: Pushed\n",
      "tfx-1.2: digest: sha256:bc14c3174452fb12cef8ea1c429969bec6ea14ca4033302961ce64bac879e0c1 size: 8102\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE  IMAGES                                                  STATUS\n",
      "13834e3f-3ac3-4096-9e01-cd058a1a30d0  2022-03-30T17:13:58+00:00  14M21S    -       gcr.io/grandelli-demo-295810/chicago-taxi-tips:tfx-1.2  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --no-source --timeout=60m --config build/pipeline-deployment.yaml --substitutions {SUBSTITUTIONS} --machine-type=e2-highcpu-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16028a3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "managed-notebooks.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:latest"
  },
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4004af63",
   "metadata": {},
   "source": [
    "# 07 - Prediction Serving\n",
    "\n",
    "The purpose of the notebook is to show how to use the deployed model for online and batch prediction.\n",
    "The notebook covers the following tasks:\n",
    "1. Test the endpoints for online prediction.\n",
    "2. Use the uploaded custom model for batch prediction.\n",
    "3. Run a the batch prediction pipeline using `Vertex Pipelines`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dad1f75",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d02a9d5",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7f3ce81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-31 14:14:12.429868: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e73bc25",
   "metadata": {},
   "source": [
    "### Setup Google Cloud project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29ea9b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID: grandelli-demo-295810\n",
      "Region: us-central1\n",
      "Bucket name: grandelli-demo-295810-partner-training-2022\n"
     ]
    }
   ],
   "source": [
    "PROJECT = 'grandelli-demo-295810' # Change to your project id.\n",
    "REGION = 'us-central1' # Change to your region.\n",
    "BUCKET = 'grandelli-demo-295810-partner-training-2022' # Change to your bucket name.\n",
    "\n",
    "if PROJECT == \"\" or PROJECT is None or PROJECT == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT = shell_output[0]\n",
    "    \n",
    "if BUCKET == \"\" or BUCKET is None or BUCKET == \"[your-bucket-name]\":\n",
    "    # Get your bucket name to GCP project id\n",
    "    BUCKET = PROJECT\n",
    "    # Try to create the bucket if it doesn't exists\n",
    "    ! gsutil mb -l $REGION gs://$BUCKET\n",
    "    print(\"\")\n",
    "    \n",
    "print(\"Project ID:\", PROJECT)\n",
    "print(\"Region:\", REGION)\n",
    "print(\"Bucket name:\", BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecba79b0",
   "metadata": {},
   "source": [
    "### Set configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "537732be",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = 'v01'\n",
    "DATASET_DISPLAY_NAME = 'chicago-taxi-tips'\n",
    "MODEL_DISPLAY_NAME = f'{DATASET_DISPLAY_NAME}-classifier-{VERSION}'\n",
    "ENDPOINT_DISPLAY_NAME = f'{DATASET_DISPLAY_NAME}-classifier'\n",
    "\n",
    "SERVE_BQ_DATASET_NAME = 'partner_training' # Change to your serving BigQuery dataset name.\n",
    "SERVE_BQ_TABLE_NAME = 'chicago_taxitrips_prep' # Change to your serving BigQuery table name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e508dd0",
   "metadata": {},
   "source": [
    "## 1. Making Online Predicitons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38be76f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.init(\n",
    "    project=PROJECT,\n",
    "    location=REGION,\n",
    "    staging_bucket=BUCKET\n",
    ")\n",
    "\n",
    "endpoint_name = vertex_ai.Endpoint.list(\n",
    "    filter=f'display_name={ENDPOINT_DISPLAY_NAME}', \n",
    "    order_by=\"update_time\")[-1].gca_resource.name\n",
    "\n",
    "endpoint = vertex_ai.Endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6b8053d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_instances = [  \n",
    "    {\n",
    "        \"dropoff_grid\": [\"POINT(-87.6 41.9)\"],\n",
    "        \"euclidean\": [2064.2696],\n",
    "        \"loc_cross\": [\"\"],\n",
    "        \"payment_type\": [\"Credit Card\"],\n",
    "        \"pickup_grid\": [\"POINT(-87.6 41.9)\"],\n",
    "        \"trip_miles\": [1.37],\n",
    "        \"trip_day\": [12],\n",
    "        \"trip_hour\": [16],\n",
    "        \"trip_month\": [2],\n",
    "        \"trip_day_of_week\": [4],\n",
    "        \"trip_seconds\": [555]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7cb447e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'scores': [0.341020465, 0.658979535], 'classes': ['tip<20%', 'tip>=20%']}\n"
     ]
    }
   ],
   "source": [
    "predictions = endpoint.predict(test_instances).predictions\n",
    "\n",
    "for prediction in predictions:\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc90ffa",
   "metadata": {},
   "source": [
    "## 2. Batch Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "046757e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKSPACE = f\"gs://{BUCKET}/{DATASET_DISPLAY_NAME}/\"\n",
    "SERVING_DATA_DIR = os.path.join(WORKSPACE, 'serving_data')\n",
    "SERVING_INPUT_DATA_DIR = os.path.join(SERVING_DATA_DIR, 'input_data')\n",
    "SERVING_OUTPUT_DATA_DIR = os.path.join(SERVING_DATA_DIR, 'output_predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e8fbc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing previous serving data...\n",
      "Creating serving data directory...\n",
      "Serving data directory is ready.\n"
     ]
    }
   ],
   "source": [
    "if tf.io.gfile.exists(SERVING_DATA_DIR):\n",
    "    print(\"Removing previous serving data...\")\n",
    "    tf.io.gfile.rmtree(SERVING_DATA_DIR)\n",
    "    \n",
    "print(\"Creating serving data directory...\")\n",
    "tf.io.gfile.mkdir(SERVING_DATA_DIR)\n",
    "print(\"Serving data directory is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7b60fa",
   "metadata": {},
   "source": [
    "### Extract serving data to Cloud Storage as JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04bb69ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.Sequence[~T]\n"
     ]
    }
   ],
   "source": [
    "from src.common import datasource_utils\n",
    "from src.preprocessing import etl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfd4cf91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    SELECT \n",
      "        IF(trip_month IS NULL, -1, trip_month) trip_month,\n",
      "        IF(trip_day IS NULL, -1, trip_day) trip_day,\n",
      "        IF(trip_day_of_week IS NULL, -1, trip_day_of_week) trip_day_of_week,\n",
      "        IF(trip_hour IS NULL, -1, trip_hour) trip_hour,\n",
      "        IF(trip_seconds IS NULL, -1, trip_seconds) trip_seconds,\n",
      "        IF(trip_miles IS NULL, -1, trip_miles) trip_miles,\n",
      "        IF(payment_type IS NULL, 'NA', payment_type) payment_type,\n",
      "        IF(pickup_grid IS NULL, 'NA', pickup_grid) pickup_grid,\n",
      "        IF(dropoff_grid IS NULL, 'NA', dropoff_grid) dropoff_grid,\n",
      "        IF(euclidean IS NULL, -1, euclidean) euclidean,\n",
      "        IF(loc_cross IS NULL, 'NA', loc_cross) loc_cross\n",
      "    FROM partner_training.chicago_taxitrips_prep \n",
      "    LIMIT 10000\n"
     ]
    }
   ],
   "source": [
    "LIMIT = 10000\n",
    "\n",
    "sql_query = datasource_utils.get_serving_source_query(\n",
    "    bq_dataset_name=SERVE_BQ_DATASET_NAME, \n",
    "    bq_table_name=SERVE_BQ_TABLE_NAME,\n",
    "    limit=LIMIT\n",
    ")\n",
    "\n",
    "print(sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f5afb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = f\"extract-{DATASET_DISPLAY_NAME}-serving-{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "\n",
    "args = {\n",
    "    'job_name': job_name,\n",
    "    #'runner': 'DataflowRunner',\n",
    "    'sql_query': sql_query,\n",
    "    'exported_data_prefix': os.path.join(SERVING_INPUT_DATA_DIR, \"data-\"),\n",
    "    'temporary_dir': os.path.join(WORKSPACE, 'tmp'),\n",
    "    'gcs_location': os.path.join(WORKSPACE, 'bq_tmp'),\n",
    "    'project': PROJECT,\n",
    "    'region': REGION,\n",
    "    'setup_file': './setup.py'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "588e1949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction started...\n",
      "INFO:root:Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "          });\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery.py:2395: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "  temp_location = pcoll.pipeline.options.view_as(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:Discarding invalid overrides: {'sql_query': \"\\n    SELECT \\n        IF(trip_month IS NULL, -1, trip_month) trip_month,\\n        IF(trip_day IS NULL, -1, trip_day) trip_day,\\n        IF(trip_day_of_week IS NULL, -1, trip_day_of_week) trip_day_of_week,\\n        IF(trip_hour IS NULL, -1, trip_hour) trip_hour,\\n        IF(trip_seconds IS NULL, -1, trip_seconds) trip_seconds,\\n        IF(trip_miles IS NULL, -1, trip_miles) trip_miles,\\n        IF(payment_type IS NULL, 'NA', payment_type) payment_type,\\n        IF(pickup_grid IS NULL, 'NA', pickup_grid) pickup_grid,\\n        IF(dropoff_grid IS NULL, 'NA', dropoff_grid) dropoff_grid,\\n        IF(euclidean IS NULL, -1, euclidean) euclidean,\\n        IF(loc_cross IS NULL, 'NA', loc_cross) loc_cross\\n    FROM partner_training.chicago_taxitrips_prep \\n    LIMIT 10000\", 'exported_data_prefix': 'gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/serving_data/input_data/data-', 'temporary_dir': 'gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/tmp', 'gcs_location': 'gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/bq_tmp'}\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.34.0\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7f3ad7e6db00> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7f3ad7e6dc20> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7f3ad7e6f170> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7f3ad7e6f200> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7f3ad7e6f3b0> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7f3ad7e6f440> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7f3ad7e6f560> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7f3ad7e6f5f0> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7f3ad7e6f680> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7f3ad7e6f710> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7f3ad7e6f950> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7f3ad7e6f8c0> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7f3ad7e6f9e0> ====================\n",
      "INFO:apache_beam.runners.worker.statecache:Creating state cache with size 100\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7f3ad743ce10> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-Data-Read-Impulse_10)+(ref_AppliedPTransform_Read-Data-Read-Map-lambda-at-iobase-py-898-_11))+(Read Data/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(Read Data/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)\n",
      "INFO:apache_beam.internal.gcp.auth:Setting socket default timeout to 60 seconds.\n",
      "INFO:apache_beam.internal.gcp.auth:socket default timeout is 60.0 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0331 14:15:31.453326082       1 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "E0331 14:15:33.326186044       1 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:oauth2client.transport:Attempting refresh to obtain initial access_token\n",
      "INFO:oauth2client.client:Refreshing access_token\n",
      "INFO:apache_beam.io.gcp.bigquery_tools:Started BigQuery job: <JobReference\n",
      " location: 'US'\n",
      " projectId: 'grandelli-demo-295810'>\n",
      " bq show -j --format=prettyjson --project_id=grandelli-demo-295810 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0331 14:15:35.733631377       1 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "E0331 14:15:37.681826754       1 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:apache_beam.io.gcp.bigquery_tools:Using location 'US' from table <TableReference\n",
      " datasetId: 'partner_training'\n",
      " projectId: 'grandelli-demo-295810'\n",
      " tableId: 'chicago_taxitrips_prep'> referenced by query \n",
      "    SELECT \n",
      "        IF(trip_month IS NULL, -1, trip_month) trip_month,\n",
      "        IF(trip_day IS NULL, -1, trip_day) trip_day,\n",
      "        IF(trip_day_of_week IS NULL, -1, trip_day_of_week) trip_day_of_week,\n",
      "        IF(trip_hour IS NULL, -1, trip_hour) trip_hour,\n",
      "        IF(trip_seconds IS NULL, -1, trip_seconds) trip_seconds,\n",
      "        IF(trip_miles IS NULL, -1, trip_miles) trip_miles,\n",
      "        IF(payment_type IS NULL, 'NA', payment_type) payment_type,\n",
      "        IF(pickup_grid IS NULL, 'NA', pickup_grid) pickup_grid,\n",
      "        IF(dropoff_grid IS NULL, 'NA', dropoff_grid) dropoff_grid,\n",
      "        IF(euclidean IS NULL, -1, euclidean) euclidean,\n",
      "        IF(loc_cross IS NULL, 'NA', loc_cross) loc_cross\n",
      "    FROM partner_training.chicago_taxitrips_prep \n",
      "    LIMIT 10000\n",
      "WARNING:apache_beam.io.gcp.bigquery_tools:Dataset grandelli-demo-295810:beam_temp_dataset_963491d359884bd0964ec9dc6f854260 does not exist so we will create it as temporary with location=US\n",
      "INFO:apache_beam.io.gcp.bigquery_tools:Started BigQuery job: <JobReference\n",
      " jobId: 'beam_bq_job_QUERY_extractchicagotaxitipsserving20220331141530_b73ac554-2_1648736140_130'\n",
      " location: 'US'\n",
      " projectId: 'grandelli-demo-295810'>\n",
      " bq show -j --format=prettyjson --project_id=grandelli-demo-295810 beam_bq_job_QUERY_extractchicagotaxitipsserving20220331141530_b73ac554-2_1648736140_130\n",
      "INFO:root:Job status: RUNNING\n",
      "INFO:root:Job status: DONE\n",
      "INFO:apache_beam.io.gcp.bigquery_tools:Started BigQuery job: <JobReference\n",
      " jobId: 'beam_bq_job_EXPORT_extractchicagotaxitipsserving20220331141530_b73ac554-2_1648736146_758'\n",
      " location: 'US'\n",
      " projectId: 'grandelli-demo-295810'>\n",
      " bq show -j --format=prettyjson --project_id=grandelli-demo-295810 beam_bq_job_EXPORT_extractchicagotaxitipsserving20220331141530_b73ac554-2_1648736146_758\n",
      "INFO:root:Job status: RUNNING\n",
      "INFO:root:Job status: DONE\n",
      "INFO:apache_beam.io.gcp.gcsio:Starting the size estimation of the input\n",
      "INFO:apache_beam.io.gcp.gcsio:Finished listing 1 files in 0.06468939781188965 seconds.\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((ref_AppliedPTransform_Write-Data-Write-WriteImpl-DoOnce-Impulse_28)+(ref_AppliedPTransform_Write-Data-Write-WriteImpl-DoOnce-FlatMap-lambda-at-core-py-3222-_29))+(ref_AppliedPTransform_Write-Data-Write-WriteImpl-DoOnce-Map-decode-_31))+(ref_AppliedPTransform_Write-Data-Write-WriteImpl-InitializeWrite_32))+(ref_PCollection_PCollection_17/Write))+(ref_PCollection_PCollection_18/Write)\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((((((ref_PCollection_PCollection_6_split/Read)+(Read Data/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Read-Data-_PassThroughThenCleanup-ParDo-PassThrough-ParDo-PassThrough-_16))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_Parse-Data_23))+(ref_AppliedPTransform_Write-Data-Write-WriteImpl-WindowInto-WindowIntoFn-_33))+(ref_AppliedPTransform_Write-Data-Write-WriteImpl-WriteBundles_34))+(ref_AppliedPTransform_Write-Data-Write-WriteImpl-Pair_35))+(Write Data/Write/WriteImpl/GroupByKey/Write)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0331 14:15:53.228782743       1 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((Write Data/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_Write-Data-Write-WriteImpl-Extract_37))+(ref_PCollection_PCollection_23/Write)\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((ref_PCollection_PCollection_17/Read)+(ref_AppliedPTransform_Write-Data-Write-WriteImpl-PreFinalize_38))+(ref_PCollection_PCollection_24/Write)\n",
      "INFO:apache_beam.io.gcp.gcsio:Starting the size estimation of the input\n",
      "INFO:apache_beam.io.gcp.gcsio:Finished listing 0 files in 0.051361799240112305 seconds.\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (ref_PCollection_PCollection_17/Read)+(ref_AppliedPTransform_Write-Data-Write-WriteImpl-FinalizeWrite_39)\n",
      "INFO:apache_beam.io.gcp.gcsio:Starting the size estimation of the input\n",
      "INFO:apache_beam.io.gcp.gcsio:Finished listing 1 files in 0.05796337127685547 seconds.\n",
      "INFO:apache_beam.io.gcp.gcsio:Starting the size estimation of the input\n",
      "INFO:apache_beam.io.gcp.gcsio:Finished listing 0 files in 0.05301308631896973 seconds.\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.40 seconds.\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_Read-Data-FilesToRemoveImpulse-Impulse_4)+(ref_AppliedPTransform_Read-Data-FilesToRemoveImpulse-FlatMap-lambda-at-core-py-3222-_5))+(ref_AppliedPTransform_Read-Data-FilesToRemoveImpulse-Map-decode-_7))+(ref_AppliedPTransform_Read-Data-MapFilesToRemove_8))+(ref_PCollection_PCollection_4/Write)\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((ref_AppliedPTransform_Read-Data-_PassThroughThenCleanup-Create-Impulse_18)+(ref_AppliedPTransform_Read-Data-_PassThroughThenCleanup-Create-FlatMap-lambda-at-core-py-3222-_19))+(ref_AppliedPTransform_Read-Data-_PassThroughThenCleanup-Create-Map-decode-_21))+(ref_AppliedPTransform_Read-Data-_PassThroughThenCleanup-ParDo-RemoveExtractedFiles-_22)\n",
      "INFO:apache_beam.io.gcp.gcsio:Starting the size estimation of the input\n",
      "INFO:apache_beam.io.gcp.gcsio:Finished listing 1 files in 0.05138111114501953 seconds.\n",
      "Data extraction completed.\n"
     ]
    }
   ],
   "source": [
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "print(\"Data extraction started...\")\n",
    "etl.run_extract_pipeline(args)\n",
    "print(\"Data extraction completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a036944a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0331 14:15:55.916792057       1 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/serving_data/input_data/data--00000-of-00001.jsonl\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls {SERVING_INPUT_DATA_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff0d72b",
   "metadata": {},
   "source": [
    "### Submit the batch prediction job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb72b16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0331 14:16:42.979925125       1 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "E0331 14:16:44.990810410       1 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    }
   ],
   "source": [
    "model_name =  vertex_ai.Model.list(\n",
    "    filter=f'display_name={MODEL_DISPLAY_NAME}',\n",
    "    order_by=\"update_time\")[-1].gca_resource.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dac58bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0331 14:17:55.324134545       1 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:Creating BatchPredictionJob\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob created. Resource name: projects/155283586619/locations/us-central1/batchPredictionJobs/1479595755067932672\n",
      "INFO:google.cloud.aiplatform.jobs:To use this BatchPredictionJob in another session:\n",
      "INFO:google.cloud.aiplatform.jobs:bpj = aiplatform.BatchPredictionJob('projects/155283586619/locations/us-central1/batchPredictionJobs/1479595755067932672')\n",
      "INFO:google.cloud.aiplatform.jobs:View Batch Prediction Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/batch-predictions/1479595755067932672?project=155283586619\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/155283586619/locations/us-central1/batchPredictionJobs/1479595755067932672 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/155283586619/locations/us-central1/batchPredictionJobs/1479595755067932672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/155283586619/locations/us-central1/batchPredictionJobs/1479595755067932672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/155283586619/locations/us-central1/batchPredictionJobs/1479595755067932672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/155283586619/locations/us-central1/batchPredictionJobs/1479595755067932672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/155283586619/locations/us-central1/batchPredictionJobs/1479595755067932672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/155283586619/locations/us-central1/batchPredictionJobs/1479595755067932672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/155283586619/locations/us-central1/batchPredictionJobs/1479595755067932672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/155283586619/locations/us-central1/batchPredictionJobs/1479595755067932672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/155283586619/locations/us-central1/batchPredictionJobs/1479595755067932672 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/155283586619/locations/us-central1/batchPredictionJobs/1479595755067932672 current state:\n",
      "JobState.JOB_STATE_SUCCEEDED\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob run completed. Resource name: projects/155283586619/locations/us-central1/batchPredictionJobs/1479595755067932672\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.jobs.BatchPredictionJob object at 0x7f3ad7451610> \n",
       "resource name: projects/155283586619/locations/us-central1/batchPredictionJobs/1479595755067932672"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_resources =  {\n",
    "    \"machine_type\": 'n1-standard-2',\n",
    "    #'accelerator_count': 1,\n",
    "    #'accelerator_type': 'NVIDIA_TESLA_T4'\n",
    "    \"starting_replica_count\": 1,\n",
    "    \"max_replica_count\": 10,\n",
    "}\n",
    "\n",
    "job_display_name = f\"{MODEL_DISPLAY_NAME}-prediction-job-{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "\n",
    "vertex_ai.BatchPredictionJob.create(\n",
    "    job_display_name=job_display_name,\n",
    "    model_name=model_name,\n",
    "    gcs_source=SERVING_INPUT_DATA_DIR + '/*.jsonl',\n",
    "    gcs_destination_prefix=SERVING_OUTPUT_DATA_DIR,\n",
    "    instances_format='jsonl',\n",
    "    predictions_format='jsonl',\n",
    "    sync=True,\n",
    "    **job_resources,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f281a9",
   "metadata": {},
   "source": [
    "## 3. Run the batch prediction pipeline using Vertex Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "809ba028",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKSPACE = f\"gs://{BUCKET}/{DATASET_DISPLAY_NAME}/\"\n",
    "ARTIFACT_STORE = os.path.join(WORKSPACE, 'tfx_artifacts')\n",
    "PIPELINE_NAME = f'{MODEL_DISPLAY_NAME}-predict-pipeline'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769a1d9e",
   "metadata": {},
   "source": [
    "### Set the pipeline configurations for the Vertex AI run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c5add19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"GCS_LOCATION\"] = f\"gs://{BUCKET}/{DATASET_DISPLAY_NAME}\"\n",
    "os.environ[\"MODEL_DISPLAY_NAME\"] = MODEL_DISPLAY_NAME\n",
    "os.environ[\"PIPELINE_NAME\"] = PIPELINE_NAME\n",
    "os.environ[\"ARTIFACT_STORE_URI\"] = ARTIFACT_STORE\n",
    "os.environ[\"BATCH_PREDICTION_BQ_DATASET_NAME\"] = SERVE_BQ_DATASET_NAME\n",
    "os.environ[\"BATCH_PREDICTION_BQ_TABLE_NAME\"] = SERVE_BQ_TABLE_NAME\n",
    "os.environ[\"SERVE_LIMIT\"] = \"1000\"\n",
    "os.environ[\"BEAM_RUNNER\"] = \"DirectRunner\"\n",
    "os.environ[\"TFX_IMAGE_URI\"] = f\"gcr.io/{PROJECT}/{DATASET_DISPLAY_NAME}:{VERSION}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f6d0e2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT: grandelli-demo-295810\n",
      "REGION: us-central1\n",
      "GCS_LOCATION: gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips\n",
      "ARTIFACT_STORE_URI: gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/tfx_artifacts\n",
      "MODEL_REGISTRY_URI: gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/model_registry\n",
      "DATASET_DISPLAY_NAME: chicago-taxi-tips\n",
      "MODEL_DISPLAY_NAME: chicago-taxi-tips-classifier-v01\n",
      "PIPELINE_NAME: chicago-taxi-tips-classifier-v01-predict-pipeline\n",
      "ML_USE_COLUMN: ml_use\n",
      "EXCLUDE_COLUMNS: trip_start_timestamp\n",
      "TRAIN_LIMIT: 0\n",
      "TEST_LIMIT: 0\n",
      "SERVE_LIMIT: 1000\n",
      "NUM_TRAIN_SPLITS: 4\n",
      "NUM_EVAL_SPLITS: 1\n",
      "ACCURACY_THRESHOLD: 0.8\n",
      "USE_KFP_SA: False\n",
      "TFX_IMAGE_URI: gcr.io/grandelli-demo-295810/chicago-taxi-tips:v01\n",
      "BEAM_RUNNER: DirectRunner\n",
      "BEAM_DIRECT_PIPELINE_ARGS: ['--project=grandelli-demo-295810', '--temp_location=gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/temp']\n",
      "BEAM_DATAFLOW_PIPELINE_ARGS: ['--project=grandelli-demo-295810', '--temp_location=gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/temp', '--region=us-central1', '--runner=DirectRunner']\n",
      "TRAINING_RUNNER: local\n",
      "VERTEX_TRAINING_ARGS: {'project': 'grandelli-demo-295810', 'worker_pool_specs': [{'machine_spec': {'machine_type': 'n1-standard-4'}, 'replica_count': 1, 'container_spec': {'image_uri': 'gcr.io/grandelli-demo-295810/chicago-taxi-tips:v01'}}]}\n",
      "VERTEX_TRAINING_CONFIG: {'ai_platform_training_enable_ucaip': True, 'ai_platform_training_ucaip_region': 'us-central1', 'ai_platform_training_args': {'project': 'grandelli-demo-295810', 'worker_pool_specs': [{'machine_spec': {'machine_type': 'n1-standard-4'}, 'replica_count': 1, 'container_spec': {'image_uri': 'gcr.io/grandelli-demo-295810/chicago-taxi-tips:v01'}}]}, 'use_gpu': False}\n",
      "SERVING_RUNTIME: tf2-cpu.2-5\n",
      "SERVING_IMAGE_URI: us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-5:latest\n",
      "BATCH_PREDICTION_BQ_DATASET_NAME: partner_training\n",
      "BATCH_PREDICTION_BQ_TABLE_NAME: chicago_taxitrips_prep\n",
      "BATCH_PREDICTION_BEAM_ARGS: {'runner': 'DirectRunner', 'temporary_dir': 'gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/temp', 'gcs_location': 'gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/temp', 'project': 'grandelli-demo-295810', 'region': 'us-central1', 'setup_file': './setup.py'}\n",
      "BATCH_PREDICTION_JOB_RESOURCES: {'machine_type': 'n1-standard-2', 'starting_replica_count': 1, 'max_replica_count': 10}\n",
      "DATASTORE_PREDICTION_KIND: chicago-taxi-tips-classifier-v01-predictions\n",
      "ENABLE_CACHE: 0\n",
      "UPLOAD_MODEL: 1\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "from src.tfx_pipelines import config\n",
    "importlib.reload(config)\n",
    "\n",
    "for key, value in config.__dict__.items():\n",
    "    if key.isupper(): print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f128b46e",
   "metadata": {},
   "source": [
    "### (Optional) Build the ML container image\n",
    "\n",
    "This is the `TFX` runtime environment for the training pipeline steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f24fa5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0331 15:43:33.120220901       1 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcr.io/grandelli-demo-295810/chicago-taxi-tips:v01\n"
     ]
    }
   ],
   "source": [
    "!echo $TFX_IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3949cc7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0331 15:43:34.423646334       1 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 59 file(s) totalling 2.6 MiB before compression.\n",
      "Some files were not included in the source upload.\n",
      "\n",
      "Check the gcloud log [/home/jupyter/.config/gcloud/logs/2022.03.31/15.43.37.124845.log] to see which files and the contents of the\n",
      "default gcloudignore file used (see `$ gcloud topic gcloudignore` to learn\n",
      "more).\n",
      "\n",
      "Uploading tarball of [.] to [gs://grandelli-demo-295810_cloudbuild/source/1648741417.350531-82bcd07ab4574fffa6dcccfd26885958.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/grandelli-demo-295810/locations/global/builds/5c369433-6f9f-49a1-af60-33d9cfd1eb22].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/5c369433-6f9f-49a1-af60-33d9cfd1eb22?project=155283586619].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"5c369433-6f9f-49a1-af60-33d9cfd1eb22\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://grandelli-demo-295810_cloudbuild/source/1648741417.350531-82bcd07ab4574fffa6dcccfd26885958.tgz#1648741417885033\n",
      "Copying gs://grandelli-demo-295810_cloudbuild/source/1648741417.350531-82bcd07ab4574fffa6dcccfd26885958.tgz#1648741417885033...\n",
      "/ [1 files][344.1 KiB/344.1 KiB]                                                \n",
      "Operation completed over 1 objects/344.1 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  2.754MB\n",
      "Step 1/5 : FROM gcr.io/tfx-oss-public/tfx:1.2.0\n",
      "1.2.0: Pulling from tfx-oss-public/tfx\n",
      "25fa05cd42bd: Pulling fs layer\n",
      "2d6e353a95ec: Pulling fs layer\n",
      "14d7996407de: Pulling fs layer\n",
      "0c9c6fc70f16: Pulling fs layer\n",
      "c3c76be11512: Pulling fs layer\n",
      "ab6e5a9c78ee: Pulling fs layer\n",
      "7bc1690abd59: Pulling fs layer\n",
      "f5b4dd7682bc: Pulling fs layer\n",
      "d6897660f71d: Pulling fs layer\n",
      "174d792fb622: Pulling fs layer\n",
      "5f8143275aca: Pulling fs layer\n",
      "56646f115483: Pulling fs layer\n",
      "798922b52524: Pulling fs layer\n",
      "e2699a9f592b: Pulling fs layer\n",
      "f43e7d1c07e4: Pulling fs layer\n",
      "1e71d5e9923d: Pulling fs layer\n",
      "bf6ae2a2e250: Pulling fs layer\n",
      "e49679b748d5: Pulling fs layer\n",
      "80208bd6f7fb: Pulling fs layer\n",
      "b83c16bef138: Pulling fs layer\n",
      "9d1427033824: Pulling fs layer\n",
      "c0028679f003: Pulling fs layer\n",
      "09c222e7ff04: Pulling fs layer\n",
      "ae6048a3aec1: Pulling fs layer\n",
      "1ced637de50b: Pulling fs layer\n",
      "762ff1eb7f16: Pulling fs layer\n",
      "f6f8f4265c8c: Pulling fs layer\n",
      "595b1c49222a: Pulling fs layer\n",
      "b6c7eb38f366: Pulling fs layer\n",
      "520be5017b4d: Pulling fs layer\n",
      "0a1aacb7e387: Pulling fs layer\n",
      "8f605134caf9: Pulling fs layer\n",
      "189ba322d030: Pulling fs layer\n",
      "af92447b9198: Pulling fs layer\n",
      "f43e7d1c07e4: Waiting\n",
      "1e71d5e9923d: Waiting\n",
      "bf6ae2a2e250: Waiting\n",
      "e49679b748d5: Waiting\n",
      "80208bd6f7fb: Waiting\n",
      "b83c16bef138: Waiting\n",
      "9d1427033824: Waiting\n",
      "c0028679f003: Waiting\n",
      "09c222e7ff04: Waiting\n",
      "ae6048a3aec1: Waiting\n",
      "1ced637de50b: Waiting\n",
      "762ff1eb7f16: Waiting\n",
      "f6f8f4265c8c: Waiting\n",
      "595b1c49222a: Waiting\n",
      "b6c7eb38f366: Waiting\n",
      "520be5017b4d: Waiting\n",
      "0a1aacb7e387: Waiting\n",
      "8f605134caf9: Waiting\n",
      "189ba322d030: Waiting\n",
      "af92447b9198: Waiting\n",
      "d6897660f71d: Waiting\n",
      "0c9c6fc70f16: Waiting\n",
      "c3c76be11512: Waiting\n",
      "ab6e5a9c78ee: Waiting\n",
      "7bc1690abd59: Waiting\n",
      "f5b4dd7682bc: Waiting\n",
      "174d792fb622: Waiting\n",
      "5f8143275aca: Waiting\n",
      "56646f115483: Waiting\n",
      "e2699a9f592b: Waiting\n",
      "798922b52524: Waiting\n",
      "2d6e353a95ec: Verifying Checksum\n",
      "2d6e353a95ec: Download complete\n",
      "0c9c6fc70f16: Verifying Checksum\n",
      "0c9c6fc70f16: Download complete\n",
      "14d7996407de: Verifying Checksum\n",
      "14d7996407de: Download complete\n",
      "25fa05cd42bd: Verifying Checksum\n",
      "25fa05cd42bd: Download complete\n",
      "c3c76be11512: Verifying Checksum\n",
      "c3c76be11512: Download complete\n",
      "7bc1690abd59: Verifying Checksum\n",
      "d6897660f71d: Verifying Checksum\n",
      "d6897660f71d: Download complete\n",
      "25fa05cd42bd: Pull complete\n",
      "2d6e353a95ec: Pull complete\n",
      "14d7996407de: Pull complete\n",
      "0c9c6fc70f16: Pull complete\n",
      "c3c76be11512: Pull complete\n",
      "f5b4dd7682bc: Download complete\n",
      "5f8143275aca: Verifying Checksum\n",
      "5f8143275aca: Download complete\n",
      "174d792fb622: Verifying Checksum\n",
      "174d792fb622: Download complete\n",
      "ab6e5a9c78ee: Download complete\n",
      "e2699a9f592b: Verifying Checksum\n",
      "e2699a9f592b: Download complete\n",
      "798922b52524: Verifying Checksum\n",
      "798922b52524: Download complete\n",
      "f43e7d1c07e4: Verifying Checksum\n",
      "f43e7d1c07e4: Download complete\n",
      "bf6ae2a2e250: Verifying Checksum\n",
      "bf6ae2a2e250: Download complete\n",
      "e49679b748d5: Download complete\n",
      "80208bd6f7fb: Download complete\n",
      "b83c16bef138: Verifying Checksum\n",
      "b83c16bef138: Download complete\n",
      "9d1427033824: Verifying Checksum\n",
      "9d1427033824: Download complete\n",
      "c0028679f003: Verifying Checksum\n",
      "c0028679f003: Download complete\n",
      "09c222e7ff04: Verifying Checksum\n",
      "09c222e7ff04: Download complete\n",
      "ae6048a3aec1: Download complete\n",
      "56646f115483: Verifying Checksum\n",
      "56646f115483: Download complete\n",
      "1e71d5e9923d: Verifying Checksum\n",
      "1e71d5e9923d: Download complete\n",
      "f6f8f4265c8c: Download complete\n",
      "1ced637de50b: Verifying Checksum\n",
      "1ced637de50b: Download complete\n",
      "b6c7eb38f366: Verifying Checksum\n",
      "b6c7eb38f366: Download complete\n",
      "520be5017b4d: Verifying Checksum\n",
      "520be5017b4d: Download complete\n",
      "0a1aacb7e387: Verifying Checksum\n",
      "0a1aacb7e387: Download complete\n",
      "8f605134caf9: Verifying Checksum\n",
      "8f605134caf9: Download complete\n",
      "189ba322d030: Download complete\n",
      "af92447b9198: Verifying Checksum\n",
      "af92447b9198: Download complete\n",
      "595b1c49222a: Verifying Checksum\n",
      "595b1c49222a: Download complete\n",
      "762ff1eb7f16: Verifying Checksum\n",
      "762ff1eb7f16: Download complete\n",
      "ab6e5a9c78ee: Pull complete\n",
      "7bc1690abd59: Pull complete\n",
      "f5b4dd7682bc: Pull complete\n",
      "d6897660f71d: Pull complete\n",
      "174d792fb622: Pull complete\n",
      "5f8143275aca: Pull complete\n",
      "56646f115483: Pull complete\n",
      "798922b52524: Pull complete\n",
      "e2699a9f592b: Pull complete\n",
      "f43e7d1c07e4: Pull complete\n",
      "1e71d5e9923d: Pull complete\n",
      "bf6ae2a2e250: Pull complete\n",
      "e49679b748d5: Pull complete\n",
      "80208bd6f7fb: Pull complete\n",
      "b83c16bef138: Pull complete\n",
      "9d1427033824: Pull complete\n",
      "c0028679f003: Pull complete\n",
      "09c222e7ff04: Pull complete\n",
      "ae6048a3aec1: Pull complete\n",
      "1ced637de50b: Pull complete\n",
      "762ff1eb7f16: Pull complete\n",
      "f6f8f4265c8c: Pull complete\n",
      "595b1c49222a: Pull complete\n",
      "b6c7eb38f366: Pull complete\n",
      "520be5017b4d: Pull complete\n",
      "0a1aacb7e387: Pull complete\n",
      "8f605134caf9: Pull complete\n",
      "189ba322d030: Pull complete\n",
      "af92447b9198: Pull complete\n",
      "Digest: sha256:eba9e7b7d9131eb5b05434feaafc4676268ad805e4b97218f58994ad2714be67\n",
      "Status: Downloaded newer image for gcr.io/tfx-oss-public/tfx:1.2.0\n",
      " ---> 0e86fadcc60c\n",
      "Step 2/5 : COPY requirements.txt requirements.txt\n",
      " ---> 8de37f6e2cbb\n",
      "Step 3/5 : RUN pip install -r requirements.txt\n",
      " ---> Running in 04f06c7ed872\n",
      "Collecting kfp==1.8.9\n",
      "  Downloading kfp-1.8.9.tar.gz (296 kB)\n",
      "Collecting google-cloud-bigquery==2.26.0\n",
      "  Downloading google_cloud_bigquery-2.26.0-py2.py3-none-any.whl (201 kB)\n",
      "Collecting google-cloud-bigquery-storage==2.7.0\n",
      "  Downloading google_cloud_bigquery_storage-2.7.0-py2.py3-none-any.whl (125 kB)\n",
      "Collecting google-cloud-aiplatform==1.4.3\n",
      "  Downloading google_cloud_aiplatform-1.4.3-py2.py3-none-any.whl (1.4 MB)\n",
      "Collecting cloudml-hypertune==0.1.0.dev6\n",
      "  Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "Collecting apache-beam==2.34.0\n",
      "  Downloading apache_beam-2.34.0-cp37-cp37m-manylinux2010_x86_64.whl (9.8 MB)\n",
      "Collecting pytest\n",
      "  Downloading pytest-7.1.1-py3-none-any.whl (297 kB)\n",
      "Collecting absl-py<=0.11,>=0.9\n",
      "  Downloading absl_py-0.11.0-py3-none-any.whl (127 kB)\n",
      "Requirement already satisfied: PyYAML<6,>=5.3 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.9->-r requirements.txt (line 1)) (5.4.1)\n",
      "Requirement already satisfied: google-cloud-storage<2,>=1.20.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.9->-r requirements.txt (line 1)) (1.41.1)\n",
      "Requirement already satisfied: kubernetes<19,>=8.0.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.9->-r requirements.txt (line 1)) (12.0.1)\n",
      "Requirement already satisfied: google-api-python-client<2,>=1.7.8 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.9->-r requirements.txt (line 1)) (1.12.8)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.9->-r requirements.txt (line 1)) (1.34.0)\n",
      "Collecting requests-toolbelt<1,>=0.8.0\n",
      "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "Collecting cloudpickle<3,>=2.0.0\n",
      "  Downloading cloudpickle-2.0.0-py3-none-any.whl (25 kB)\n",
      "Collecting kfp-server-api<2.0.0,>=1.1.2\n",
      "  Downloading kfp-server-api-1.8.1.tar.gz (54 kB)\n",
      "Requirement already satisfied: jsonschema<4,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.9->-r requirements.txt (line 1)) (3.2.0)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.9->-r requirements.txt (line 1)) (0.8.9)\n",
      "Requirement already satisfied: click<9,>=7.1.2 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.9->-r requirements.txt (line 1)) (7.1.2)\n",
      "Collecting Deprecated<2,>=1.2.7\n",
      "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting strip-hints<1,>=0.1.8\n",
      "  Downloading strip-hints-0.1.10.tar.gz (29 kB)\n",
      "Collecting docstring-parser<1,>=0.7.3\n",
      "  Downloading docstring_parser-0.13.tar.gz (23 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Collecting kfp-pipeline-spec<0.2.0,>=0.1.13\n",
      "  Downloading kfp_pipeline_spec-0.1.14-py3-none-any.whl (18 kB)\n",
      "Collecting fire<1,>=0.3.1\n",
      "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
      "Requirement already satisfied: protobuf<4,>=3.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.9->-r requirements.txt (line 1)) (3.16.0)\n",
      "Requirement already satisfied: uritemplate<4,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.9->-r requirements.txt (line 1)) (3.0.1)\n",
      "Requirement already satisfied: pydantic<2,>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.9->-r requirements.txt (line 1)) (1.8.2)\n",
      "Collecting typer<1.0,>=0.3.2\n",
      "  Downloading typer-0.4.1-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: typing-extensions<4,>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.9->-r requirements.txt (line 1)) (3.7.4.3)\n",
      "Requirement already satisfied: proto-plus>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.26.0->-r requirements.txt (line 2)) (1.19.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.26.0->-r requirements.txt (line 2)) (2.25.1)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.26.0->-r requirements.txt (line 2)) (1.7.2)\n",
      "Requirement already satisfied: google-api-core[grpc]<3.0.0dev,>=1.29.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.26.0->-r requirements.txt (line 2)) (1.31.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.26.0->-r requirements.txt (line 2)) (1.3.2)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.26.0->-r requirements.txt (line 2)) (20.9)\n",
      "Collecting grpcio<2.0dev,>=1.38.1\n",
      "  Downloading grpcio-1.44.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
      "Requirement already satisfied: libcst>=0.2.5 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery-storage==2.7.0->-r requirements.txt (line 3)) (0.3.19)\n",
      "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam==2.34.0->-r requirements.txt (line 6)) (3.12.0)\n",
      "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam==2.34.0->-r requirements.txt (line 6)) (2.6.0)\n",
      "Requirement already satisfied: crcmod<2.0,>=1.7 in /opt/conda/lib/python3.7/site-packages (from apache-beam==2.34.0->-r requirements.txt (line 6)) (1.7)\n",
      "Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam==2.34.0->-r requirements.txt (line 6)) (0.3.1.1)\n",
      "Requirement already satisfied: numpy<1.21.0,>=1.14.3 in /opt/conda/lib/python3.7/site-packages (from apache-beam==2.34.0->-r requirements.txt (line 6)) (1.19.5)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam==2.34.0->-r requirements.txt (line 6)) (2.8.2)\n",
      "Requirement already satisfied: pydot<2,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam==2.34.0->-r requirements.txt (line 6)) (1.4.2)\n",
      "Collecting orjson<4.0\n",
      "  Downloading orjson-3.6.7-cp37-cp37m-manylinux_2_24_x86_64.whl (255 kB)\n",
      "Requirement already satisfied: avro-python3!=1.9.2,<1.10.0,>=1.8.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam==2.34.0->-r requirements.txt (line 6)) (1.9.2.1)\n",
      "Requirement already satisfied: httplib2<0.20.0,>=0.8 in /opt/conda/lib/python3.7/site-packages (from apache-beam==2.34.0->-r requirements.txt (line 6)) (0.19.1)\n",
      "Requirement already satisfied: oauth2client<5,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam==2.34.0->-r requirements.txt (line 6)) (4.1.3)\n",
      "Requirement already satisfied: fastavro<2,>=0.21.4 in /opt/conda/lib/python3.7/site-packages (from apache-beam==2.34.0->-r requirements.txt (line 6)) (1.4.4)\n",
      "Requirement already satisfied: pyarrow<6.0.0,>=0.15.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam==2.34.0->-r requirements.txt (line 6)) (2.0.0)\n",
      "Requirement already satisfied: pytz>=2018.3 in /opt/conda/lib/python3.7/site-packages (from apache-beam==2.34.0->-r requirements.txt (line 6)) (2021.1)\n",
      "Requirement already satisfied: future<1.0.0,>=0.18.2 in /opt/conda/lib/python3.7/site-packages (from apache-beam==2.34.0->-r requirements.txt (line 6)) (0.18.2)\n",
      "Collecting py>=1.8.2\n",
      "  Downloading py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
      "Collecting iniconfig\n",
      "  Downloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from pytest->-r requirements.txt (line 7)) (1.1.0)\n",
      "Collecting pluggy<2.0,>=0.12\n",
      "  Downloading pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.7/site-packages (from pytest->-r requirements.txt (line 7)) (20.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.12 in /opt/conda/lib/python3.7/site-packages (from pytest->-r requirements.txt (line 7)) (4.6.3)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from absl-py<=0.11,>=0.9->kfp==1.8.9->-r requirements.txt (line 1)) (1.15.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated<2,>=1.2.7->kfp==1.8.9->-r requirements.txt (line 1)) (1.12.1)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.7/site-packages (from fire<1,>=0.3.1->kfp==1.8.9->-r requirements.txt (line 1)) (1.1.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.29.0->google-cloud-bigquery==2.26.0->-r requirements.txt (line 2)) (49.6.0.post20210108)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.29.0->google-cloud-bigquery==2.26.0->-r requirements.txt (line 2)) (1.53.0)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp==1.8.9->-r requirements.txt (line 1)) (0.1.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.8.9->-r requirements.txt (line 1)) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.8.9->-r requirements.txt (line 1)) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.8.9->-r requirements.txt (line 1)) (0.2.7)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery==2.26.0->-r requirements.txt (line 2)) (1.1.2)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery==2.26.0->-r requirements.txt (line 2)) (1.14.6)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery==2.26.0->-r requirements.txt (line 2)) (2.20)\n",
      "Requirement already satisfied: docopt in /opt/conda/lib/python3.7/site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam==2.34.0->-r requirements.txt (line 6)) (0.6.2)\n",
      "Requirement already satisfied: pyparsing<3,>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from httplib2<0.20.0,>=0.8->apache-beam==2.34.0->-r requirements.txt (line 6)) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.12->pytest->-r requirements.txt (line 7)) (3.5.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp==1.8.9->-r requirements.txt (line 1)) (0.17.3)\n",
      "Requirement already satisfied: urllib3>=1.15 in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.8.9->-r requirements.txt (line 1)) (1.26.6)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.8.9->-r requirements.txt (line 1)) (2021.5.30)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<19,>=8.0.0->kfp==1.8.9->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<19,>=8.0.0->kfp==1.8.9->-r requirements.txt (line 1)) (0.57.0)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from libcst>=0.2.5->google-cloud-bigquery-storage==2.7.0->-r requirements.txt (line 3)) (0.7.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /opt/conda/lib/python3.7/site-packages (from oauth2client<5,>=2.0.1->apache-beam==2.34.0->-r requirements.txt (line 6)) (0.4.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery==2.26.0->-r requirements.txt (line 2)) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery==2.26.0->-r requirements.txt (line 2)) (4.0.0)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints<1,>=0.1.8->kfp==1.8.9->-r requirements.txt (line 1)) (0.36.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from typing-inspect>=0.4.0->libcst>=0.2.5->google-cloud-bigquery-storage==2.7.0->-r requirements.txt (line 3)) (0.4.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<19,>=8.0.0->kfp==1.8.9->-r requirements.txt (line 1)) (3.1.1)\n",
      "Building wheels for collected packages: kfp, cloudml-hypertune, docstring-parser, fire, kfp-server-api, strip-hints\n",
      "  Building wheel for kfp (setup.py): started\n",
      "  Building wheel for kfp (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp: filename=kfp-1.8.9-py3-none-any.whl size=409653 sha256=579dd229cefad19961f8e6ac678ac0630002e1bd7a7a079391eab4632061c996\n",
      "  Stored in directory: /root/.cache/pip/wheels/0e/20/7e/c2c43249eb0538c5aa2542bcc9b02affb0211ed5617fbd4abc\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3988 sha256=dd50c3d7dfe7caafcd8fe6f4da5a528a3107409cd9ac3e128f1e24626c5de7d4\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/ff/87/e7bed0c2741fe219b3d6da67c2431d7f7fedb183032e00f81e\n",
      "  Building wheel for docstring-parser (PEP 517): started\n",
      "  Building wheel for docstring-parser (PEP 517): finished with status 'done'\n",
      "  Created wheel for docstring-parser: filename=docstring_parser-0.13-py3-none-any.whl size=31866 sha256=afb459786d146e810a79486aa384153243c2d3d9f7fbc9a90b762386de0132d2\n",
      "  Stored in directory: /root/.cache/pip/wheels/bd/88/3c/d1aa049309f7945178cac9fbe6561a86424f432da57c18ca0f\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115928 sha256=7ce3fb65be613a933bc1d4a32364d54bd606ee9adb57d37b731de10015a3afc7\n",
      "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
      "  Building wheel for kfp-server-api (setup.py): started\n",
      "  Building wheel for kfp-server-api (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp-server-api: filename=kfp_server_api-1.8.1-py3-none-any.whl size=95548 sha256=99edb72a24a1011b33e7cc52ae9e1497d50a85a5be6285e8df290722e83805da\n",
      "  Stored in directory: /root/.cache/pip/wheels/f5/4e/2e/6795bd3ed456a43652e7de100aca275ec179c9a8dfbcc65626\n",
      "  Building wheel for strip-hints (setup.py): started\n",
      "  Building wheel for strip-hints (setup.py): finished with status 'done'\n",
      "  Created wheel for strip-hints: filename=strip_hints-0.1.10-py2.py3-none-any.whl size=22279 sha256=26d2c826a896215aadeebef8635f43e788a462937c6d2f80f577232c9eda9f68\n",
      "  Stored in directory: /root/.cache/pip/wheels/5e/14/c3/6e44e9b2545f2d570b03f5b6d38c00b7534aa8abb376978363\n",
      "Successfully built kfp cloudml-hypertune docstring-parser fire kfp-server-api strip-hints\n",
      "Installing collected packages: grpcio, typer, strip-hints, requests-toolbelt, py, pluggy, orjson, kfp-server-api, kfp-pipeline-spec, iniconfig, google-cloud-bigquery, fire, docstring-parser, Deprecated, cloudpickle, absl-py, pytest, kfp, google-cloud-bigquery-storage, google-cloud-aiplatform, cloudml-hypertune, apache-beam\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.34.1\n",
      "    Uninstalling grpcio-1.34.1:\n",
      "      Successfully uninstalled grpcio-1.34.1\n",
      "  Attempting uninstall: kfp-pipeline-spec\n",
      "    Found existing installation: kfp-pipeline-spec 0.1.9\n",
      "    Uninstalling kfp-pipeline-spec-0.1.9:\n",
      "      Successfully uninstalled kfp-pipeline-spec-0.1.9\n",
      "  Attempting uninstall: google-cloud-bigquery\n",
      "    Found existing installation: google-cloud-bigquery 2.20.0\n",
      "    Uninstalling google-cloud-bigquery-2.20.0:\n",
      "      Successfully uninstalled google-cloud-bigquery-2.20.0\n",
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 1.6.0\n",
      "    Uninstalling cloudpickle-1.6.0:\n",
      "      Successfully uninstalled cloudpickle-1.6.0\n",
      "  Attempting uninstall: absl-py\n",
      "    Found existing installation: absl-py 0.12.0\n",
      "    Uninstalling absl-py-0.12.0:\n",
      "      Successfully uninstalled absl-py-0.12.0\n",
      "  Attempting uninstall: google-cloud-bigquery-storage\n",
      "    Found existing installation: google-cloud-bigquery-storage 2.6.2\n",
      "    Uninstalling google-cloud-bigquery-storage-2.6.2:\n",
      "      Successfully uninstalled google-cloud-bigquery-storage-2.6.2\n",
      "  Attempting uninstall: google-cloud-aiplatform\n",
      "    Found existing installation: google-cloud-aiplatform 0.7.1\n",
      "    Uninstalling google-cloud-aiplatform-0.7.1:\n",
      "      Successfully uninstalled google-cloud-aiplatform-0.7.1\n",
      "  Attempting uninstall: apache-beam\n",
      "    Found existing installation: apache-beam 2.31.0\n",
      "    Uninstalling apache-beam-2.31.0:\n",
      "      Successfully uninstalled apache-beam-2.31.0\n",
      "Successfully installed Deprecated-1.2.13 absl-py-0.11.0 apache-beam-2.34.0 cloudml-hypertune-0.1.0.dev6 cloudpickle-2.0.0 docstring-parser-0.13 fire-0.4.0 google-cloud-aiplatform-1.4.3 google-cloud-bigquery-2.26.0 google-cloud-bigquery-storage-2.7.0 grpcio-1.44.0 iniconfig-1.1.1 kfp-1.8.9 kfp-pipeline-spec-0.1.14 kfp-server-api-1.8.1 orjson-3.6.7 pluggy-1.0.0 py-1.11.0 pytest-7.1.1 requests-toolbelt-0.9.1 strip-hints-0.1.10 typer-0.4.1\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-io 0.18.0 requires tensorflow-io-gcs-filesystem==0.18.0, which is not installed.\n",
      "tfx 1.2.0 requires google-cloud-aiplatform<0.8,>=0.5.0, but you have google-cloud-aiplatform 1.4.3 which is incompatible.\n",
      "tfx 1.2.0 requires google-cloud-bigquery<2.21,>=1.28.0, but you have google-cloud-bigquery 2.26.0 which is incompatible.\n",
      "tfx-bsl 1.2.0 requires google-cloud-bigquery<2.21,>=1.28.0, but you have google-cloud-bigquery 2.26.0 which is incompatible.\n",
      "tensorflow 2.5.0 requires grpcio~=1.34.0, but you have grpcio 1.44.0 which is incompatible.\n",
      "tensorflow-transform 1.2.0 requires google-cloud-bigquery<2.21,>=1.28.0, but you have google-cloud-bigquery 2.26.0 which is incompatible.\n",
      "tensorflow-model-analysis 0.33.0 requires google-cloud-bigquery<2.21,>=1.28.0, but you have google-cloud-bigquery 2.26.0 which is incompatible.\n",
      "tensorflow-data-validation 1.2.0 requires google-cloud-bigquery<2.21,>=1.28.0, but you have google-cloud-bigquery 2.26.0 which is incompatible.\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "WARNING: You are using pip version 21.2.4; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\n",
      "Removing intermediate container 04f06c7ed872\n",
      " ---> fa1d12a01be5\n",
      "Step 4/5 : COPY src/ src/\n",
      " ---> cf02f71e188c\n",
      "Step 5/5 : ENV PYTHONPATH=\"/pipeline:${PYTHONPATH}\"\n",
      " ---> Running in 70c459cf534c\n",
      "Removing intermediate container 70c459cf534c\n",
      " ---> 739d1569393d\n",
      "Successfully built 739d1569393d\n",
      "Successfully tagged gcr.io/grandelli-demo-295810/chicago-taxi-tips:v01\n",
      "PUSH\n",
      "Pushing gcr.io/grandelli-demo-295810/chicago-taxi-tips:v01\n",
      "The push refers to repository [gcr.io/grandelli-demo-295810/chicago-taxi-tips]\n",
      "d9cd3b8edf9d: Preparing\n",
      "e24201c4e8cb: Preparing\n",
      "d1b01478856c: Preparing\n",
      "d42c05f73feb: Preparing\n",
      "3119d30f29a9: Preparing\n",
      "121c9dfc7bce: Preparing\n",
      "868786b3710b: Preparing\n",
      "5bb1aa5df10d: Preparing\n",
      "f028010939aa: Preparing\n",
      "dc99c4ea3a81: Preparing\n",
      "37b508c5711b: Preparing\n",
      "756ab564e194: Preparing\n",
      "2ae86808a3d1: Preparing\n",
      "1dccbdf9b557: Preparing\n",
      "cfcbdbc2b748: Preparing\n",
      "937ab8f29c2e: Preparing\n",
      "5d417b2f7486: Preparing\n",
      "d6a297a3e6e4: Preparing\n",
      "6474a5e8117f: Preparing\n",
      "fe498124ed57: Preparing\n",
      "d5454704bb3d: Preparing\n",
      "fb896ef24b4b: Preparing\n",
      "5087113f67c8: Preparing\n",
      "2a92857a1d48: Preparing\n",
      "0ded97864c52: Preparing\n",
      "b50bbaac3e32: Preparing\n",
      "262ea1af4c10: Preparing\n",
      "b420a468ca49: Preparing\n",
      "608c205798d1: Preparing\n",
      "0760cd6d4269: Preparing\n",
      "fb4755c89c2a: Preparing\n",
      "22cfb9034da6: Preparing\n",
      "8bec4fbfce85: Preparing\n",
      "3b129ca3db46: Preparing\n",
      "64cb1a1930ab: Preparing\n",
      "600ef5a43f1f: Preparing\n",
      "8f8f0266f834: Preparing\n",
      "37b508c5711b: Waiting\n",
      "fe498124ed57: Waiting\n",
      "937ab8f29c2e: Waiting\n",
      "d6a297a3e6e4: Waiting\n",
      "5d417b2f7486: Waiting\n",
      "d5454704bb3d: Waiting\n",
      "6474a5e8117f: Waiting\n",
      "cfcbdbc2b748: Waiting\n",
      "fb896ef24b4b: Waiting\n",
      "756ab564e194: Waiting\n",
      "5087113f67c8: Waiting\n",
      "2ae86808a3d1: Waiting\n",
      "1dccbdf9b557: Waiting\n",
      "2a92857a1d48: Waiting\n",
      "0ded97864c52: Waiting\n",
      "b50bbaac3e32: Waiting\n",
      "262ea1af4c10: Waiting\n",
      "608c205798d1: Waiting\n",
      "121c9dfc7bce: Waiting\n",
      "5bb1aa5df10d: Waiting\n",
      "0760cd6d4269: Waiting\n",
      "f028010939aa: Waiting\n",
      "fb4755c89c2a: Waiting\n",
      "dc99c4ea3a81: Waiting\n",
      "868786b3710b: Waiting\n",
      "600ef5a43f1f: Waiting\n",
      "8f8f0266f834: Waiting\n",
      "22cfb9034da6: Waiting\n",
      "8bec4fbfce85: Waiting\n",
      "64cb1a1930ab: Waiting\n",
      "3b129ca3db46: Waiting\n",
      "3119d30f29a9: Layer already exists\n",
      "d42c05f73feb: Layer already exists\n",
      "121c9dfc7bce: Layer already exists\n",
      "868786b3710b: Layer already exists\n",
      "5bb1aa5df10d: Layer already exists\n",
      "f028010939aa: Layer already exists\n",
      "dc99c4ea3a81: Layer already exists\n",
      "37b508c5711b: Layer already exists\n",
      "756ab564e194: Layer already exists\n",
      "2ae86808a3d1: Layer already exists\n",
      "1dccbdf9b557: Layer already exists\n",
      "cfcbdbc2b748: Layer already exists\n",
      "5d417b2f7486: Layer already exists\n",
      "937ab8f29c2e: Layer already exists\n",
      "d6a297a3e6e4: Layer already exists\n",
      "6474a5e8117f: Layer already exists\n",
      "fe498124ed57: Layer already exists\n",
      "d5454704bb3d: Layer already exists\n",
      "fb896ef24b4b: Layer already exists\n",
      "5087113f67c8: Layer already exists\n",
      "2a92857a1d48: Layer already exists\n",
      "b50bbaac3e32: Layer already exists\n",
      "0ded97864c52: Layer already exists\n",
      "262ea1af4c10: Layer already exists\n",
      "b420a468ca49: Layer already exists\n",
      "608c205798d1: Layer already exists\n",
      "0760cd6d4269: Layer already exists\n",
      "fb4755c89c2a: Layer already exists\n",
      "8bec4fbfce85: Layer already exists\n",
      "22cfb9034da6: Layer already exists\n",
      "3b129ca3db46: Layer already exists\n",
      "64cb1a1930ab: Layer already exists\n",
      "8f8f0266f834: Layer already exists\n",
      "600ef5a43f1f: Layer already exists\n",
      "d1b01478856c: Pushed\n",
      "d9cd3b8edf9d: Pushed\n",
      "e24201c4e8cb: Pushed\n",
      "v01: digest: sha256:9a448bf8e2ad37a77ac0bef874942d9af9b79e56127b21f0a01b3f862c26116a size: 8102\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                               IMAGES                                              STATUS\n",
      "5c369433-6f9f-49a1-af60-33d9cfd1eb22  2022-03-31T15:43:38+00:00  3M33S     gs://grandelli-demo-295810_cloudbuild/source/1648741417.350531-82bcd07ab4574fffa6dcccfd26885958.tgz  gcr.io/grandelli-demo-295810/chicago-taxi-tips:v01  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --tag $TFX_IMAGE_URI . --timeout=15m --machine-type=e2-highcpu-8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a9890d",
   "metadata": {},
   "source": [
    "### Compile pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "09c8a3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Pipeline components: ['bigquery_data_gen', 'vertex_batch_prediction', 'datastore_prediction_writer']\n",
      "INFO:root:Beam pipeline args: ['--project=grandelli-demo-295810', '--temp_location=gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/temp']\n"
     ]
    }
   ],
   "source": [
    "from src.tfx_pipelines import runner\n",
    "\n",
    "pipeline_definition_file = f'{config.PIPELINE_NAME}.json'\n",
    "pipeline_definition = runner.compile_prediction_pipeline(pipeline_definition_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc2792a",
   "metadata": {},
   "source": [
    "### Submit run to Vertex Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "37dcc92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/kfp/v2/google/client/client.py:173: FutureWarning: AIPlatformClient will be deprecated in v2.0.0. Please use PipelineJob https://googleapis.dev/python/aiplatform/latest/_modules/google/cloud/aiplatform/pipeline_jobs.html in Vertex SDK. Install the SDK using \"pip install google-cloud-aiplatform\"\n",
      "  category=FutureWarning,\n",
      "E0331 14:56:03.220886862       1 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "See the Pipeline job <a href=\"https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/chicago-taxi-tips-classifier-v01-predict-pipeline-20220331145605?project=grandelli-demo-295810\" target=\"_blank\" >here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'name': 'projects/155283586619/locations/us-central1/pipelineJobs/chicago-taxi-tips-classifier-v01-predict-pipeline-20220331145605',\n",
       " 'displayName': 'chicago-taxi-tips-classifier-v01-predict-pipeline-20220331145605',\n",
       " 'createTime': '2022-03-31T14:56:05.453498Z',\n",
       " 'updateTime': '2022-03-31T14:56:05.453498Z',\n",
       " 'pipelineSpec': {'deploymentConfig': {'@type': 'type.googleapis.com/ml_pipelines.PipelineDeploymentConfig',\n",
       "   'executors': {'bigquery_data_gen_executor': {'container': {'image': 'gcr.io/grandelli-demo-295810/chicago-taxi-tips:v01',\n",
       "      'command': ['python',\n",
       "       '-m',\n",
       "       'tfx.orchestration.kubeflow.v2.container.kubeflow_v2_run_executor'],\n",
       "      'args': ['--executor_class_path',\n",
       "       'src.tfx_pipelines.components.bigquery_data_gen_Executor',\n",
       "       '--json_serialized_invocation_args',\n",
       "       '{{$}}',\n",
       "       '--project=grandelli-demo-295810',\n",
       "       '--temp_location=gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/temp']}},\n",
       "    'datastore_prediction_writer_executor': {'container': {'image': 'gcr.io/grandelli-demo-295810/chicago-taxi-tips:v01',\n",
       "      'command': ['python',\n",
       "       '-m',\n",
       "       'tfx.orchestration.kubeflow.v2.container.kubeflow_v2_run_executor'],\n",
       "      'args': ['--executor_class_path',\n",
       "       'src.tfx_pipelines.components.datastore_prediction_writer_Executor',\n",
       "       '--json_serialized_invocation_args',\n",
       "       '{{$}}',\n",
       "       '--project=grandelli-demo-295810',\n",
       "       '--temp_location=gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/temp']}},\n",
       "    'vertex_batch_prediction_executor': {'container': {'image': 'gcr.io/grandelli-demo-295810/chicago-taxi-tips:v01',\n",
       "      'command': ['python',\n",
       "       '-m',\n",
       "       'tfx.orchestration.kubeflow.v2.container.kubeflow_v2_run_executor'],\n",
       "      'args': ['--executor_class_path',\n",
       "       'src.tfx_pipelines.components.vertex_batch_prediction_Executor',\n",
       "       '--json_serialized_invocation_args',\n",
       "       '{{$}}',\n",
       "       '--project=grandelli-demo-295810',\n",
       "       '--temp_location=gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/temp']}}}},\n",
       "  'components': {'bigquery_data_gen': {'inputDefinitions': {'parameters': {'beam_args': {'type': 'STRING'},\n",
       "      'output_data_format': {'type': 'STRING'},\n",
       "      'sql_query': {'type': 'STRING'}}},\n",
       "    'outputDefinitions': {'artifacts': {'serving_dataset': {'artifactType': {'instanceSchema': 'title: tfx.Dataset\\ntype: object\\n'}}}},\n",
       "    'executorLabel': 'bigquery_data_gen_executor'},\n",
       "   'datastore_prediction_writer': {'inputDefinitions': {'artifacts': {'prediction_results': {'artifactType': {'instanceSchema': 'title: tfx.Dataset\\ntype: object\\n'}}},\n",
       "     'parameters': {'beam_args': {'type': 'STRING'},\n",
       "      'datastore_kind': {'type': 'STRING'},\n",
       "      'predictions_format': {'type': 'STRING'}}},\n",
       "    'executorLabel': 'datastore_prediction_writer_executor'},\n",
       "   'vertex_batch_prediction': {'inputDefinitions': {'artifacts': {'serving_dataset': {'artifactType': {'instanceSchema': 'title: tfx.Dataset\\ntype: object\\n'}}},\n",
       "     'parameters': {'instances_format': {'type': 'STRING'},\n",
       "      'job_resources': {'type': 'STRING'},\n",
       "      'model_display_name': {'type': 'STRING'},\n",
       "      'predictions_format': {'type': 'STRING'},\n",
       "      'project': {'type': 'STRING'},\n",
       "      'region': {'type': 'STRING'}}},\n",
       "    'outputDefinitions': {'artifacts': {'prediction_results': {'artifactType': {'instanceSchema': 'title: tfx.Dataset\\ntype: object\\n'}}}},\n",
       "    'executorLabel': 'vertex_batch_prediction_executor'}},\n",
       "  'schemaVersion': '2.0.0',\n",
       "  'root': {'dag': {'tasks': {'bigquery_data_gen': {'taskInfo': {'name': 'bigquery_data_gen'},\n",
       "      'inputs': {'parameters': {'beam_args': {'runtimeValue': {'constantValue': {'stringValue': '{\"runner\": \"DirectRunner\", \"temporary_dir\": \"gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/temp\", \"gcs_location\": \"gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/temp\", \"project\": \"grandelli-demo-295810\", \"region\": \"us-central1\", \"setup_file\": \"./setup.py\"}'}}},\n",
       "        'output_data_format': {'runtimeValue': {'constantValue': {'stringValue': 'jsonl'}}},\n",
       "        'sql_query': {'runtimeValue': {'constantValue': {'stringValue': \"\\n    SELECT \\n        IF(trip_month IS NULL, -1, trip_month) trip_month,\\n        IF(trip_day IS NULL, -1, trip_day) trip_day,\\n        IF(trip_day_of_week IS NULL, -1, trip_day_of_week) trip_day_of_week,\\n        IF(trip_hour IS NULL, -1, trip_hour) trip_hour,\\n        IF(trip_seconds IS NULL, -1, trip_seconds) trip_seconds,\\n        IF(trip_miles IS NULL, -1, trip_miles) trip_miles,\\n        IF(payment_type IS NULL, 'NA', payment_type) payment_type,\\n        IF(pickup_grid IS NULL, 'NA', pickup_grid) pickup_grid,\\n        IF(dropoff_grid IS NULL, 'NA', dropoff_grid) dropoff_grid,\\n        IF(euclidean IS NULL, -1, euclidean) euclidean,\\n        IF(loc_cross IS NULL, 'NA', loc_cross) loc_cross\\n    FROM partner_training.chicago_taxitrips_prep \\n    LIMIT 1000\"}}}}},\n",
       "      'componentRef': {'name': 'bigquery_data_gen'}},\n",
       "     'datastore_prediction_writer': {'taskInfo': {'name': 'datastore_prediction_writer'},\n",
       "      'inputs': {'parameters': {'beam_args': {'runtimeValue': {'constantValue': {'stringValue': '{\"runner\": \"DirectRunner\", \"temporary_dir\": \"gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/temp\", \"gcs_location\": \"gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/temp\", \"project\": \"grandelli-demo-295810\", \"region\": \"us-central1\", \"setup_file\": \"./setup.py\"}'}}},\n",
       "        'datastore_kind': {'runtimeValue': {'constantValue': {'stringValue': 'chicago-taxi-tips-classifier-v01-predictions'}}},\n",
       "        'predictions_format': {'runtimeValue': {'constantValue': {'stringValue': 'jsonl'}}}},\n",
       "       'artifacts': {'prediction_results': {'taskOutputArtifact': {'producerTask': 'vertex_batch_prediction',\n",
       "          'outputArtifactKey': 'prediction_results'}}}},\n",
       "      'dependentTasks': ['vertex_batch_prediction'],\n",
       "      'componentRef': {'name': 'datastore_prediction_writer'}},\n",
       "     'vertex_batch_prediction': {'taskInfo': {'name': 'vertex_batch_prediction'},\n",
       "      'inputs': {'parameters': {'instances_format': {'runtimeValue': {'constantValue': {'stringValue': 'jsonl'}}},\n",
       "        'job_resources': {'runtimeValue': {'constantValue': {'stringValue': '{\"machine_type\": \"n1-standard-2\", \"starting_replica_count\": 1, \"max_replica_count\": 10}'}}},\n",
       "        'model_display_name': {'runtimeValue': {'constantValue': {'stringValue': 'chicago-taxi-tips-classifier-v01'}}},\n",
       "        'predictions_format': {'runtimeValue': {'constantValue': {'stringValue': 'jsonl'}}},\n",
       "        'project': {'runtimeValue': {'constantValue': {'stringValue': 'grandelli-demo-295810'}}},\n",
       "        'region': {'runtimeValue': {'constantValue': {'stringValue': 'us-central1'}}}},\n",
       "       'artifacts': {'serving_dataset': {'taskOutputArtifact': {'producerTask': 'bigquery_data_gen',\n",
       "          'outputArtifactKey': 'serving_dataset'}}}},\n",
       "      'dependentTasks': ['bigquery_data_gen'],\n",
       "      'componentRef': {'name': 'vertex_batch_prediction'}}}}},\n",
       "  'sdkVersion': 'tfx-1.2.0',\n",
       "  'pipelineInfo': {'name': 'chicago-taxi-tips-classifier-v01-predict-pipeline'},\n",
       "  'deploymentSpec': {'executors': {'bigquery_data_gen_executor': {'container': {'image': 'gcr.io/grandelli-demo-295810/chicago-taxi-tips:v01',\n",
       "      'command': ['python',\n",
       "       '-m',\n",
       "       'tfx.orchestration.kubeflow.v2.container.kubeflow_v2_run_executor'],\n",
       "      'args': ['--executor_class_path',\n",
       "       'src.tfx_pipelines.components.bigquery_data_gen_Executor',\n",
       "       '--json_serialized_invocation_args',\n",
       "       '{{$}}',\n",
       "       '--project=grandelli-demo-295810',\n",
       "       '--temp_location=gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/temp']}},\n",
       "    'datastore_prediction_writer_executor': {'container': {'image': 'gcr.io/grandelli-demo-295810/chicago-taxi-tips:v01',\n",
       "      'command': ['python',\n",
       "       '-m',\n",
       "       'tfx.orchestration.kubeflow.v2.container.kubeflow_v2_run_executor'],\n",
       "      'args': ['--executor_class_path',\n",
       "       'src.tfx_pipelines.components.datastore_prediction_writer_Executor',\n",
       "       '--json_serialized_invocation_args',\n",
       "       '{{$}}',\n",
       "       '--project=grandelli-demo-295810',\n",
       "       '--temp_location=gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/temp']}},\n",
       "    'vertex_batch_prediction_executor': {'container': {'image': 'gcr.io/grandelli-demo-295810/chicago-taxi-tips:v01',\n",
       "      'command': ['python',\n",
       "       '-m',\n",
       "       'tfx.orchestration.kubeflow.v2.container.kubeflow_v2_run_executor'],\n",
       "      'args': ['--executor_class_path',\n",
       "       'src.tfx_pipelines.components.vertex_batch_prediction_Executor',\n",
       "       '--json_serialized_invocation_args',\n",
       "       '{{$}}',\n",
       "       '--project=grandelli-demo-295810',\n",
       "       '--temp_location=gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/temp']}}}}},\n",
       " 'state': 'PIPELINE_STATE_PENDING',\n",
       " 'labels': {'tfx_py_version': '3-7',\n",
       "  'tfx_runner': 'kubeflow_v2',\n",
       "  'tfx_version': '1-2-0'},\n",
       " 'runtimeConfig': {'gcsOutputDirectory': 'gs://grandelli-demo-295810-partner-training-2022/chicago-taxi-tips/tfx_artifacts/chicago-taxi-tips-classifier-v01-predict-pipeline'},\n",
       " 'serviceAccount': '155283586619-compute@developer.gserviceaccount.com'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kfp.v2.google.client import AIPlatformClient\n",
    "\n",
    "pipeline_client = AIPlatformClient(\n",
    "    project_id=PROJECT, region=REGION)\n",
    "                 \n",
    "pipeline_client.create_run_from_job_spec(\n",
    "    job_spec_path=pipeline_definition_file\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0d5bef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "managed-notebooks.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:latest"
  },
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
